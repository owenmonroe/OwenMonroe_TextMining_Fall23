{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains processing basics and visualizations to help you understand different NLP techniques that you have studied. We will illustrate the steps mainly using built-in nltk and spacy functions. They are purely meant for presentation and understanding. We do not expect you to implement those things as part of any requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Lexical analysis\n",
    "\n",
    "The first part of the notebook covers lexical analysis step on the COVID tweet dataset, specifically, part-of-speech (POS) tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying POS tagging to COVID tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here that you need for different processing steps\n",
    "import nltk\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set:  44957\n",
      "Data set:  44955\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>CleanedTweet</th>\n",
       "      <th>Accounts</th>\n",
       "      <th>TokenizedTweet</th>\n",
       "      <th>StopwordRemovedTweet</th>\n",
       "      <th>StemmedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@menyrbie @phil_gahan @chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https t co ifz9fan2pa and https t co xx6ghgfz...</td>\n",
       "      <td>['menyrbie', 'phil_gahan', 'chrisitv']</td>\n",
       "      <td>['https', 't', 'co', 'ifz9fan2pa', 'and', 'htt...</td>\n",
       "      <td>['https', 'co', 'ifz9fan2pa', 'https', 'co', '...</td>\n",
       "      <td>['http', 't', 'co', 'ifz9fan2pa', 'and', 'http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['advice', 'talk', 'to', 'your', 'neighbours',...</td>\n",
       "      <td>['advice', 'talk', 'neighbours', 'family', 'ex...</td>\n",
       "      <td>['advic', 'talk', 'to', 'your', 'neighbour', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>coronavirus australia: woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworths', 'to...</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworths', 'gi...</td>\n",
       "      <td>['coronaviru', 'australia', 'woolworth', 'to',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['covid19france', 'covid_19', 'covid19', 'coro...</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['my', 'food', 'stock', 'is', 'not', 'the', 'o...</td>\n",
       "      <td>['food', 'stock', 'one', 'empty', 'please', 'p...</td>\n",
       "      <td>['my', 'food', 'stock', 'is', 'not', 'the', 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>me, ready to go at supermarket during the #cov...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['covid19', 'coronavirus', 'coronavirusfrance'...</td>\n",
       "      <td>me ready to go at supermarket during the outbr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['me', 'ready', 'to', 'go', 'at', 'supermarket...</td>\n",
       "      <td>['ready', 'go', 'supermarket', 'outbreak', 'pa...</td>\n",
       "      <td>['me', 'readi', 'to', 'go', 'at', 'supermarket...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UserName ScreenName   Location     TweetAt  \\\n",
       "0     3799      48751     London  16-03-2020   \n",
       "1     3800      48752         UK  16-03-2020   \n",
       "2     3801      48753  Vagabonds  16-03-2020   \n",
       "3     3802      48754        NaN  16-03-2020   \n",
       "4     3803      48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \\\n",
       "0  @menyrbie @phil_gahan @chrisitv https://t.co/i...   Neutral   \n",
       "1  advice talk to your neighbours family to excha...  Positive   \n",
       "2  coronavirus australia: woolworths to give elde...  Positive   \n",
       "3  my food stock is not the only one which is emp...  Positive   \n",
       "4  me, ready to go at supermarket during the #cov...  Negative   \n",
       "\n",
       "                                            Hashtags  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  ['covid19france', 'covid_19', 'covid19', 'coro...   \n",
       "4  ['covid19', 'coronavirus', 'coronavirusfrance'...   \n",
       "\n",
       "                                        CleanedTweet  \\\n",
       "0   https t co ifz9fan2pa and https t co xx6ghgfz...   \n",
       "1  advice talk to your neighbours family to excha...   \n",
       "2  coronavirus australia woolworths to give elder...   \n",
       "3  my food stock is not the only one which is emp...   \n",
       "4  me ready to go at supermarket during the outbr...   \n",
       "\n",
       "                                 Accounts  \\\n",
       "0  ['menyrbie', 'phil_gahan', 'chrisitv']   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "\n",
       "                                      TokenizedTweet  \\\n",
       "0  ['https', 't', 'co', 'ifz9fan2pa', 'and', 'htt...   \n",
       "1  ['advice', 'talk', 'to', 'your', 'neighbours',...   \n",
       "2  ['coronavirus', 'australia', 'woolworths', 'to...   \n",
       "3  ['my', 'food', 'stock', 'is', 'not', 'the', 'o...   \n",
       "4  ['me', 'ready', 'to', 'go', 'at', 'supermarket...   \n",
       "\n",
       "                                StopwordRemovedTweet  \\\n",
       "0  ['https', 'co', 'ifz9fan2pa', 'https', 'co', '...   \n",
       "1  ['advice', 'talk', 'neighbours', 'family', 'ex...   \n",
       "2  ['coronavirus', 'australia', 'woolworths', 'gi...   \n",
       "3  ['food', 'stock', 'one', 'empty', 'please', 'p...   \n",
       "4  ['ready', 'go', 'supermarket', 'outbreak', 'pa...   \n",
       "\n",
       "                                        StemmedTweet  \n",
       "0  ['http', 't', 'co', 'ifz9fan2pa', 'and', 'http...  \n",
       "1  ['advic', 'talk', 'to', 'your', 'neighbour', '...  \n",
       "2  ['coronaviru', 'australia', 'woolworth', 'to',...  \n",
       "3  ['my', 'food', 'stock', 'is', 'not', 'the', 'o...  \n",
       "4  ['me', 'readi', 'to', 'go', 'at', 'supermarket...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv into dataframe and remove lines which contain missing values in the OriginalTweet column\n",
    "\n",
    "data_df = pd.read_csv(\"Dataset/covid.csv\")\n",
    "\n",
    "print (\"Data set: \", len(data_df))\n",
    "\n",
    "data_df = data_df[data_df['OriginalTweet'].notna()]\n",
    "print (\"Data set: \", len(data_df))\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Penn Treebank-style tokenization and POS tagging\n",
    "\n",
    "#### POS tagging using nltk\n",
    "Refer to this link for explanation of all POS tag abbreviations: \n",
    "https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# class for tokenization\n",
    "class Splitter(object):\n",
    "    # load the tokenizer\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    #split input \n",
    "    def split(self, text):\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "# class for POS tagging\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def pos_tag(self, sentences):\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "    \n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i followed this when i went shopping a few days ago. it's a pain but necessary! protect yourself from grocery shopping - consumer reports #covid2019 #stayhealthy https://t.co/48ng14me6e\n",
      "\n",
      "\n",
      "('i', 'i', ['NN'])\n",
      "('followed', 'followed', ['VBD'])\n",
      "('this', 'this', ['DT'])\n",
      "('when', 'when', ['WRB'])\n",
      "('i', 'i', ['JJ'])\n",
      "('went', 'went', ['VBD'])\n",
      "('shopping', 'shopping', ['VBG'])\n",
      "('a', 'a', ['DT'])\n",
      "('few', 'few', ['JJ'])\n",
      "('days', 'days', ['NNS'])\n",
      "('ago', 'ago', ['RB'])\n",
      "('.', '.', ['.'])\n",
      "\n",
      "\n",
      "('it', 'it', ['PRP'])\n",
      "(\"'s\", \"'s\", ['VBZ'])\n",
      "('a', 'a', ['DT'])\n",
      "('pain', 'pain', ['NN'])\n",
      "('but', 'but', ['CC'])\n",
      "('necessary', 'necessary', ['JJ'])\n",
      "('!', '!', ['.'])\n",
      "\n",
      "\n",
      "('protect', 'protect', ['VB'])\n",
      "('yourself', 'yourself', ['PRP'])\n",
      "('from', 'from', ['IN'])\n",
      "('grocery', 'grocery', ['NN'])\n",
      "('shopping', 'shopping', ['NN'])\n",
      "('-', '-', [':'])\n",
      "('consumer', 'consumer', ['NN'])\n",
      "('reports', 'reports', ['NNS'])\n",
      "('#', '#', ['#'])\n",
      "('covid2019', 'covid2019', ['JJ'])\n",
      "('#', '#', ['#'])\n",
      "('stayhealthy', 'stayhealthy', ['JJ'])\n",
      "('https', 'https', ['NN'])\n",
      "(':', ':', [':'])\n",
      "('//t.co/48ng14me6e', '//t.co/48ng14me6e', ['NN'])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POS tagging on an example tweet \n",
    "# We use OriginalTweet as input because stemming and stopword removal would make POS tagging somewhat meaningless, as the integrity of sentences/tokens is violated. \n",
    "\n",
    "print(data_df.OriginalTweet.tolist()[98])\n",
    "print(\"\\n\")\n",
    "\n",
    "tweet = data_df.OriginalTweet.tolist()[98]\n",
    "splitted_sentences = splitter.split(tweet)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "for sentence in pos_tagged_sentences:\n",
    "    for words in sentence:\n",
    "        print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging using spacy\n",
    "\n",
    "Refer to this link for more explanation and examples- https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i          PRON       PRP       \n",
      "followed   VERB       VBD       \n",
      "this       DET        DT        \n",
      "when       ADV        WRB       \n",
      "i          PRON       PRP       \n",
      "went       VERB       VBD       \n",
      "shopping   VERB       VBG       \n",
      "a          DET        DT        \n",
      "few        ADJ        JJ        \n",
      "days       NOUN       NNS       \n",
      "ago        ADV        RB        \n",
      ".          PUNCT      .         \n",
      "it         PRON       PRP       \n",
      "'s         AUX        VBZ       \n",
      "a          DET        DT        \n",
      "pain       NOUN       NN        \n",
      "but        CCONJ      CC        \n",
      "necessary  ADJ        JJ        \n",
      "!          PUNCT      .         \n",
      "protect    VERB       VB        \n",
      "yourself   PRON       PRP       \n",
      "from       ADP        IN        \n",
      "grocery    NOUN       NN        \n",
      "shopping   NOUN       NN        \n",
      "-          PUNCT      HYPH      \n",
      "consumer   NOUN       NN        \n",
      "reports    NOUN       NNS       \n",
      "#          SYM        $         \n",
      "covid2019  PROPN      NNP       \n",
      "#          NOUN       NN        \n",
      "stayhealthy NOUN       NN        \n",
      "https://t.co/48ng14me6e INTJ       UH        \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(tweet)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of nltk and spacy POS tagging results\n",
    "\n",
    "Does one tagger perform better than the other? What are some of the differences you observe?\n",
    "You can also try out tweets with more irregular text, which is likely to yield poorer results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Syntactic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Chunking with nltk\n",
    "\n",
    "We can use regular expressions in NLTK for chunking. The following is a simple noun phrase chunking example. Study the `np` regular expression and try to interpret what it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('followed', 'VBD'),\n",
       " ('this', 'DT'),\n",
       " ('when', 'WRB'),\n",
       " ('i', 'JJ'),\n",
       " ('went', 'VBD'),\n",
       " ('shopping', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('few', 'JJ'),\n",
       " ('days', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('.', '.'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pain', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('necessary', 'JJ'),\n",
       " ('!', '.'),\n",
       " ('protect', 'VB'),\n",
       " ('yourself', 'PRP'),\n",
       " ('from', 'IN'),\n",
       " ('grocery', 'NN'),\n",
       " ('shopping', 'NN'),\n",
       " ('-', ':'),\n",
       " ('consumer', 'NN'),\n",
       " ('reports', 'NNS'),\n",
       " ('#', '#'),\n",
       " ('covid2019', 'JJ'),\n",
       " ('#', '#'),\n",
       " ('stayhealthy', 'JJ'),\n",
       " ('https', 'NN'),\n",
       " (':', ':'),\n",
       " ('//t.co/48ng14me6e', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# NLTK comes with RegexParser() function, which can help us with creating a simple noun phrase chunker.\n",
    "np = (\"NP: {<DT>?<JJ>*<NN>+}\")\n",
    "\n",
    "# create the regex for chunking\n",
    "chunking = nltk.RegexpParser(np)\n",
    "\n",
    "# tokenize the tweet sentence\n",
    "sent_token = nltk.word_tokenize(tweet)\n",
    "\n",
    "# POS tagging, a prerequisite for chunking\n",
    "tagging = nltk.pos_tag(sent_token)\n",
    "tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP i/NN)\n",
      "  followed/VBD\n",
      "  this/DT\n",
      "  when/WRB\n",
      "  i/JJ\n",
      "  went/VBD\n",
      "  shopping/VBG\n",
      "  a/DT\n",
      "  few/JJ\n",
      "  days/NNS\n",
      "  ago/RB\n",
      "  ./.\n",
      "  it/PRP\n",
      "  's/VBZ\n",
      "  (NP a/DT pain/NN)\n",
      "  but/CC\n",
      "  necessary/JJ\n",
      "  !/.\n",
      "  protect/VB\n",
      "  yourself/PRP\n",
      "  from/IN\n",
      "  (NP grocery/NN shopping/NN)\n",
      "  -/:\n",
      "  (NP consumer/NN)\n",
      "  reports/NNS\n",
      "  #/#\n",
      "  covid2019/JJ\n",
      "  #/#\n",
      "  (NP stayhealthy/JJ https/NN)\n",
      "  :/:\n",
      "  (NP //t.co/48ng14me6e/NN))\n"
     ]
    }
   ],
   "source": [
    "#!pip install svgling\n",
    "\n",
    "# visualize the chunks\n",
    "tree = chunking.parse(tagging)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP i/NN)\n",
      "(NP a/DT pain/NN)\n",
      "(NP grocery/NN shopping/NN)\n",
      "(NP consumer/NN)\n",
      "(NP stayhealthy/JJ https/NN)\n",
      "(NP //t.co/48ng14me6e/NN)\n"
     ]
    }
   ],
   "source": [
    "# Print only the noun phrase chunks\n",
    "for i in tree:\n",
    "    if \"NP\" in str(i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with spacy\n",
    "\n",
    "To identify chunks in spacy, we need to first parse dependencies. \n",
    "\n",
    "For more details on spacy linguistic processing, refer to https://spacy.io/usage/linguistic-features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"94be80b3aec74137a509d9c146ccc99e-0\" class=\"displacy\" width=\"2750\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">i</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">followed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">when</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">i</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">went</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">shopping</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">few</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">days</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">ago.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1350\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1350\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">pain</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1650\">necessary!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1650\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1750\">protect</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">yourself</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1950\">from</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1950\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2050\">grocery</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2050\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">shopping -</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2250\">consumer</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2350\">reports #</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2350\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2450\">covid2019 #</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2550\">stayhealthy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2550\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2650\">https://t.co/48ng14me6e</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2650\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-0\" stroke-width=\"2px\" d=\"M70,302.0 C70,252.0 125.0,252.0 125.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,304.0 L62,292.0 78,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-1\" stroke-width=\"2px\" d=\"M170,302.0 C170,252.0 225.0,252.0 225.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M225.0,304.0 L233.0,292.0 217.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-2\" stroke-width=\"2px\" d=\"M370,302.0 C370,202.0 530.0,202.0 530.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,304.0 L362,292.0 378,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-3\" stroke-width=\"2px\" d=\"M470,302.0 C470,252.0 525.0,252.0 525.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,304.0 L462,292.0 478,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-4\" stroke-width=\"2px\" d=\"M170,302.0 C170,102.0 540.0,102.0 540.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M540.0,304.0 L548.0,292.0 532.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-5\" stroke-width=\"2px\" d=\"M570,302.0 C570,252.0 625.0,252.0 625.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M625.0,304.0 L633.0,292.0 617.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-6\" stroke-width=\"2px\" d=\"M770,302.0 C770,202.0 930.0,202.0 930.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,304.0 L762,292.0 778,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-7\" stroke-width=\"2px\" d=\"M870,302.0 C870,252.0 925.0,252.0 925.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,304.0 L862,292.0 878,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-8\" stroke-width=\"2px\" d=\"M970,302.0 C970,252.0 1025.0,252.0 1025.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,304.0 L962,292.0 978,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-9\" stroke-width=\"2px\" d=\"M670,302.0 C670,102.0 1040.0,102.0 1040.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1040.0,304.0 L1048.0,292.0 1032.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-10\" stroke-width=\"2px\" d=\"M1170,302.0 C1170,252.0 1225.0,252.0 1225.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1170,304.0 L1162,292.0 1178,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-11\" stroke-width=\"2px\" d=\"M1370,302.0 C1370,252.0 1425.0,252.0 1425.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1370,304.0 L1362,292.0 1378,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-12\" stroke-width=\"2px\" d=\"M1270,302.0 C1270,202.0 1430.0,202.0 1430.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1430.0,304.0 L1438.0,292.0 1422.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-13\" stroke-width=\"2px\" d=\"M1470,302.0 C1470,252.0 1525.0,252.0 1525.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1525.0,304.0 L1533.0,292.0 1517.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-14\" stroke-width=\"2px\" d=\"M1470,302.0 C1470,202.0 1630.0,202.0 1630.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1630.0,304.0 L1638.0,292.0 1622.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-15\" stroke-width=\"2px\" d=\"M1770,302.0 C1770,252.0 1825.0,252.0 1825.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1825.0,304.0 L1833.0,292.0 1817.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-16\" stroke-width=\"2px\" d=\"M1770,302.0 C1770,202.0 1930.0,202.0 1930.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1930.0,304.0 L1938.0,292.0 1922.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-17\" stroke-width=\"2px\" d=\"M2070,302.0 C2070,152.0 2335.0,152.0 2335.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2070,304.0 L2062,292.0 2078,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-18\" stroke-width=\"2px\" d=\"M2170,302.0 C2170,252.0 2225.0,252.0 2225.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,304.0 L2162,292.0 2178,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-19\" stroke-width=\"2px\" d=\"M2270,302.0 C2270,252.0 2325.0,252.0 2325.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2270,304.0 L2262,292.0 2278,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-20\" stroke-width=\"2px\" d=\"M1970,302.0 C1970,102.0 2340.0,102.0 2340.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2340.0,304.0 L2348.0,292.0 2332.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-21\" stroke-width=\"2px\" d=\"M2470,302.0 C2470,252.0 2525.0,252.0 2525.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2470,304.0 L2462,292.0 2478,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-22\" stroke-width=\"2px\" d=\"M1770,302.0 C1770,52.0 2545.0,52.0 2545.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2545.0,304.0 L2553.0,292.0 2537.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-94be80b3aec74137a509d9c146ccc99e-0-23\" stroke-width=\"2px\" d=\"M1770,302.0 C1770,2.0 2650.0,2.0 2650.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-94be80b3aec74137a509d9c146ccc99e-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2650.0,304.0 L2658.0,292.0 2642.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displacy provides nice visualization features\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(tweet)    \n",
    "\n",
    "# visualize sentence dependencies\n",
    "displacy.render(doc, style='dep', jupyter = True, options = {'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency parsing\n",
    "\n",
    "There is limited built-in functionality when it comes to constituency parsing in nltk and spacy, although they provide wrappers to other existing tools like CoreNLP library. \n",
    "\n",
    "Stanza is a state-of-the-art NLP pipeline that includes constituency parsing (in addition to other linguistic analyses) and has a similar feel to spacy. You need to install it first. If you're interested in getting started with Stanza, see the link: https://stanfordnlp.github.io/stanza/#getting-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Semantic analysis\n",
    "\n",
    "### Named entity recognition \n",
    "\n",
    "Simple examples of named entity recognition using nltk and spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg is one of the founders of Facebook, a company from the United States\n",
      "[('Mark Zuckerberg', 'PERSON'),\n",
      " ('one', 'CARDINAL'),\n",
      " ('the United States', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "tweet = \"Mark Zuckerberg is one of the founders of Facebook, a company from the United States\"\n",
    "doc = nlp(tweet)\n",
    "print(tweet)\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg is one of the founders of Facebook, a company from the United States\n",
      "PERSON Mark\n",
      "ORGANIZATION Zuckerberg\n",
      "GPE Facebook\n",
      "GPE United States\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Download necessary packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    " \n",
    "# a function that performs named entity recognition and prints the results\n",
    "def nltk_ner(text): \n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    return\n",
    "\n",
    "print(tweet) \n",
    "nltk_ner(tweet)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is better? Try some other sentences before you come to a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need an efficient solution for NER, spaCy is a good choice with its pre-trained NER models. However, if you require more customization for domain-specific tasks, NLTK might be a better fit (you can train your own NER models). Ultimately, the choice depends on your project's requirements and your familiarity with the libraries. You may even choose to use both libraries in different parts of your NER pipeline if it best suits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ambiguity\n",
    "\n",
    "Let's try out some examples that were mentioned in class or others of your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of lexical ambiguity \n",
    "\n",
    "Let's take the following two sentences.\n",
    "\n",
    "```\n",
    "sentence1 = \"The key broke in the lock.\"\n",
    "sentence2 = \"The key problem was not one of quality but of quantity.\"\n",
    "```\n",
    "Both sentences have the word \"key\" but with different POS and meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging using nltk for the above sentences\n",
    "Let's see how our nltk POS tagger tags these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key broke in the lock.\n",
      "[[('The', 'The', ['DT']), ('key', 'key', ['JJ']), ('broke', 'broke', ['NN']), ('in', 'in', ['IN']), ('the', 'the', ['DT']), ('lock', 'lock', ['NN']), ('.', '.', ['.'])]]\n",
      "\n",
      "\n",
      "The key problem was not one of quality but of quantity.\n",
      "[[('The', 'The', ['DT']), ('key', 'key', ['NN']), ('problem', 'problem', ['NN']), ('was', 'was', ['VBD']), ('not', 'not', ['RB']), ('one', 'one', ['CD']), ('of', 'of', ['IN']), ('quality', 'quality', ['NN']), ('but', 'but', ['CC']), ('of', 'of', ['IN']), ('quantity', 'quantity', ['NN']), ('.', '.', ['.'])]]\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The key broke in the lock.\"\n",
    "sentence2 = \"The key problem was not one of quality but of quantity.\"\n",
    "\n",
    "print(sentence1)\n",
    "forpos= sentence1.strip()\n",
    "splitted_sentences = splitter.split(forpos)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "print(pos_tagged_sentences)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(sentence2)\n",
    "forpos= sentence2.strip()\n",
    "splitted_sentences = splitter.split(forpos)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "print(pos_tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Are the results as expected? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging using spacy for the above sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The key broke in the lock.\"\n",
    "sentence2 = \"The key problem was not one of quality but of quantity.\"\n",
    "\n",
    "print(sentence1)\n",
    "doc1 = nlp(sentence1)  \n",
    "for token in doc1:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}}')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(sentence2)\n",
    "\n",
    "doc2 = nlp(sentence2)  \n",
    "for token in doc2:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does spacy results compare to nltk? Does it handle the word \"key\" better? \n",
    "\n",
    "Try other examples of interest. For example, you may try \"I made her duck\". \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
