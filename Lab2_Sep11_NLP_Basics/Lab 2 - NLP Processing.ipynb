{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains processing basics and visualizations to help you understand different NLP techniques that you have studied. We will illustrate the steps mainly using built-in nltk and spacy functions. They are purely meant for presentation and understanding. We do not expect you to implement those things as part of any requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Lexical analysis\n",
    "\n",
    "The first part of the notebook covers lexical analysis step on the COVID tweet dataset, specifically, part-of-speech (POS) tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying POS tagging to COVID tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here that you need for different processing steps\n",
    "import nltk\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv into dataframe and remove lines which contain missing values in the OriginalTweet column\n",
    "\n",
    "data_df = pd.read_csv(\"Dataset/covid.csv\")\n",
    "\n",
    "print (\"Data set: \", len(data_df))\n",
    "\n",
    "data_df = data_df[data_df['OriginalTweet'].notna()]\n",
    "print (\"Data set: \", len(data_df))\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Penn Treebank-style tokenization and POS tagging\n",
    "\n",
    "#### POS tagging using nltk\n",
    "Refer to this link for explanation of all POS tag abbreviations: \n",
    "https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# class for tokenization\n",
    "class Splitter(object):\n",
    "    # load the tokenizer\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    #split input \n",
    "    def split(self, text):\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "# class for POS tagging\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def pos_tag(self, sentences):\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "    \n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# POS tagging on an example tweet \n",
    "# We use OriginalTweet as input because stemming and stopword removal would make POS tagging somewhat meaningless, as the integrity of sentences/tokens is violated. \n",
    "\n",
    "print(data_df.OriginalTweet.tolist()[98])\n",
    "print(\"\\n\")\n",
    "\n",
    "tweet = data_df.OriginalTweet.tolist()[98]\n",
    "splitted_sentences = splitter.split(tweet)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "for sentence in pos_tagged_sentences:\n",
    "    for words in sentence:\n",
    "        print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging using spacy\n",
    "\n",
    "Refer to this link for more explanation and examples- https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(tweet)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of nltk and spacy POS tagging results\n",
    "\n",
    "Does one tagger perform better than the other? What are some of the differences you observe?\n",
    "You can also try out tweets with more irregular text, which is likely to yield poorer results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Syntactic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Chunking with nltk\n",
    "\n",
    "We can use regular expressions in NLTK for chunking. The following is a simple noun phrase chunking example. Study the `np` regular expression and try to interpret what it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# NLTK comes with RegexParser() function, which can help us with creating a simple noun phrase chunker.\n",
    "np = (\"NP: {<DT>?<JJ>*<NN>+}\")\n",
    "\n",
    "# create the regex for chunking\n",
    "chunking = nltk.RegexpParser(np)\n",
    "\n",
    "# tokenize the tweet sentence\n",
    "sent_token = nltk.word_tokenize(tweet)\n",
    "\n",
    "# POS tagging, a prerequisite for chunking\n",
    "tagging = nltk.pos_tag(sent_token)\n",
    "tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install svgling\n",
    "\n",
    "# visualize the chunks\n",
    "tree = chunking.parse(tagging)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only the noun phrase chunks\n",
    "for i in tree:\n",
    "    if \"NP\" in str(i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with spacy\n",
    "\n",
    "To identify chunks in spacy, we need to first parse dependencies. \n",
    "\n",
    "For more details on spacy linguistic processing, refer to https://spacy.io/usage/linguistic-features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# displacy provides nice visualization features\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(tweet)    \n",
    "\n",
    "# visualize sentence dependencies\n",
    "displacy.render(doc, style='dep', jupyter = True, options = {'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spacy provides more information about the chunks. \n",
    "\n",
    "df = pd.DataFrame(columns=[\"chunk.text\", \"chunk.root.text\", \"chunk.root.dep\", \"chunk.root.head.text\"])\n",
    "i=0\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    # chunk text\n",
    "    df.at[i,\"chunk.text\"] = chunk.text\n",
    "    # chunk head\n",
    "    df.at[i,\"chunk.root.text\"] = chunk.root.text\n",
    "    df.at[i,\"chunk.root.dep\"] = chunk.root.dep_\n",
    "    df.at[i,\"chunk.root.head.text\"] = chunk.root.head.text\n",
    "    i+=1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency parsing\n",
    "\n",
    "There is limited built-in functionality when it comes to constituency parsing in nltk and spacy, although they provide wrappers to other existing tools like CoreNLP library. \n",
    "\n",
    "Stanza is a state-of-the-art NLP pipeline that includes constituency parsing (in addition to other linguistic analyses) and has a similar feel to spacy. You need to install it first. If you're interested in getting started with Stanza, see the link: https://stanfordnlp.github.io/stanza/#getting-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Semantic analysis\n",
    "\n",
    "### Named entity recognition \n",
    "\n",
    "Simple examples of named entity recognition using nltk and spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "tweet = \"Mark Zuckerberg is one of the founders of Facebook, a company from the United States\"\n",
    "doc = nlp(tweet)\n",
    "print(tweet)\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download necessary packages\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    " \n",
    "# a function that performs named entity recognition and prints the results\n",
    "def nltk_ner(text): \n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    return\n",
    "\n",
    "print(tweet) \n",
    "nltk_ner(tweet)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is better? Try some other sentences before you come to a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need an efficient solution for NER, spaCy is a good choice with its pre-trained NER models. However, if you require more customization for domain-specific tasks, NLTK might be a better fit (you can train your own NER models). Ultimately, the choice depends on your project's requirements and your familiarity with the libraries. You may even choose to use both libraries in different parts of your NER pipeline if it best suits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ambiguity\n",
    "\n",
    "Let's try out some examples that were mentioned in class or others of your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of lexical ambiguity \n",
    "\n",
    "Let's take the following two sentences.\n",
    "\n",
    "```\n",
    "sentence1 = \"The key broke in the lock.\"\n",
    "sentence2 = \"The key problem was not one of quality but of quantity.\"\n",
    "```\n",
    "Both sentences have the word \"key\" but with different POS and meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging using nltk for the above sentences\n",
    "Let's see how our nltk POS tagger tags these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The key broke in the lock.\"\n",
    "sentence2 = \"The key problem was not one of quality but of quantity.\"\n",
    "\n",
    "print(sentence1)\n",
    "forpos= sentence1.strip()\n",
    "splitted_sentences = splitter.split(forpos)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "print(pos_tagged_sentences)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(sentence2)\n",
    "forpos= sentence2.strip()\n",
    "splitted_sentences = splitter.split(forpos)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "print(pos_tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Are the results as expected? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging using spacy for the above sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The key broke in the lock.\"\n",
    "sentence2 = \"The key problem was not one of quality but of quantity.\"\n",
    "\n",
    "print(sentence1)\n",
    "doc1 = nlp(sentence1)  \n",
    "for token in doc1:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}}')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(sentence2)\n",
    "\n",
    "doc2 = nlp(sentence2)  \n",
    "for token in doc2:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does spacy results compare to nltk? Does it handle the word \"key\" better? \n",
    "\n",
    "Try other examples of interest. For example, you may try \"I made her duck\". \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
