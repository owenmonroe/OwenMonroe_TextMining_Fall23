{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzb6EJ3DBOd6"
   },
   "source": [
    "This notebook implements sentiment analysis on IMDB movie review dataset using Keras\n",
    "\n",
    "Credit : https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n",
    "\n",
    "The tutorial is very detailed. Check the narratives there if you have confusion about the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wJbIjYhBdB9"
   },
   "source": [
    "# Load the IMDB Dataset with Keras\n",
    "\n",
    "Keras provides built-in access to the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UY-45HZ8BeA7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnCfjsQXBky_",
    "outputId": "eb90e855-c2a5-4414-8b50-5d504d2ff488"
   },
   "outputs": [],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zt_wZhh6Bvm5",
    "outputId": "92d1fe9f-5d80-4423-dc88-5cd7e480248a"
   },
   "outputs": [],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aii9BCx2Bz69",
    "outputId": "8180486e-fa6d-4104-914a-0ef31b054eec"
   },
   "outputs": [],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "r4tpttheB3pZ",
    "outputId": "a527116e-5af5-47c7-8927-719569a4341f"
   },
   "outputs": [],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijnFxs5nDDeY"
   },
   "source": [
    "# Simple Multi-Layer Perceptron Model for the IMDB Dataset\n",
    "\n",
    "You can start by developing a simple multi-layer perceptron model with a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE4XV_DXDEQP"
   },
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz_gZNQVDN_w"
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u50xqE7yDRan"
   },
   "outputs": [],
   "source": [
    "# You will bound reviews at 500 words, truncating longer reviews and zero-padding shorter ones\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XZmw7y_DajU",
    "outputId": "eba61f01-f4af-4495-e969-b07d1c1adaa2"
   },
   "outputs": [],
   "source": [
    "# The model uses logarithmic loss and is optimized using the efficient ADAM optimization procedure\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXKl05g5Dfnz",
    "outputId": "1f04d6c9-47eb-44fa-9877-8ee8a171ab71"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6ORfnIEDvlj"
   },
   "source": [
    "# One-Dimensional Convolutional Neural Network Model for the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hj0CssmzDxwq"
   },
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWVYJLdBD3qq"
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXpeIDVoD5ql",
    "outputId": "62618fa2-9796-48cc-d3a9-5d8e224ee820"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O35RlxqjD7vC",
    "outputId": "855599e6-a93e-4280-dfb8-978e1a515442"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C5HFawxEDpz"
   },
   "source": [
    "There is a lot of opportunity for further optimization, such as using deeper and/or larger convolutional layers.\n",
    "\n",
    "Which one of MLP and CNN works better? Why?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
