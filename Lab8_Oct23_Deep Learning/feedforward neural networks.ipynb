{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "A feedforward neural network is a type of artificial neural network in which nodes' connections do not form a loop. CNN is a typical example of feedforward neural network.\n",
    "\n",
    "This notebook explores logistic regression and feedforward neural networks for binary text classification, using the pytorch library. \n",
    "\n",
    "## Recommended Preparation\n",
    "\n",
    "Before starting this tutorial it is recommended that you have installed PyTorch,\n",
    "and have a basic understanding of Tensors:\n",
    "\n",
    "-  For installation instructions: https://pytorch.org/ \n",
    "-  Get started with PyTorch in general and learn the basics of Tensors: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "-  For a wide and deep overview: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "Credits: I am using this notebook that I really like which explains and implements every step in feedforward neural networks. https://github.com/dbamman/anlp21/blob/main/9.neural/FFNN.ipynb\n",
    "\n",
    "## Data\n",
    "\n",
    "Included in the ``Datasets/large_movie_review_dataset`` directory are the big movie review dataset we've explored before, including train.tsv, dev.tsv, and test.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/87/08/4555e05425caca1ad362277a3c38960b40672601639e2cc0d330ba489386/torch-2.1.0-cp38-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.1.0-cp38-none-macosx_11_0_arm64.whl.metadata (24 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from torch) (4.7.1)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from jinja2->torch) (2.0.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torch-2.1.0-cp38-none-macosx_11_0_arm64.whl (59.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.12.4 fsspec-2023.10.0 mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, max_data_points=None):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "\n",
    "    # shuffle the data\n",
    "    tmp = list(zip(X, Y))\n",
    "    random.shuffle(tmp)\n",
    "    X, Y = zip(*tmp)\n",
    "    \n",
    "    if max_data_points == None:\n",
    "        return X, Y\n",
    "    \n",
    "    return X[:max_data_points], Y[:max_data_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"Datasets/large_movie_review_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll limit the training and dev data to 10,000 data points for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX, devY=read_data(\"%s/dev.tsv\" % directory, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll represent the data using simple binary indicators of the most frequent 1,000 words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000, analyzer=str.split, lowercase=True, strip_accents=None, binary=True)\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "X_dev = vectorizer.transform(devX)\n",
    "\n",
    "_,vocabSize=X_train.shape\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_Train and Y_Train:  (10000, 1000) (10000,)\n",
      "Shape of X_dev and Y_dev:  (5000, 1000) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_Train and Y_Train: \", X_train.shape, Y_train.shape)\n",
    "print(\"Shape of X_dev and Y_dev: \", X_dev.shape, Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segregate your data into list of lists containing the data\n",
    "\n",
    "def get_batches(x, y, batch_size=12):\n",
    "    batches_x=[]\n",
    "    batches_y=[]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        batches_x.append(x[i:i+batch_size])\n",
    "        batches_y.append(y[i:i+batch_size])\n",
    "    \n",
    "    return batches_x, batches_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the sparse matrices to dense matrices and then convert them to tensors for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_x, train_batches_y = get_batches(torch.from_numpy(X_train.todense()).float(), torch.LongTensor(Y_train))\n",
    "dev_batches_x, dev_batches_y = get_batches(torch.from_numpy(X_dev.todense()).float(), torch.LongTensor(Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([0, 0, 0,  ..., 1, 0, 1])\n",
      "834 12 834\n"
     ]
    }
   ],
   "source": [
    "# Try to print your X and Y values to understand step by step transformation of data that we will feed to the Model.\n",
    "#to see how your tensors look\n",
    "print(torch.from_numpy(X_train.todense()).float())\n",
    "print(torch.LongTensor(Y_train))\n",
    "print(len(train_batches_x), len(train_batches_x[0]), len(train_batches_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define our own accuracy_score like function\n",
    "def evaluate(model, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    corr = 0.\n",
    "    total = 0.\n",
    "    # Why do we need with torch no_grad?\n",
    "    # However, the with torch. no_grad() tells PyTorch to not calculate the gradients, \n",
    "    # and the program explicitly uses it here (as with most neural networks) in order to not update the gradients \n",
    "    # when it is updating the weights as that would affect the back propagation.\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x, y):\n",
    "            y_preds=model.forward(x)\n",
    "            for idx, y_pred in enumerate(y_preds):\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y[idx]:\n",
    "                    corr += 1.\n",
    "                total+=1                          \n",
    "    return corr/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about no_grad() here- https://pytorch.org/docs/stable/generated/torch.no_grad.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # torch.nn.Linear transforms an input of size input_dim (e.g., 1000 above) to an output of size output_dim \n",
    "        # (e.g., 2 classes for positive/negative)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, output_dim) \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.linear1(input)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_1_Hidden_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        \n",
    "#         hidden_dim=100\n",
    "        # the first layer transforms an input of size input_dim (e.g., 1,000 above) to an output of size hidden_dim (e.g., 100)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # the second layer transforms an input of size hidden_dim (e.g., 100) to an output of size output_dim (e.g., 2 classes for positive/negative)       \n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input): \n",
    "        # pass the input through the first layer\n",
    "        layer1_output = self.linear1(input)\n",
    "        \n",
    "        # pass the output through a non-linearity (here, tanh)\n",
    "        layer1_output = torch.tanh(layer1_output)\n",
    "        \n",
    "        # and then pass the output from that first layer as input to the second layer\n",
    "        layer2_output = self.linear2(layer1_output)\n",
    "\n",
    "        return layer2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_filename, train_batches_x, train_batches_y, dev_batches_x, dev_batches_y):\n",
    "    \n",
    "    # initializing the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    \n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "    best_dev_acc = 0.\n",
    "    \n",
    "    # we'll only train for 5 epochs for this exercise, but in practice you'd want to train for more epochs\n",
    "    # (in theory until you stop seeing improvements in accuracy on your *development* data)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in zip(train_batches_x, train_batches_y):\n",
    "            y_pred=model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, 2), y.view(-1))\n",
    "            losses.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        dev_accuracy=evaluate(model, dev_batches_x, dev_batches_y)\n",
    "        \n",
    "        # we're going to save the model that performs the best on *dev* data\n",
    "        if dev_accuracy > best_dev_acc:\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(\"%.3f is better than %.3f, saving model ...\" % (dev_accuracy, best_dev_acc))\n",
    "            best_dev_acc = dev_accuracy\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            \n",
    "    model.load_state_dict(torch.load(model_filename))            \n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
    "    return best_dev_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843 is better than 0.000, saving model ...\n",
      "Epoch 0, dev accuracy: 0.843\n",
      "0.850 is better than 0.843, saving model ...\n",
      "Epoch 1, dev accuracy: 0.850\n",
      "Epoch 2, dev accuracy: 0.847\n",
      "Epoch 3, dev accuracy: 0.849\n",
      "Epoch 4, dev accuracy: 0.848\n",
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.850\n"
     ]
    }
   ],
   "source": [
    "logreg=LogisticRegressionClassifier(1000, 2)\n",
    "dev_accuracy=train(logreg, \"logreg.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843 is better than 0.000, saving model ...\n",
      "Epoch 0, dev accuracy: 0.843\n",
      "Epoch 1, dev accuracy: 0.839\n",
      "Epoch 2, dev accuracy: 0.830\n",
      "Epoch 3, dev accuracy: 0.830\n",
      "Epoch 4, dev accuracy: 0.828\n",
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.843\n"
     ]
    }
   ],
   "source": [
    "ffnn1=FFNN_1_Hidden_Layer(1000, 2, hidden_dim=100)\n",
    "dev_accuracy=train(ffnn1, \"ffnn1.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks converge to different solutions as a function of their *initialization* (the random choice of the initial values for parameters).  Let's train the `FFNN_1_Hidden_Layer` model 10 times and then plot the distribution of dev accuracies using [pandas.DataFrame.plot.density](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_accuracies=[]\n",
    "\n",
    "for i in range(10):\n",
    "    ffnn1=FFNN_1_Hidden_Layer(1000, 2, hidden_dim=100)\n",
    "    dev_accuracy=train(ffnn1, \"ffnn1.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)\n",
    "    dev_accuracies.append(dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/seaborn-kdeplot\n",
    "df=pd.DataFrame(dev_accuracies)\n",
    "ax = df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding more layers to the FFNN below and experimenting with dropout rate, hidden layer sizes, and different choices of non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Experiment(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim=100\n",
    "        \n",
    "        # the first layer transforms an input of size input_dim (e.g., 1000 above) to an output of size hidden_dim (e.g., 100)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # a dropout layer randomly sets the output from the previous layer to 0 p% of the time\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # the second layer transforms an input of size hidden_dim (e.g., 100) to an output of size output_dim (e.g., 2 classes for positive/negative)       \n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input): \n",
    "        # pass the input through the first layer\n",
    "        layer1_output = self.linear1(input)\n",
    "        \n",
    "        # pass that output through a non-linearity (here, tanh)\n",
    "        # alternatives include torch.relu and torch.sigmoid\n",
    "        layer1_output = torch.tanh(layer1_output)\n",
    "        \n",
    "        # then dropout some outputs during training time (not test time)\n",
    "        layer1_output=self.dropout(layer1_output)\n",
    "        \n",
    "        # and then pass the output from that first layer as input to the second layer\n",
    "        layer2_output = self.linear2(layer1_output)\n",
    "\n",
    "        return layer2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845 is better than 0.000, saving model ...\n",
      "Epoch 0, dev accuracy: 0.845\n",
      "Epoch 1, dev accuracy: 0.842\n",
      "Epoch 2, dev accuracy: 0.839\n",
      "Epoch 3, dev accuracy: 0.833\n",
      "Epoch 4, dev accuracy: 0.831\n",
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.845\n"
     ]
    }
   ],
   "source": [
    "ffnn_e=FFNN_Experiment(1000, 2, hidden_dim=100)\n",
    "dev_accuracy=train(ffnn_e, \"ffnn_e.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
