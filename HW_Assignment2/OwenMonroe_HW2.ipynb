{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the Notebook for Owen Monroe's Homework Assigment 2 for IS-567 Text Mining\n",
    "##### September 23, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('chatgpt_train.csv')\n",
    "test_df = pd.read_csv('chatgpt_test.csv')\n",
    "train_df['review'][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834\n",
      "458\n"
     ]
    }
   ],
   "source": [
    "# Creates lists and reporting length\n",
    "\n",
    "tr_rev_list = train_df['review'].to_list()\n",
    "te_rev_list = test_df['review'].to_list()\n",
    "tr_rev_list\n",
    "te_rev_list\n",
    "print(len(tr_rev_list))\n",
    "print(len(te_rev_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829\n",
      "458\n"
     ]
    }
   ],
   "source": [
    "# Removing Empties and reporting length\n",
    "full_train_df = train_df.dropna()\n",
    "full_test_df = test_df.dropna()\n",
    "\n",
    "print(len(full_train_df['review'].to_list()))\n",
    "print(len(full_test_df['review'].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/38d5fhqx2_jf385d_b3cq3j00000gn/T/ipykernel_45660/3634727750.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_task2_df['NLTK Tokenized Reviews'] = train_task2_df['review'].apply(nltk.word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "# Copying training Data and POS tagging\n",
    "train_task2_df = full_train_df\n",
    "\n",
    "# Tokenizing the Reviews\n",
    "train_task2_df['NLTK Tokenized Reviews'] = train_task2_df['review'].apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library Download\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/38d5fhqx2_jf385d_b3cq3j00000gn/T/ipykernel_45660/121813578.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_task2_df['POS Tagged Tokens'] = train_task2_df['NLTK Tokenized Reviews'].apply(pos_tag)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>POS Tagged Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ChatGPT is an impressive AI language model tha...</td>\n",
       "      <td>[(ChatGPT, NNP), (is, VBZ), (an, DT), (impress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Though there is a problem that I had found mys...</td>\n",
       "      <td>[(Though, IN), (there, EX), (is, VBZ), (a, DT)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I downloaded the ChatGPT app with high hopes, ...</td>\n",
       "      <td>[(I, PRP), (downloaded, VBD), (the, DT), (Chat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  \\\n",
       "21  ChatGPT is an impressive AI language model tha...   \n",
       "22  Though there is a problem that I had found mys...   \n",
       "23  I downloaded the ChatGPT app with high hopes, ...   \n",
       "\n",
       "                                    POS Tagged Tokens  \n",
       "21  [(ChatGPT, NNP), (is, VBZ), (an, DT), (impress...  \n",
       "22  [(Though, IN), (there, EX), (is, VBZ), (a, DT)...  \n",
       "23  [(I, PRP), (downloaded, VBD), (the, DT), (Chat...  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tagging and Reporting\n",
    "\n",
    "train_task2_df['POS Tagged Tokens'] = train_task2_df['NLTK Tokenized Reviews'].apply(pos_tag)\n",
    "train_task2_df[['review','POS Tagged Tokens']][20:23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829, 5551) \n",
      "\n",
      "(458, 5551) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building Model\n",
    "train_text = full_train_df[\"review\"]\n",
    "test_text = full_test_df[\"review\"]\n",
    "\n",
    "# Setting n-gram to uni-grams\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# Creating training data representation\n",
    "train_data_cv = vectorizer.fit_transform(train_text.values.astype('U'))\n",
    "print(train_data_cv.shape,\"\\n\") \n",
    "\n",
    "# Creating test data representation\n",
    "test_data_cv = vectorizer.transform(test_text.values.astype('U'))\n",
    "print(test_data_cv.shape,\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 694)\t1\n",
      "  (0, 2885)\t1\n",
      "  (1, 2233)\t1\n",
      "  (1, 2278)\t1\n",
      "  (1, 3129)\t1\n",
      "  (1, 4898)\t1\n",
      "  (2, 1929)\t1\n",
      "  (2, 2666)\t1\n",
      "  (3, 2645)\t1\n",
      "  (3, 3193)\t1\n",
      "  (3, 5384)\t1\n",
      "  (4, 2170)\t1\n",
      "  (5, 291)\t1\n",
      "  (5, 2135)\t1\n",
      "  (5, 2660)\t1\n",
      "  (5, 3891)\t1\n",
      "  (5, 4932)\t1\n",
      "  (6, 1923)\t1\n",
      "  (7, 291)\t1\n",
      "  (7, 3311)\t1\n",
      "  (8, 2146)\t2\n",
      "  (8, 2660)\t1\n",
      "  (8, 3372)\t1\n",
      "  (8, 4932)\t1\n",
      "  (10, 776)\t1\n",
      "  :\t:\n",
      "  (455, 5437)\t1\n",
      "  (455, 5464)\t2\n",
      "  (455, 5466)\t1\n",
      "  (455, 5503)\t5\n",
      "  (456, 115)\t1\n",
      "  (456, 197)\t1\n",
      "  (456, 230)\t1\n",
      "  (456, 373)\t1\n",
      "  (456, 386)\t1\n",
      "  (456, 388)\t1\n",
      "  (456, 497)\t1\n",
      "  (456, 562)\t1\n",
      "  (456, 701)\t1\n",
      "  (456, 776)\t1\n",
      "  (456, 1435)\t1\n",
      "  (456, 1974)\t1\n",
      "  (456, 2059)\t1\n",
      "  (456, 3303)\t1\n",
      "  (456, 3341)\t2\n",
      "  (456, 4216)\t1\n",
      "  (456, 4507)\t1\n",
      "  (456, 4906)\t1\n",
      "  (456, 4985)\t1\n",
      "  (456, 5403)\t1\n",
      "  (456, 5464)\t1\n"
     ]
    }
   ],
   "source": [
    "print(test_data_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train and Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 3, 1, 1, 5, 5, 5, 1, 1, 5, 5, 5,\n",
       "       1, 5, 5, 1, 5, 5, 1, 5, 4, 5, 5, 5, 1, 5, 5, 5, 5, 5, 1, 3, 5, 5,\n",
       "       5, 1, 5, 5, 5, 5, 5, 5, 1, 5, 1, 5, 3, 4, 5, 5, 1, 5, 5, 1, 1, 5,\n",
       "       5, 5, 5, 1, 5, 4, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 3, 5, 5,\n",
       "       5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5,\n",
       "       1, 1, 1, 5, 1, 1, 5, 5, 5, 5, 5, 2, 2, 5, 3, 1, 5, 5, 5, 1, 5, 5,\n",
       "       5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 2, 1, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 1, 3, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 3, 5, 5, 5, 5,\n",
       "       1, 4, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5,\n",
       "       5, 1, 4, 5, 3, 5, 5, 5, 5, 4, 1, 5, 5, 1, 5, 5, 5, 1, 5, 5, 5, 1,\n",
       "       5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 1, 5, 5, 5, 5, 3, 1,\n",
       "       1, 3, 1, 1, 1, 1, 5, 5, 1, 5, 5, 4, 5, 1, 5, 1, 4, 1, 4, 5, 5, 1,\n",
       "       4, 4, 5, 1, 1, 1, 1, 1, 4, 1, 5, 4, 4, 5, 1, 1, 1, 1, 5, 5, 1, 4,\n",
       "       1, 1, 1, 5, 5, 1, 4, 1, 5, 1, 1, 4, 5, 1, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv = train_data_cv\n",
    "y_train_rev_series = full_train_df[\"rating\"]\n",
    "x_test_cv = test_data_cv\n",
    "y_test_rev_series = full_test_df[\"rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions = mnb_model.predict(x_test_cv)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6331877729257642\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Classifier\n",
    "# Accuracy Classification Score\n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list = list(predictions)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predict_label_list)\n",
    "print(acc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro precision score = 0.4365165675446049\n",
      "micro precision score = 0.6331877729257642\n",
      "weighted precision score = 0.5855492442277733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.56565657, 0.4       , 0.13333333, 0.38888889, 0.69470405])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision Score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "true_label_array = np.array(true_label_list)\n",
    "predict_label_array = np.array(predict_label_list)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_array, predict_label_array, average='macro')\n",
    "prec_score_micro = precision_score(true_label_array, predict_label_array, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_array, predict_label_array, average='weighted')\n",
    "\n",
    "print(f'macro precision score = {prec_score_macro}')\n",
    "print(f'micro precision score = {prec_score_micro}')\n",
    "print(f'weighted precision score = {prec_score_weighted}')\n",
    "prec_none_array = precision_score(true_label_array, predict_label_array, average=None)\n",
    "prec_none_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro recall score = 0.4365165675446049\n",
      "micro recall score = 0.6331877729257642\n",
      "weighted recall score = 0.5855492442277733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.48695652, 0.07692308, 0.08      , 0.15909091, 0.89919355])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rec_score_macro = recall_score(true_label_array, predict_label_array, average='macro')\n",
    "rec_score_micro = recall_score(true_label_array, predict_label_array, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_array, predict_label_array, average='weighted')\n",
    "\n",
    "print(f'macro recall score = {prec_score_macro}')\n",
    "print(f'micro recall score = {prec_score_micro}')\n",
    "print(f'weighted recall score = {prec_score_weighted}')\n",
    "rec_none_array = recall_score(true_label_array, predict_label_array, average=None)\n",
    "rec_none_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1 score = 0.35240689572225525\n",
      "micro F1 score = 0.6331877729257642\n",
      "weighted F1 score = 0.5903218267264796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.52336449, 0.12903226, 0.1       , 0.22580645, 0.78383128])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 Score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score_macro = f1_score(true_label_array, predict_label_array, average='macro')\n",
    "f1_score_micro = f1_score(true_label_array, predict_label_array, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_array, predict_label_array, average='weighted')\n",
    "\n",
    "print(f'macro F1 score = {f1_score_macro}')\n",
    "print(f'micro F1 score = {f1_score_micro}')\n",
    "print(f'weighted F1 score = {f1_score_weighted}')\n",
    "f1_none_array = f1_score(true_label_array, predict_label_array, average=None)\n",
    "f1_none_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 1\n",
      "\n",
      "Accuracy for label 1: 0.49\n",
      "\n",
      "macro precision score for label 1 = 0.25\n",
      "micro precision score for label 1 = 0.48695652173913045\n",
      "weighted precision score for label 1 = 1.0\n",
      "\n",
      "macro recall score for label 1 = 0.12173913043478261\n",
      "micro recall score for label 1 = 0.48695652173913045\n",
      "weighted recall score for label 1 = 0.48695652173913045\n",
      "\n",
      "macro F1 score for label 1 = 0.16374269005847952\n",
      "micro F1 score for label 1 = 0.48695652173913045\n",
      "weighted F1 score for label 1 = 0.6549707602339181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 1,  Unigrams (Chat-GPT Help)\n",
    "\n",
    "# Constructing Filter for Label == 1\n",
    "label_1 = 1 \n",
    "\n",
    "label_filter = [1 if label == label_1 else 0 for label in true_label_list]\n",
    "\n",
    "true_labels_for_label_1 = [true_label_list[i] for i, mask in enumerate(label_filter) if mask == 1]\n",
    "predicted_labels_for_lable_1 = [predict_label_list[i] for i, mask in enumerate(label_filter) if mask == 1]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_label_1 = accuracy_score(true_labels_for_label_1, predicted_labels_for_lable_1)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_label_1 = precision_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='macro')\n",
    "prec_score_micro_label_1 = precision_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='micro')\n",
    "prec_score_weighted_label_1 = precision_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_label_1 = recall_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='macro')\n",
    "rec_score_micro_label_1 = recall_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='micro')\n",
    "rec_score_weighted_label_1 = recall_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_label_1 = f1_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='macro')\n",
    "f1_score_micro_label_1 = f1_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='micro')\n",
    "f1_score_weighted_label_1 = f1_score(true_labels_for_label_1, predicted_labels_for_lable_1, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 1')\n",
    "print()\n",
    "print(f'Accuracy for label {label_1}: {accuracy_for_label_1:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {label_1} = {prec_score_macro_label_1}')\n",
    "print(f'micro precision score for label {label_1} = {prec_score_micro_label_1}')\n",
    "print(f'weighted precision score for label {label_1} = {prec_score_weighted_label_1}')\n",
    "print()\n",
    "print(f'macro recall score for label {label_1} = {rec_score_macro_label_1}')\n",
    "print(f'micro recall score for label {label_1} = {rec_score_micro_label_1}')\n",
    "print(f'weighted recall score for label {label_1} = {rec_score_weighted_label_1}')\n",
    "print()\n",
    "print(f'macro F1 score for label {label_1} = {f1_score_macro_label_1}')\n",
    "print(f'micro F1 score for label {label_1} = {f1_score_micro_label_1}')\n",
    "print(f'weighted F1 score for label {label_1} = {f1_score_weighted_label_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 2\n",
      "\n",
      "Accuracy for label 2: 0.08\n",
      "\n",
      "macro precision score for label 2 = 0.2\n",
      "micro precision score for label 2 = 0.07692307692307693\n",
      "weighted precision score for label 2 = 1.0\n",
      "\n",
      "macro recall score for label 2 = 0.34043281122804264\n",
      "micro recall score for label 2 = 0.6331877729257642\n",
      "weighted recall score for label 2 = 0.6331877729257642\n",
      "\n",
      "macro F1 score for label 2 = 0.028571428571428574\n",
      "micro F1 score for label 2 = 0.07692307692307693\n",
      "weighted F1 score for label 2 = 0.14285714285714288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 2, Unigrams (Chat-GPT Help)\n",
    "\n",
    "# Constructing Filter for Label == 2\n",
    "label_2 = 2 \n",
    "\n",
    "label_filter = [2 if label == label_2 else 0 for label in true_label_list]\n",
    "\n",
    "true_labels_for_label_2 = [true_label_list[i] for i, mask in enumerate(label_filter) if mask == 2]\n",
    "predicted_labels_for_lable_2 = [predict_label_list[i] for i, mask in enumerate(label_filter) if mask == 2]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_label_2 = accuracy_score(true_labels_for_label_2, predicted_labels_for_lable_2)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_label_2 = precision_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='macro')\n",
    "prec_score_micro_label_2 = precision_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='micro')\n",
    "prec_score_weighted_label_2 = precision_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_label_2 = recall_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='macro')\n",
    "rec_score_micro_label_2 = recall_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='micro')\n",
    "rec_score_weighted_label_2 = recall_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_label_2 = f1_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='macro')\n",
    "f1_score_micro_label_2 = f1_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='micro')\n",
    "f1_score_weighted_label_2 = f1_score(true_labels_for_label_2, predicted_labels_for_lable_2, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 2')\n",
    "print()\n",
    "print(f'Accuracy for label {label_2}: {accuracy_for_label_2:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {label_2} = {prec_score_macro_label_2}')\n",
    "print(f'micro precision score for label {label_2} = {prec_score_micro_label_2}')\n",
    "print(f'weighted precision score for label {label_2} = {prec_score_weighted_label_2}')\n",
    "print()\n",
    "print(f'macro recall score for label {label_2} = {rec_score_macro}')\n",
    "print(f'micro recall score for label {label_2} = {rec_score_micro}')\n",
    "print(f'weighted recall score for label {label_2} = {rec_score_weighted}')\n",
    "print()\n",
    "print(f'macro F1 score for label {label_2} = {f1_score_macro_label_2}')\n",
    "print(f'micro F1 score for label {label_2} = {f1_score_micro_label_2}')\n",
    "print(f'weighted F1 score for label {label_2} = {f1_score_weighted_label_2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 3\n",
      "\n",
      "Accuracy for label 3: 0.08\n",
      "\n",
      "macro precision score for label 3 = 0.2\n",
      "micro precision score for label 3 = 0.07692307692307693\n",
      "weighted precision score for label 3 = 1.0\n",
      "\n",
      "macro recall score for label 3 = 0.015384615384615385\n",
      "micro recall score for label 3 = 0.07692307692307693\n",
      "weighted recall score for label 3 = 0.07692307692307693\n",
      "\n",
      "macro F1 score for label 3 = 0.028571428571428574\n",
      "micro F1 score for label 3 = 0.07692307692307693\n",
      "weighted F1 score for label 3 = 0.14285714285714288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 3, Unigrams (Chat-GPT Help)\n",
    "\n",
    "# Constructing Filter for Label == 3\n",
    "label_3 = 3 \n",
    "\n",
    "label_filter = [3 if label == label_2 else 0 for label in true_label_list]\n",
    "\n",
    "true_labels_for_label_3 = [true_label_list[i] for i, mask in enumerate(label_filter) if mask == 3]\n",
    "predicted_labels_for_lable_3 = [predict_label_list[i] for i, mask in enumerate(label_filter) if mask == 3]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_label_3 = accuracy_score(true_labels_for_label_3, predicted_labels_for_lable_3)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_label_3 = precision_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='macro')\n",
    "prec_score_micro_label_3 = precision_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='micro')\n",
    "prec_score_weighted_label_3 = precision_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_label_3 = recall_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='macro')\n",
    "rec_score_micro_label_3 = recall_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='micro')\n",
    "rec_score_weighted_label_3 = recall_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_label_3 = f1_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='macro')\n",
    "f1_score_micro_label_3 = f1_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='micro')\n",
    "f1_score_weighted_label_3 = f1_score(true_labels_for_label_3, predicted_labels_for_lable_3, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 3')\n",
    "print()\n",
    "print(f'Accuracy for label {label_3}: {accuracy_for_label_3:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {label_3} = {prec_score_macro_label_3}')\n",
    "print(f'micro precision score for label {label_3} = {prec_score_micro_label_3}')\n",
    "print(f'weighted precision score for label {label_3} = {prec_score_weighted_label_3}')\n",
    "print()\n",
    "print(f'macro recall score for label {label_3} = {rec_score_macro_label_3}')\n",
    "print(f'micro recall score for label {label_3} = {rec_score_micro_label_3}')\n",
    "print(f'weighted recall score for label {label_3} = {rec_score_weighted_label_3}')\n",
    "print()\n",
    "print(f'macro F1 score for label {label_3} = {f1_score_macro_label_3}')\n",
    "print(f'micro F1 score for label {label_3} = {f1_score_micro_label_3}')\n",
    "print(f'weighted F1 score for label {label_3} = {f1_score_weighted_label_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 4\n",
      "\n",
      "Accuracy for label 4: 0.08\n",
      "\n",
      "macro precision score for label 4 = 0.2\n",
      "micro precision score for label 4 = 0.07692307692307693\n",
      "weighted precision score for label 4 = 1.0\n",
      "\n",
      "macro recall score for label 4 = 0.015384615384615385\n",
      "micro recall score for label 4 = 0.07692307692307693\n",
      "weighted recall score for label 4 = 0.07692307692307693\n",
      "\n",
      "macro F1 score for label 4 = 0.028571428571428574\n",
      "micro F1 score for label 4 = 0.07692307692307693\n",
      "weighted F1 score for label 4 = 0.14285714285714288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 4, Unigrams (Chat-GPT Help)\n",
    "\n",
    "# Constructing Filter for Label == 4\n",
    "label_4 = 4 \n",
    "\n",
    "label_filter = [4 if label == label_2 else 0 for label in true_label_list]\n",
    "\n",
    "true_labels_for_label_4 = [true_label_list[i] for i, mask in enumerate(label_filter) if mask == 4]\n",
    "predicted_labels_for_lable_4 = [predict_label_list[i] for i, mask in enumerate(label_filter) if mask == 4]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_label_4 = accuracy_score(true_labels_for_label_4, predicted_labels_for_lable_4)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_label_4 = precision_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='macro')\n",
    "prec_score_micro_label_4 = precision_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='micro')\n",
    "prec_score_weighted_label_4 = precision_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_label_4 = recall_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='macro')\n",
    "rec_score_micro_label_4 = recall_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='micro')\n",
    "rec_score_weighted_label_4 = recall_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_label_4 = f1_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='macro')\n",
    "f1_score_micro_label_4 = f1_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='micro')\n",
    "f1_score_weighted_label_4 = f1_score(true_labels_for_label_4, predicted_labels_for_lable_4, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 4')\n",
    "print()\n",
    "print(f'Accuracy for label {label_4}: {accuracy_for_label_4:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {label_4} = {prec_score_macro_label_4}')\n",
    "print(f'micro precision score for label {label_4} = {prec_score_micro_label_4}')\n",
    "print(f'weighted precision score for label {label_4} = {prec_score_weighted_label_4}')\n",
    "print()\n",
    "print(f'macro recall score for label {label_4} = {rec_score_macro_label_4}')\n",
    "print(f'micro recall score for label {label_4} = {rec_score_micro_label_4}')\n",
    "print(f'weighted recall score for label {label_4} = {rec_score_weighted_label_4}')\n",
    "print()\n",
    "print(f'macro F1 score for label {label_4} = {f1_score_macro_label_4}')\n",
    "print(f'micro F1 score for label {label_4} = {f1_score_micro_label_4}')\n",
    "print(f'weighted F1 score for label {label_4} = {f1_score_weighted_label_4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 5\n",
      "\n",
      "Accuracy for label 5: 0.08\n",
      "\n",
      "macro precision score for label 5 = 0.2\n",
      "micro precision score for label 5 = 0.07692307692307693\n",
      "weighted precision score for label 5 = 1.0\n",
      "\n",
      "macro recall score for label 5 = 0.015384615384615385\n",
      "micro recall score for label 5 = 0.07692307692307693\n",
      "weighted recall score for label 5 = 0.07692307692307693\n",
      "\n",
      "macro F1 score for label 5 = 0.028571428571428574\n",
      "micro F1 score for label 5 = 0.07692307692307693\n",
      "weighted F1 score for label 5 = 0.14285714285714288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 5, Unigrams (Chat-GPT Help)\n",
    "\n",
    "# Constructing Filter for Label == 5\n",
    "label_5 = 5\n",
    "\n",
    "label_filter = [5 if label == label_2 else 0 for label in true_label_list]\n",
    "\n",
    "true_labels_for_label_5 = [true_label_list[i] for i, mask in enumerate(label_filter) if mask == 5]\n",
    "predicted_labels_for_lable_5 = [predict_label_list[i] for i, mask in enumerate(label_filter) if mask == 5]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_label_5 = accuracy_score(true_labels_for_label_5, predicted_labels_for_lable_5)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_label_5 = precision_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='macro')\n",
    "prec_score_micro_label_5 = precision_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='micro')\n",
    "prec_score_weighted_label_5 = precision_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_label_5 = recall_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='macro')\n",
    "rec_score_micro_label_5 = recall_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='micro')\n",
    "rec_score_weighted_label_5 = recall_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_label_5= f1_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='macro')\n",
    "f1_score_micro_label_5 = f1_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='micro')\n",
    "f1_score_weighted_label_5 = f1_score(true_labels_for_label_5, predicted_labels_for_lable_5, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 5')\n",
    "print()\n",
    "print(f'Accuracy for label {label_5}: {accuracy_for_label_4:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {label_5} = {prec_score_macro_label_5}')\n",
    "print(f'micro precision score for label {label_5} = {prec_score_micro_label_5}')\n",
    "print(f'weighted precision score for label {label_5} = {prec_score_weighted_label_5}')\n",
    "print()\n",
    "print(f'macro recall score for label {label_5} = {rec_score_macro_label_5}')\n",
    "print(f'micro recall score for label {label_5} = {rec_score_micro_label_5}')\n",
    "print(f'weighted recall score for label {label_5} = {rec_score_weighted_label_5}')\n",
    "print()\n",
    "print(f'macro F1 score for label {label_5} = {f1_score_macro_label_5}')\n",
    "print(f'micro F1 score for label {label_5} = {f1_score_micro_label_5}')\n",
    "print(f'weighted F1 score for label {label_5} = {f1_score_weighted_label_5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Add Bi-grams Features (Continue to Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829, 41006) \n",
      "\n",
      "(458, 41006) \n",
      "\n",
      "  (0, 37347)\t1\n",
      "  (0, 35638)\t9\n",
      "  (0, 35036)\t5\n",
      "  (0, 26826)\t1\n",
      "  (0, 38075)\t1\n",
      "  (0, 22562)\t1\n",
      "  (0, 5228)\t2\n",
      "  (0, 37876)\t2\n",
      "  (0, 7119)\t2\n",
      "  (0, 24675)\t3\n",
      "  (0, 22722)\t2\n",
      "  (0, 39767)\t1\n",
      "  (0, 9651)\t3\n",
      "  (0, 14656)\t1\n",
      "  (0, 7449)\t1\n",
      "  (0, 39475)\t1\n",
      "  (0, 18831)\t6\n",
      "  (0, 10297)\t1\n",
      "  (0, 29943)\t3\n",
      "  (0, 28240)\t2\n",
      "  (0, 23095)\t1\n",
      "  (0, 18313)\t3\n",
      "  (0, 27163)\t2\n",
      "  (0, 10007)\t1\n",
      "  (0, 33738)\t16\n",
      "  :\t:\n",
      "  (1826, 16885)\t1\n",
      "  (1827, 18831)\t1\n",
      "  (1827, 4887)\t1\n",
      "  (1827, 39808)\t1\n",
      "  (1827, 17933)\t1\n",
      "  (1827, 25276)\t1\n",
      "  (1827, 13528)\t1\n",
      "  (1827, 4958)\t1\n",
      "  (1827, 20432)\t1\n",
      "  (1827, 7401)\t1\n",
      "  (1827, 1099)\t1\n",
      "  (1827, 27219)\t1\n",
      "  (1827, 16051)\t1\n",
      "  (1827, 18845)\t1\n",
      "  (1827, 1100)\t1\n",
      "  (1827, 39925)\t1\n",
      "  (1827, 20433)\t1\n",
      "  (1827, 7402)\t1\n",
      "  (1827, 13529)\t1\n",
      "  (1827, 17971)\t1\n",
      "  (1827, 27220)\t1\n",
      "  (1827, 25369)\t1\n",
      "  (1828, 18831)\t1\n",
      "  (1828, 14581)\t1\n",
      "  (1828, 19049)\t1\n"
     ]
    }
   ],
   "source": [
    "# Extracting Bi-gram features \n",
    "\n",
    "# Building Model\n",
    "train_text = full_train_df[\"review\"]\n",
    "test_text = full_test_df[\"review\"]\n",
    "\n",
    "# Setting n-gram range to uni-grams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "# Creating training data representation\n",
    "train_data_bigram_cv = vectorizer.fit_transform(train_text.values.astype('U'))\n",
    "print(train_data_bigram_cv.shape,\"\\n\") \n",
    "\n",
    "# Creating test data representation\n",
    "test_data_bigram_cv = vectorizer.transform(test_text.values.astype('U'))\n",
    "print(test_data_bigram_cv.shape,\"\\n\") \n",
    "\n",
    "print(train_data_bigram_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 3, 1, 1, 4, 5, 5, 5, 1, 5, 5, 5,\n",
       "       1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 1, 5, 1, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 1, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 3, 4, 5, 5, 1, 5, 5, 4, 1, 5,\n",
       "       5, 5, 5, 1, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5,\n",
       "       1, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 4, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 4, 1, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 1, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5,\n",
       "       5, 1, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 1, 5, 1, 1, 5, 1, 1, 1, 1, 1, 1, 5, 5, 5, 1, 5, 5, 5, 5, 1, 1,\n",
       "       1, 1, 1, 1, 5, 1, 5, 5, 1, 5, 5, 5, 1, 1, 5, 1, 4, 1, 4, 5, 5, 1,\n",
       "       5, 5, 5, 1, 1, 1, 1, 1, 5, 1, 5, 5, 5, 5, 1, 1, 1, 1, 5, 5, 1, 5,\n",
       "       1, 1, 5, 5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a New Naive Bayes Classifier\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_bi_cv = train_data_bigram_cv\n",
    "y_train_rev_series = full_train_df[\"rating\"]\n",
    "x_test_bi_cv = test_data_bigram_cv\n",
    "y_test_rev_series = full_test_df[\"rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_bi_cv, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "bi_predictions = mnb_model.predict(x_test_bi_cv)\n",
    "bi_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for unigrams + bigrams:  0.6244541484716157\n",
      "Precision macro score for unigrams + bigrams:  0.27141238135713275\n",
      "Recall macro score for unigrams + bigrams:  0.28008542649496365\n",
      "F1 macro score for unigrams + bigrams:  0.26332604636538254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Simpler Model Evaluation: Unigrams + Bigrams\n",
    "\n",
    "true_bi_label_list = y_test_rev_series.to_list()\n",
    "predict_bi_label_list = list(bi_predictions)\n",
    "\n",
    "print (\"Accuracy score for unigrams + bigrams: \", accuracy_score(true_bi_label_list, bi_predictions))\n",
    "print (\"Precision macro score for unigrams + bigrams: \", precision_score(true_bi_label_list, predict_bi_label_list, average= 'macro'))\n",
    "print (\"Recall macro score for unigrams + bigrams: \", recall_score(true_bi_label_list, predict_bi_label_list, average= 'macro'))\n",
    "print (\"F1 macro score for unigrams + bigrams: \", f1_score(true_bi_label_list, predict_bi_label_list, average= 'macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 1 - unigrams + bigrams\n",
      "\n",
      "Accuracy for label 1: 0.43\n",
      "\n",
      "macro precision score for label 1 = 0.25\n",
      "micro precision score for label 1 = 0.4260869565217391\n",
      "weighted precision score for label 1 = 1.0\n",
      "\n",
      "macro recall score for label 1 = 0.10652173913043478\n",
      "micro recall score for label 1 = 0.4260869565217391\n",
      "weighted recall score for label 1 = 0.4260869565217391\n",
      "\n",
      "macro F1 score for label 1 = 0.14939024390243902\n",
      "micro F1 score for label 1 = 0.4260869565217391\n",
      "weighted F1 score for label 1 = 0.5975609756097561\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 1,  Unigrams + Bigrams \n",
    "\n",
    "# Constructing Filter for Label == 1\n",
    "bi_label_1 = 1 \n",
    "\n",
    "bi_label_filter = [1 if label == bi_label_1 else 0 for label in true_bi_label_list]\n",
    "\n",
    "true_bi_labels_for_label_1 = [true_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 1]\n",
    "predicted_bi_labels_for_lable_1 = [predict_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 1]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_bi_label_1 = accuracy_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_bi_label_1 = precision_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='macro')\n",
    "prec_score_micro_bi_label_1 = precision_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='micro')\n",
    "prec_score_weighted_bi_label_1 = precision_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_bi_label_1 = recall_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='macro')\n",
    "rec_score_micro_bi_label_1 = recall_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='micro')\n",
    "rec_score_weighted_bi_label_1 = recall_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_bi_label_1 = f1_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='macro')\n",
    "f1_score_micro_bi_label_1 = f1_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='micro')\n",
    "f1_score_weighted_bi_label_1 = f1_score(true_bi_labels_for_label_1, predicted_bi_labels_for_lable_1, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 1 - unigrams + bigrams')\n",
    "print()\n",
    "print(f'Accuracy for label {bi_label_1}: {accuracy_for_bi_label_1:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {bi_label_1} = {prec_score_macro_bi_label_1}')\n",
    "print(f'micro precision score for label {bi_label_1} = {prec_score_micro_bi_label_1}')\n",
    "print(f'weighted precision score for label {bi_label_1} = {prec_score_weighted_bi_label_1}')\n",
    "print()\n",
    "print(f'macro recall score for label {bi_label_1} = {rec_score_macro_bi_label_1}')\n",
    "print(f'micro recall score for label {bi_label_1} = {rec_score_micro_bi_label_1}')\n",
    "print(f'weighted recall score for label {bi_label_1} = {rec_score_weighted_bi_label_1}')\n",
    "print()\n",
    "print(f'macro F1 score for label {bi_label_1} = {f1_score_macro_bi_label_1}')\n",
    "print(f'micro F1 score for label {bi_label_1} = {f1_score_micro_bi_label_1}')\n",
    "print(f'weighted F1 score for label {bi_label_1} = {f1_score_weighted_bi_label_1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 2 - unigrams + bigrams\n",
      "\n",
      "Accuracy for label 2: 0.00\n",
      "\n",
      "zero division precision score for label 2 = [0. 1. 0. 0. 0.]\n",
      "macro precision score for label 2 = 0.0\n",
      "micro precision score for label 2 = 0.0\n",
      "weighted precision score for label 2 = 0.0\n",
      "\n",
      "zero division recall score for label 2 = [1. 0. 1. 1. 1.]\n",
      "macro recall score for label 2 = 0.0\n",
      "micro recall score for label 2 = 0.0\n",
      "weighted recall score for label 2 = 0.0\n",
      "\n",
      "zero division F1 score for label 2 = [0. 0. 0. 0. 0.]\n",
      "macro F1 score for label 2 = 0.0\n",
      "micro F1 score for label 2 = 0.0\n",
      "weighted F1 score for label 2 = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 2,  Unigrams + Bigrams \n",
    "\n",
    "# Constructing Filter for Label == 2\n",
    "bi_label_2 = 2 \n",
    "\n",
    "bi_label_filter = [2 if label == bi_label_2 else 0 for label in true_bi_label_list]\n",
    "\n",
    "true_bi_labels_for_label_2 = [true_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 2]\n",
    "predicted_bi_labels_for_lable_2 = [predict_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 2]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_bi_label_2 = accuracy_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_zero_bi_label_2 = precision_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average=None, zero_division=1)\n",
    "prec_score_macro_bi_label_2 = precision_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='macro')\n",
    "prec_score_micro_bi_label_2 = precision_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='micro')\n",
    "prec_score_weighted_bi_label_2 = precision_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_zero_bi_label_2 = recall_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average=None, zero_division= 1)\n",
    "rec_score_macro_bi_label_2 = recall_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='macro')\n",
    "rec_score_micro_bi_label_2 = recall_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='micro')\n",
    "rec_score_weighted_bi_label_2 = recall_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_zero_bi_label_2 = f1_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average=None, zero_division=1)\n",
    "f1_score_macro_bi_label_2 = f1_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='macro')\n",
    "f1_score_micro_bi_label_2 = f1_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='micro')\n",
    "f1_score_weighted_bi_label_2 = f1_score(true_bi_labels_for_label_2, predicted_bi_labels_for_lable_2, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 2 - unigrams + bigrams')\n",
    "print()\n",
    "print(f'Accuracy for label {bi_label_2}: {accuracy_for_bi_label_2:.2f}')\n",
    "print()\n",
    "print(f'zero division precision score for label {bi_label_2} = {prec_score_zero_bi_label_2}')\n",
    "print(f'macro precision score for label {bi_label_2} = {prec_score_macro_bi_label_2}')\n",
    "print(f'micro precision score for label {bi_label_2} = {prec_score_micro_bi_label_2}')\n",
    "print(f'weighted precision score for label {bi_label_2} = {prec_score_weighted_bi_label_2}')\n",
    "print()\n",
    "print(f'zero division recall score for label {bi_label_2} = {rec_score_zero_bi_label_2}')\n",
    "print(f'macro recall score for label {bi_label_2} = {rec_score_macro_bi_label_2}')\n",
    "print(f'micro recall score for label {bi_label_2} = {rec_score_micro_bi_label_2}')\n",
    "print(f'weighted recall score for label {bi_label_2} = {rec_score_weighted_bi_label_2}')\n",
    "print()\n",
    "print(f'zero division F1 score for label {bi_label_2} = {f1_score_zero_bi_label_2}')\n",
    "print(f'macro F1 score for label {bi_label_2} = {f1_score_macro_bi_label_2}')\n",
    "print(f'micro F1 score for label {bi_label_2} = {f1_score_micro_bi_label_2}')\n",
    "print(f'weighted F1 score for label {bi_label_2} = {f1_score_weighted_bi_label_2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 3 - unigrams + bigrams\n",
      "\n",
      "Accuracy for label 3: 0.00\n",
      "\n",
      "macro precision score for label 3 = 0.0\n",
      "micro precision score for label 3 = 0.0\n",
      "weighted precision score for label 3 = 0.0\n",
      "\n",
      "macro recall score for label 3 = 0.0\n",
      "micro recall score for label 3 = 0.0\n",
      "weighted recall score for label 3 = 0.0\n",
      "\n",
      "macro F1 score for label 3 = 0.0\n",
      "micro F1 score for label 3 = 0.0\n",
      "weighted F1 score for label 3 = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 3,  Unigrams + Bigrams \n",
    "\n",
    "# Constructing Filter for Label == 3\n",
    "bi_label_3 = 3\n",
    "\n",
    "bi_label_filter = [3 if label == bi_label_3 else 0 for label in true_bi_label_list]\n",
    "\n",
    "true_bi_labels_for_label_3 = [true_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 3]\n",
    "predicted_bi_labels_for_lable_3 = [predict_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 3]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_bi_label_3 = accuracy_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_bi_label_3 = precision_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='macro')\n",
    "prec_score_micro_bi_label_3 = precision_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='micro')\n",
    "prec_score_weighted_bi_label_3 = precision_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_bi_label_3 = recall_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='macro')\n",
    "rec_score_micro_bi_label_3 = recall_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='micro')\n",
    "rec_score_weighted_bi_label_3 = recall_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_bi_label_3 = f1_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='macro')\n",
    "f1_score_micro_bi_label_3 = f1_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='micro')\n",
    "f1_score_weighted_bi_label_3 = f1_score(true_bi_labels_for_label_3, predicted_bi_labels_for_lable_3, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 3 - unigrams + bigrams')\n",
    "print()\n",
    "print(f'Accuracy for label {bi_label_3}: {accuracy_for_bi_label_3:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {bi_label_3} = {prec_score_macro_bi_label_3}')\n",
    "print(f'micro precision score for label {bi_label_3} = {prec_score_micro_bi_label_3}')\n",
    "print(f'weighted precision score for label {bi_label_3} = {prec_score_weighted_bi_label_3}')\n",
    "print()\n",
    "print(f'macro recall score for label {bi_label_3} = {rec_score_macro_bi_label_3}')\n",
    "print(f'micro recall score for label {bi_label_3} = {rec_score_micro_bi_label_3}')\n",
    "print(f'weighted recall score for label {bi_label_3} = {rec_score_weighted_bi_label_3}')\n",
    "print()\n",
    "print(f'macro F1 score for label {bi_label_3} = {f1_score_macro_bi_label_3}')\n",
    "print(f'micro F1 score for label {bi_label_3} = {f1_score_micro_bi_label_3}')\n",
    "print(f'weighted F1 score for label {bi_label_3} = {f1_score_weighted_bi_label_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 4 - unigrams + bigrams\n",
      "\n",
      "Accuracy for label 4: 0.02\n",
      "\n",
      "macro precision score for label 4 = 0.25\n",
      "micro precision score for label 4 = 0.022727272727272728\n",
      "weighted precision score for label 4 = 1.0\n",
      "\n",
      "macro recall score for label 4 = 0.005681818181818182\n",
      "micro recall score for label 4 = 0.022727272727272728\n",
      "weighted recall score for label 4 = 0.022727272727272728\n",
      "\n",
      "macro F1 score for label 4 = 0.011111111111111112\n",
      "micro F1 score for label 4 = 0.022727272727272728\n",
      "weighted F1 score for label 4 = 0.044444444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 4,  Unigrams + Bigrams \n",
    "\n",
    "# Constructing Filter for Label == 4\n",
    "bi_label_4 = 4\n",
    "\n",
    "bi_label_filter = [4 if label == bi_label_4 else 0 for label in true_bi_label_list]\n",
    "\n",
    "true_bi_labels_for_label_4 = [true_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 4]\n",
    "predicted_bi_labels_for_lable_4 = [predict_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 4]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_bi_label_4 = accuracy_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_bi_label_4 = precision_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='macro')\n",
    "prec_score_micro_bi_label_4 = precision_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='micro')\n",
    "prec_score_weighted_bi_label_4 = precision_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_bi_label_4 = recall_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='macro')\n",
    "rec_score_micro_bi_label_4 = recall_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='micro')\n",
    "rec_score_weighted_bi_label_4 = recall_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_bi_label_4 = f1_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='macro')\n",
    "f1_score_micro_bi_label_4 = f1_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='micro')\n",
    "f1_score_weighted_bi_label_4 = f1_score(true_bi_labels_for_label_4, predicted_bi_labels_for_lable_4, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 4 - unigrams + bigrams')\n",
    "print()\n",
    "print(f'Accuracy for label {bi_label_4}: {accuracy_for_bi_label_4:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {bi_label_4} = {prec_score_macro_bi_label_4}')\n",
    "print(f'micro precision score for label {bi_label_4} = {prec_score_micro_bi_label_4}')\n",
    "print(f'weighted precision score for label {bi_label_4} = {prec_score_weighted_bi_label_4}')\n",
    "print()\n",
    "print(f'macro recall score for label {bi_label_4} = {rec_score_macro_bi_label_4}')\n",
    "print(f'micro recall score for label {bi_label_4} = {rec_score_micro_bi_label_4}')\n",
    "print(f'weighted recall score for label {bi_label_4} = {rec_score_weighted_bi_label_4}')\n",
    "print()\n",
    "print(f'macro F1 score for label {bi_label_4} = {f1_score_macro_bi_label_4}')\n",
    "print(f'micro F1 score for label {bi_label_4} = {f1_score_micro_bi_label_4}')\n",
    "print(f'weighted F1 score for label {bi_label_4} = {f1_score_weighted_bi_label_4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting Scores for Label 5 - unigrams + bigrams\n",
      "\n",
      "Accuracy for label 5: 0.02\n",
      "\n",
      "macro precision score for label 5 = 0.3333333333333333\n",
      "micro precision score for label 5 = 0.9516129032258065\n",
      "weighted precision score for label 5 = 1.0\n",
      "\n",
      "macro recall score for label 5 = 0.3172043010752688\n",
      "micro recall score for label 5 = 0.9516129032258065\n",
      "weighted recall score for label 5 = 0.9516129032258065\n",
      "\n",
      "macro F1 score for label 5 = 0.325068870523416\n",
      "micro F1 score for label 5 = 0.9516129032258065\n",
      "weighted F1 score for label 5 = 0.9752066115702479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Labels: 5,  Unigrams + Bigrams \n",
    "\n",
    "# Constructing Filter for Label == 5\n",
    "bi_label_5 = 5\n",
    "\n",
    "bi_label_filter = [5 if label == bi_label_5 else 0 for label in true_bi_label_list]\n",
    "\n",
    "true_bi_labels_for_label_5 = [true_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 5]\n",
    "predicted_bi_labels_for_lable_5 = [predict_bi_label_list[i] for i, mask in enumerate(bi_label_filter) if mask == 5]\n",
    "\n",
    "# Accuracy Scoring for New List\n",
    "accuracy_for_bi_label_5 = accuracy_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5)\n",
    "\n",
    "# Precision Scoring for New List\n",
    "prec_score_macro_bi_label_5 = precision_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='macro')\n",
    "prec_score_micro_bi_label_5 = precision_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='micro')\n",
    "prec_score_weighted_bi_label_5 = precision_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='weighted')\n",
    "\n",
    "# Recall Scoring for New List\n",
    "rec_score_macro_bi_label_5 = recall_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='macro')\n",
    "rec_score_micro_bi_label_5 = recall_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='micro')\n",
    "rec_score_weighted_bi_label_5 = recall_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='weighted')\n",
    "\n",
    "#F1 Scoring for New List\n",
    "f1_score_macro_bi_label_5 = f1_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='macro')\n",
    "f1_score_micro_bi_label_5 = f1_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='micro')\n",
    "f1_score_weighted_bi_label_5 = f1_score(true_bi_labels_for_label_5, predicted_bi_labels_for_lable_5, average='weighted')\n",
    "\n",
    "\n",
    "# Reporting All \n",
    "print('Reporting Scores for Label 5 - unigrams + bigrams')\n",
    "print()\n",
    "print(f'Accuracy for label {bi_label_5}: {accuracy_for_bi_label_4:.2f}')\n",
    "print()\n",
    "print(f'macro precision score for label {bi_label_5} = {prec_score_macro_bi_label_5}')\n",
    "print(f'micro precision score for label {bi_label_5} = {prec_score_micro_bi_label_5}')\n",
    "print(f'weighted precision score for label {bi_label_5} = {prec_score_weighted_bi_label_5}')\n",
    "print()\n",
    "print(f'macro recall score for label {bi_label_5} = {rec_score_macro_bi_label_5}')\n",
    "print(f'micro recall score for label {bi_label_5} = {rec_score_micro_bi_label_5}')\n",
    "print(f'weighted recall score for label {bi_label_5} = {rec_score_weighted_bi_label_5}')\n",
    "print()\n",
    "print(f'macro F1 score for label {bi_label_5} = {f1_score_macro_bi_label_5}')\n",
    "print(f'micro F1 score for label {bi_label_5} = {f1_score_micro_bi_label_5}')\n",
    "print(f'weighted F1 score for label {bi_label_5} = {f1_score_weighted_bi_label_5}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Adding Tri-gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829, 97367) \n",
      "\n",
      "(458, 97367) \n",
      "\n",
      "  (0, 88181)\t1\n",
      "  (0, 83615)\t9\n",
      "  (0, 81998)\t5\n",
      "  (0, 63324)\t1\n",
      "  (0, 90083)\t1\n",
      "  (0, 53192)\t1\n",
      "  (0, 13161)\t2\n",
      "  (0, 89621)\t2\n",
      "  (0, 17504)\t2\n",
      "  (0, 58432)\t3\n",
      "  (0, 53540)\t2\n",
      "  (0, 94178)\t1\n",
      "  (0, 22813)\t3\n",
      "  (0, 33804)\t1\n",
      "  (0, 18396)\t1\n",
      "  (0, 93506)\t1\n",
      "  (0, 44026)\t6\n",
      "  (0, 24127)\t1\n",
      "  (0, 69557)\t3\n",
      "  (0, 66160)\t2\n",
      "  (0, 54558)\t1\n",
      "  (0, 42397)\t3\n",
      "  (0, 63984)\t2\n",
      "  (0, 23554)\t1\n",
      "  (0, 77753)\t16\n",
      "  :\t:\n",
      "  (1827, 18310)\t1\n",
      "  (1827, 2378)\t1\n",
      "  (1827, 64100)\t1\n",
      "  (1827, 37079)\t1\n",
      "  (1827, 44056)\t1\n",
      "  (1827, 2379)\t1\n",
      "  (1827, 94612)\t1\n",
      "  (1827, 48397)\t1\n",
      "  (1827, 18311)\t1\n",
      "  (1827, 31273)\t1\n",
      "  (1827, 41524)\t1\n",
      "  (1827, 64101)\t1\n",
      "  (1827, 60184)\t1\n",
      "  (1827, 44057)\t1\n",
      "  (1827, 2380)\t1\n",
      "  (1827, 94613)\t1\n",
      "  (1827, 48398)\t1\n",
      "  (1827, 18312)\t1\n",
      "  (1827, 12511)\t1\n",
      "  (1827, 31274)\t1\n",
      "  (1827, 41525)\t1\n",
      "  (1827, 64102)\t1\n",
      "  (1828, 44026)\t1\n",
      "  (1828, 33615)\t1\n",
      "  (1828, 44749)\t1\n"
     ]
    }
   ],
   "source": [
    "# Extracting Tri-gram features \n",
    "\n",
    "# Building Model\n",
    "train_text = full_train_df[\"review\"]\n",
    "test_text = full_test_df[\"review\"]\n",
    "\n",
    "# Setting n-gram range to uni-grams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 3))\n",
    "\n",
    "# Creating training data representation\n",
    "train_data_trigram_cv = vectorizer.fit_transform(train_text.values.astype('U'))\n",
    "print(train_data_trigram_cv.shape,\"\\n\") \n",
    "\n",
    "# Creating test data representation\n",
    "test_data_trigram_cv = vectorizer.transform(test_text.values.astype('U'))\n",
    "print(test_data_trigram_cv.shape,\"\\n\") \n",
    "\n",
    "print(train_data_trigram_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 3, 1, 1, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 1, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 1, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 3, 4, 5, 5, 1, 5, 5, 4, 1, 5,\n",
       "       5, 5, 5, 1, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5,\n",
       "       1, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 4, 1, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5,\n",
       "       5, 1, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 1, 5, 1, 1, 5, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1,\n",
       "       1, 5, 1, 1, 5, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 1, 4, 1, 4, 5, 5, 1,\n",
       "       5, 5, 5, 1, 1, 1, 1, 1, 5, 1, 5, 5, 5, 5, 1, 1, 1, 5, 5, 5, 1, 5,\n",
       "       1, 1, 5, 5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training another New Naive Bayes Classifier\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_tri_cv = train_data_trigram_cv\n",
    "y_train_rev_series = full_train_df[\"rating\"]\n",
    "x_test_tri_cv = test_data_trigram_cv\n",
    "y_test_rev_series = full_test_df[\"rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_tri_cv, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "tri_predictions = mnb_model.predict(x_test_tri_cv)\n",
    "tri_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for unigrams + bigrams:  0.6244541484716157\n",
      "Precision macro score for unigrams + bigrams:  0.27141238135713275\n",
      "Recall macro score for unigrams + bigrams:  0.28008542649496365\n",
      "F1 macro score for unigrams + bigrams:  0.26332604636538254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Simpler Model Evaluation: Unigrams + Bigrams + Trigrams\n",
    "\n",
    "true_tri_label_list = y_test_rev_series.to_list()\n",
    "predict_tri_label_list = list(tri_predictions)\n",
    "\n",
    "print (\"Accuracy score for unigrams + bigrams: \", accuracy_score(true_bi_label_list, bi_predictions))\n",
    "print (\"Precision macro score for unigrams + bigrams: \", precision_score(true_bi_label_list, predict_bi_label_list, average= 'macro'))\n",
    "print (\"Recall macro score for unigrams + bigrams: \", recall_score(true_bi_label_list, predict_bi_label_list, average= 'macro'))\n",
    "print (\"F1 macro score for unigrams + bigrams: \", f1_score(true_bi_label_list, predict_bi_label_list, average= 'macro'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
