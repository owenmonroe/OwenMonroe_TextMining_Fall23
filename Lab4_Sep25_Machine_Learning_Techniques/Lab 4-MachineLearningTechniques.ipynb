{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd95b96",
   "metadata": {},
   "source": [
    "This notebook illustrates the process of building and comparing different models for classifying movie review sentiment, including Logistic Regression, k-nearest neighbor (KNN), and Support Vector Machines (SVM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec34a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train set:  10000\n",
      "Original test set:  2500\n",
      "After removing empty reviews, train set size:  10000\n",
      "After removing empty reviews, test set size:  2500\n",
      "After removing instances with no labels, train set size:  10000\n",
      "After removing instances with no labels, test set size:  2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### LOAD DATASETS ####\n",
    "\n",
    "train_data_file = \"train_small.csv\"\n",
    "test_data_file = \"test_small.csv\"\n",
    "\n",
    "# Import train and test dataset into data frames and print out the original lengths\n",
    "train_data_df = pd.read_csv(train_data_file)\n",
    "test_data_df = pd.read_csv(test_data_file)\n",
    "print (\"Original train set: \",len(train_data_df))\n",
    "print (\"Original test set: \",len(test_data_df))\n",
    "\n",
    "### CLEAN DATASETS ###\n",
    "# Remove empty rows from both sets and print out the new lengths\n",
    "train_data_df = train_data_df[~train_data_df[\"review\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"review\"].isnull()]\n",
    "print (\"After removing empty reviews, train set size: \",len(train_data_df))\n",
    "print (\"After removing empty reviews, test set size: \",len(test_data_df))\n",
    "\n",
    "# Remove rows with null labels\n",
    "train_data_df = train_data_df[~train_data_df[\"sentiment\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"sentiment\"].isnull()]\n",
    "print (\"After removing instances with no labels, train set size: \", len(train_data_df))\n",
    "print (\"After removing instances with no labels, test set size: \", len(test_data_df))\n",
    "\n",
    "# print out top 5 rows of the train set\n",
    "display(train_data_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b1973",
   "metadata": {},
   "source": [
    "### Count-based feature extraction using scikit-learn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac1f3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52522  ...  [('one', 32799), ('of', 32624), ('the', 46654), ('other', 33143), ('reviewers', 38996), ('has', 21177), ('mentioned', 29519), ('that', 46645), ('after', 1404), ('watching', 50735), ('just', 25230), ('oz', 33501), ('episode', 15679), ('you', 52219), ('ll', 27402), ('be', 4335), ('hooked', 22227), ('they', 46753), ('are', 2793), ('right', 39215), ('as', 3012), ('this', 46806), ('is', 24390), ('exactly', 16108), ('what', 51062), ('happened', 21019), ('with', 51540), ('me', 29232), ('br', 6007), ('first', 17476), ('thing', 46779), ('struck', 44844), ('about', 788), ('was', 50689), ('its', 24476), ('brutality', 6474), ('and', 2204), ('unflinching', 48907), ('scenes', 40654), ('violence', 50167), ('which', 51117), ('set', 41482), ('in', 23209), ('from', 18505), ('word', 51720), ('go', 19676), ('trust', 48109), ('not', 32203), ('show', 42050), ('for', 17988), ('faint', 16652), ('hearted', 21401), ('or', 32960), ('timid', 47073), ('pulls', 36772), ('no', 32020), ('punches', 36802), ('regards', 38127), ('to', 47195), ('drugs', 14271), ('sex', 41540), ('hardcore', 21056), ('classic', 8821), ('use', 49495), ('it', 24455), ('called', 7063), ('nickname', 31858), ('given', 19491), ('oswald', 33134), ('maximum', 29027), ('security', 41160), ('state', 44253), ('penitentary', 34375), ('focuses', 17873), ('mainly', 28240), ('on', 32789), ('emerald', 15199), ('city', 8739), ('an', 2145), ('experimental', 16344), ('section', 41149), ('prison', 36224), ('where', 51101), ('all', 1774), ('cells', 7828), ('have', 21256), ('glass', 19538), ('fronts', 18515), ('face', 16584), ('inwards', 24281), ('so', 43155), ('privacy', 36233), ('high', 21801), ('agenda', 1432), ('em', 15128), ('home', 22124), ('many', 28552), ('aryans', 3009), ('muslims', 31144), ('gangstas', 18893)] \n",
      "\n",
      "(10000, 52522) \n",
      "\n",
      "(2500, 52522) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# use original reviews for model building\n",
    "train_text = train_data_df[\"review\"]\n",
    "test_text = test_data_df[\"review\"]\n",
    "\n",
    "# set the n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# create training data representation\n",
    "train_data_cv = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# observe the words in the created dictionary across the document\n",
    "print(len(vectorizer.vocabulary_), \" ... \", list(vectorizer.vocabulary_.items())[0:100],\"\\n\")\n",
    "\n",
    "print(train_data_cv.shape,\"\\n\") \n",
    "\n",
    "# create test data representation\n",
    "test_data_cv = vectorizer.transform(test_text)\n",
    "print(test_data_cv.shape,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541f231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and test data and the labels\n",
    "x_train = train_data_cv\n",
    "y_train = train_data_df[\"sentiment\"]   # true train labels\n",
    "x_test = test_data_cv\n",
    "y_test = test_data_df[\"sentiment\"]     # true test labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2d860",
   "metadata": {},
   "source": [
    "Once the training and testing data are ready, we can train models using different algorithms and compare the accuracy of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61f078",
   "metadata": {},
   "source": [
    "## Logistic Regression using count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5e52c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8716\n",
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.87      1257\n",
      "    positive       0.87      0.88      0.87      1243\n",
      "\n",
      "    accuracy                           0.87      2500\n",
      "   macro avg       0.87      0.87      0.87      2500\n",
      "weighted avg       0.87      0.87      0.87      2500\n",
      "\n",
      "[[1088  169]\n",
      " [ 152 1091]]\n"
     ]
    }
   ],
   "source": [
    "# import evaluation libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we are using the libLinear implementation of logistic regression, which tends to be more efficient\n",
    "model1 = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# train the model \n",
    "model1.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "predictions = model1.predict(x_test)\n",
    "\n",
    "# print evaluation results\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eebe32",
   "metadata": {},
   "source": [
    "Now modify the code above so that predictions are made and evaluated on the training set. What do you observe? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58dcf19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8624\n",
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.86      1257\n",
      "    positive       0.86      0.86      0.86      1243\n",
      "\n",
      "    accuracy                           0.86      2500\n",
      "   macro avg       0.86      0.86      0.86      2500\n",
      "weighted avg       0.86      0.86      0.86      2500\n",
      "\n",
      "[[1081  176]\n",
      " [ 168 1075]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', ..., 'positive', 'positive',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with other solvers, l1-l2 penalty, C regularization parameter\n",
    "\n",
    "# solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\n",
    "# penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}\n",
    "# C regularization parameter float, default=1.0\n",
    "model2 = LogisticRegression(random_state=0, solver='liblinear', penalty='l2', C=10)\n",
    "model2.fit(x_train, y_train)\n",
    "predictions = model2.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09bfdbd",
   "metadata": {},
   "source": [
    "## KNN classifier using count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747b544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6348\n",
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.55      0.60      1257\n",
      "           1       0.61      0.72      0.66      1243\n",
      "\n",
      "    accuracy                           0.63      2500\n",
      "   macro avg       0.64      0.64      0.63      2500\n",
      "weighted avg       0.64      0.63      0.63      2500\n",
      "\n",
      "[[697 560]\n",
      " [353 890]]\n",
      "[1 1 0 ... 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=9)\n",
    "neigh.fit(x_train, y_train)\n",
    "predictions = neigh.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04476f26",
   "metadata": {},
   "source": [
    "## SVM classifier using count-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a14cf8",
   "metadata": {},
   "source": [
    "Note the significant difference between the shape of the input data provided to the SVM algorithm compared to previous models. Observe the steps and find out why and how we prepare the data fed to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a144f87b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train for few examples  [1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0]\n",
      "y_train after encoding for those examples  [1 1 1 0 1 1 1 0 0 1]\n",
      "x_train for few examples    (0, 788)\t1\n",
      "  (0, 963)\t1\n",
      "  (0, 1404)\t1\n",
      "  (0, 1432)\t1\n",
      "  (0, 1491)\t1\n",
      "  (0, 1774)\t1\n",
      "  (0, 2145)\t1\n",
      "  (0, 2204)\t6\n",
      "  (0, 2603)\t1\n",
      "  (0, 2793)\t2\n",
      "  (0, 2904)\t1\n",
      "  (0, 3009)\t1\n",
      "  (0, 3012)\t4\n",
      "  (0, 3378)\t1\n",
      "  (0, 3588)\t2\n",
      "  (0, 4335)\t2\n",
      "  (0, 4446)\t1\n",
      "  (0, 4574)\t1\n",
      "  (0, 5123)\t1\n",
      "  (0, 6007)\t6\n",
      "  (0, 6474)\t1\n",
      "  (0, 6832)\t2\n",
      "  (0, 7063)\t1\n",
      "  (0, 7158)\t1\n",
      "  (0, 7828)\t1\n",
      "  :\t:\n",
      "  (4, 46670)\t1\n",
      "  (4, 46680)\t1\n",
      "  (4, 46739)\t4\n",
      "  (4, 46753)\t1\n",
      "  (4, 46779)\t1\n",
      "  (4, 46806)\t2\n",
      "  (4, 47054)\t2\n",
      "  (4, 47195)\t7\n",
      "  (4, 47680)\t1\n",
      "  (4, 48685)\t1\n",
      "  (4, 49485)\t2\n",
      "  (4, 49719)\t1\n",
      "  (4, 50249)\t1\n",
      "  (4, 50286)\t1\n",
      "  (4, 50724)\t1\n",
      "  (4, 50794)\t1\n",
      "  (4, 50809)\t4\n",
      "  (4, 51062)\t1\n",
      "  (4, 51101)\t1\n",
      "  (4, 51117)\t1\n",
      "  (4, 51509)\t1\n",
      "  (4, 51540)\t1\n",
      "  (4, 51731)\t1\n",
      "  (4, 51754)\t1\n",
      "  (4, 52206)\t1\n",
      "x_train after scaling for those examples    (0, 788)\t0.9291190302891659\n",
      "  (0, 963)\t23.58700498884833\n",
      "  (0, 1404)\t1.5703749942780427\n",
      "  (0, 1432)\t18.921077897000238\n",
      "  (0, 1491)\t70.7177502473036\n",
      "  (0, 1774)\t0.794027150484866\n",
      "  (0, 2145)\t0.8365406908048053\n",
      "  (0, 2204)\t1.0595307841237172\n",
      "  (0, 2603)\t7.175678560682974\n",
      "  (0, 2793)\t1.2342815744417328\n",
      "  (0, 2904)\t2.31716273007829\n",
      "  (0, 3009)\t30.15456130718878\n",
      "  (0, 3012)\t1.7287523140447534\n",
      "  (0, 3378)\t6.778280966242827\n",
      "  (0, 3588)\t5.476976925943405\n",
      "  (0, 4335)\t1.4839638536992044\n",
      "  (0, 4446)\t3.6846834209649777\n",
      "  (0, 4574)\t1.6556433213113857\n",
      "  (0, 5123)\t50.01000300100034\n",
      "  (0, 6007)\t1.1164166074337023\n",
      "  (0, 6474)\t16.030108453420166\n",
      "  (0, 6832)\t1.1177148987510326\n",
      "  (0, 7063)\t3.7844063463387694\n",
      "  (0, 7158)\t1.0950636678342023\n",
      "  (0, 7828)\t21.339479988816\n",
      "  :\t:\n",
      "  (4, 46670)\t1.0303761910291915\n",
      "  (4, 46680)\t5.339407290781449\n",
      "  (4, 46739)\t7.0578027860745305\n",
      "  (4, 46753)\t0.6258466010918856\n",
      "  (4, 46779)\t2.082565573993721\n",
      "  (4, 46806)\t0.8192891517897251\n",
      "  (4, 47054)\t2.4004402158090987\n",
      "  (4, 47195)\t1.3923865402304287\n",
      "  (4, 47680)\t40.83708200920927\n",
      "  (4, 48685)\t3.9716705466028452\n",
      "  (4, 49485)\t3.7534037178717194\n",
      "  (4, 49719)\t24.27420435385202\n",
      "  (4, 50249)\t10.092677633537642\n",
      "  (4, 50286)\t16.03802311152165\n",
      "  (4, 50724)\t1.6546469363500065\n",
      "  (4, 50794)\t1.5000081919420876\n",
      "  (4, 50809)\t3.9564401033933545\n",
      "  (4, 51062)\t0.9217954617056444\n",
      "  (4, 51101)\t1.6045550633595718\n",
      "  (4, 51117)\t1.0791652948274708\n",
      "  (4, 51509)\t5.121575278912271\n",
      "  (4, 51540)\t0.4914386238818787\n",
      "  (4, 51731)\t2.058998644362501\n",
      "  (4, 51754)\t1.8417652063561956\n",
      "  (4, 52206)\t4.686960447023196\n",
      "Accuracy score:  0.8264\n",
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83      1257\n",
      "           1       0.82      0.83      0.83      1243\n",
      "\n",
      "    accuracy                           0.83      2500\n",
      "   macro avg       0.83      0.83      0.83      2500\n",
      "weighted avg       0.83      0.83      0.83      2500\n",
      "\n",
      "[[1031  226]\n",
      " [ 208 1035]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# observe the label format of data\n",
    "print(\"y_train for few examples \",y_train[0:20])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# observe the format of label after transformation\n",
    "print(\"y_train after encoding for those examples \",y_train[0:10])\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "\n",
    "# observe the input format of data\n",
    "print(\"x_train for few examples \",x_train[0:5])\n",
    "\n",
    "# observe the format of data after transformation\n",
    "print(\"x_train after scaling for those examples \",x_train_std[0:5])\n",
    "\n",
    "\n",
    "# Create SVM model\n",
    "clf1 = svm.LinearSVC()\n",
    "\n",
    "#clf1 = SGDClassifier()\n",
    "\n",
    "clf1.fit(x_train_std, y_train)\n",
    "\n",
    "# predict on test data\n",
    "predictions = clf1.predict(x_test_std)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da252d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7764\n",
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78      1257\n",
      "           1       0.78      0.76      0.77      1243\n",
      "\n",
      "    accuracy                           0.78      2500\n",
      "   macro avg       0.78      0.78      0.78      2500\n",
      "weighted avg       0.78      0.78      0.78      2500\n",
      "\n",
      "[[991 266]\n",
      " [293 950]]\n"
     ]
    }
   ],
   "source": [
    "# Explore parameters to LinearSVC like C, penalty\n",
    "\n",
    "# penalty{‘l1’, ‘l2’}\n",
    "# loss{‘hinge’, ‘squared_hinge’}\n",
    "# C Regularization parameter: float, default=1.0\n",
    "\n",
    "# Create SVM model\n",
    "#clf2 = svm.LinearSVC(loss='hinge', penalty='l2', C=0.12, max_iter=1000)\n",
    "\n",
    "clf2 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=500)\n",
    "\n",
    "clf2.fit(x_train_std, y_train)\n",
    "\n",
    "# predict on test data\n",
    "predictions = clf2.predict(x_test_std)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634c6dd",
   "metadata": {},
   "source": [
    "## Using tf-idf features with original reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa74b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 52522) \n",
      "\n",
      "(2500, 52522) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "train_data_tfidf = tf.fit_transform(train_text)\n",
    "print(train_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "test_data_tfidf = tf.transform(test_text)\n",
    "print(test_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb74a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data_tfidf\n",
    "y_train = train_data_df[\"sentiment\"]\n",
    "x_test = test_data_tfidf\n",
    "y_test = test_data_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77a0f5",
   "metadata": {},
   "source": [
    "## Logistic Regression using tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c45cb51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.878\n",
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.88      1257\n",
      "    positive       0.87      0.88      0.88      1243\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.88      0.88      0.88      2500\n",
      "weighted avg       0.88      0.88      0.88      2500\n",
      "\n",
      "[[1097  160]\n",
      " [ 145 1098]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0,solver='liblinear')\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a1ef2",
   "metadata": {},
   "source": [
    "## KNN classifier using tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a564c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(x_train, y_train)\n",
    "predictions = neigh.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421144",
   "metadata": {},
   "source": [
    "If you use the datasets in last week's lab, this step may be extremely slow given the computation needed for KNN and the large feature space. That's why we use train_small and test_small :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7ce5b",
   "metadata": {},
   "source": [
    "## SVM classifier using tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d166d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "# Create SVM model\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x_train_std, y_train)\n",
    "\n",
    "# predict on test data\n",
    "predictions = clf.predict(x_test_std)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43858a5",
   "metadata": {},
   "source": [
    "How do count and tf-idf feature vectors compare for different classifiers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
