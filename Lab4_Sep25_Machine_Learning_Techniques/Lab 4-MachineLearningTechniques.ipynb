{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd95b96",
   "metadata": {},
   "source": [
    "This notebook illustrates the process of building and comparing different models for classifying movie review sentiment, including Logistic Regression, k-nearest neighbor (KNN), and Support Vector Machines (SVM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### LOAD DATASETS ####\n",
    "\n",
    "train_data_file = \"train_small.csv\"\n",
    "test_data_file = \"test_small.csv\"\n",
    "\n",
    "# Import train and test dataset into data frames and print out the original lengths\n",
    "train_data_df = pd.read_csv(train_data_file)\n",
    "test_data_df = pd.read_csv(test_data_file)\n",
    "print (\"Original train set: \",len(train_data_df))\n",
    "print (\"Original test set: \",len(test_data_df))\n",
    "\n",
    "### CLEAN DATASETS ###\n",
    "# Remove empty rows from both sets and print out the new lengths\n",
    "train_data_df = train_data_df[~train_data_df[\"review\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"review\"].isnull()]\n",
    "print (\"After removing empty reviews, train set size: \",len(train_data_df))\n",
    "print (\"After removing empty reviews, test set size: \",len(test_data_df))\n",
    "\n",
    "# Remove rows with null labels\n",
    "train_data_df = train_data_df[~train_data_df[\"sentiment\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"sentiment\"].isnull()]\n",
    "print (\"After removing instances with no labels, train set size: \", len(train_data_df))\n",
    "print (\"After removing instances with no labels, test set size: \", len(test_data_df))\n",
    "\n",
    "# print out top 5 rows of the train set\n",
    "display(train_data_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b1973",
   "metadata": {},
   "source": [
    "### Count-based feature extraction using scikit-learn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1f3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# use original reviews for model building\n",
    "train_text = train_data_df[\"review\"]\n",
    "test_text = test_data_df[\"review\"]\n",
    "\n",
    "# set the n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# create training data representation\n",
    "train_data_cv = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# observe the words in the created dictionary across the document\n",
    "print(len(vectorizer.vocabulary_), \" ... \", list(vectorizer.vocabulary_.items())[0:100],\"\\n\")\n",
    "\n",
    "print(train_data_cv.shape,\"\\n\") \n",
    "\n",
    "# create test data representation\n",
    "test_data_cv = vectorizer.transform(test_text)\n",
    "print(test_data_cv.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and test data and the labels\n",
    "x_train = train_data_cv\n",
    "y_train = train_data_df[\"sentiment\"]\n",
    "x_test = test_data_cv\n",
    "y_test = test_data_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2d860",
   "metadata": {},
   "source": [
    "Once the training and testing data are ready, we can train models using different algorithms and compare the accuracy of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61f078",
   "metadata": {},
   "source": [
    "## Logistic Regression using count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluation libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we are using the libLinear implementation of logistic regression, which tends to be more efficient\n",
    "model1 = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# train the model \n",
    "model1.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "predictions = model1.predict(x_test)\n",
    "\n",
    "# print evaluation results\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eebe32",
   "metadata": {},
   "source": [
    "Now modify the code above so that predictions are made and evaluated on the training set. What do you observe? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with other solvers, l1-l2 penalty, C regularization parameter\n",
    "\n",
    "# solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\n",
    "# penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}\n",
    "# C regularization parameter float, default=1.0\n",
    "model2 = LogisticRegression(random_state=0, solver='liblinear', penalty='l2', C=10)\n",
    "model2.fit(x_train, y_train)\n",
    "predictions = model2.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09bfdbd",
   "metadata": {},
   "source": [
    "## KNN classifier using count-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(x_train, y_train)\n",
    "predictions = neigh.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030df7f",
   "metadata": {},
   "source": [
    "Experiment with different k parameters. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04476f26",
   "metadata": {},
   "source": [
    "## SVM classifier using count-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a14cf8",
   "metadata": {},
   "source": [
    "Note the significant difference between the shape of the input data provided to the SVM algorithm compared to previous models. Observe the steps and find out why and how we prepare the data fed to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144f87b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# observe the label format of data\n",
    "print(\"y_train for few examples \",y_train[0:10])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# observe the format of label after transformation\n",
    "print(\"y_train after encoding for those examples \",y_train[0:10])\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "\n",
    "# observe the input format of data\n",
    "print(\"x_train for few examples \",x_train[0:5])\n",
    "\n",
    "# observe the format of data after transformation\n",
    "print(\"x_train after scaling for those examples \",x_train_std[0:5])\n",
    "\n",
    "\n",
    "# Create SVM model\n",
    "clf1 = svm.LinearSVC()\n",
    "\n",
    "#clf1 = SGDClassifier()\n",
    "\n",
    "clf1.fit(x_train_std, y_train)\n",
    "\n",
    "# predict on test data\n",
    "predictions = clf1.predict(x_test_std)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da252d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore parameters to LinearSVC like C, penalty\n",
    "\n",
    "# penalty{‘l1’, ‘l2’}\n",
    "# loss{‘hinge’, ‘squared_hinge’}\n",
    "# C Regularization parameter: float, default=1.0\n",
    "\n",
    "# Create SVM model\n",
    "clf2 = svm.LinearSVC(loss='hinge', penalty='l2', C=0.12, max_iter=1000)\n",
    "\n",
    "# clf2 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=500)\n",
    "\n",
    "clf2.fit(x_train_std, y_train)\n",
    "\n",
    "# predict on test data\n",
    "predictions = clf2.predict(x_test_std)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634c6dd",
   "metadata": {},
   "source": [
    "## Using tf-idf features with original reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "train_data_tfidf = tf.fit_transform(train_text)\n",
    "print(train_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "test_data_tfidf = tf.transform(test_text)\n",
    "print(test_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb74a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data_tfidf\n",
    "y_train = train_data_df[\"sentiment\"]\n",
    "x_test = test_data_tfidf\n",
    "y_test = test_data_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77a0f5",
   "metadata": {},
   "source": [
    "## Logistic Regression using tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0,solver='liblinear')\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a1ef2",
   "metadata": {},
   "source": [
    "## KNN classifier using tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a564c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(x_train, y_train)\n",
    "predictions = neigh.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421144",
   "metadata": {},
   "source": [
    "If you use the datasets in last week's lab, this step may be extremely slow given the computation needed for KNN and the large feature space. That's why we use train_small and test_small :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7ce5b",
   "metadata": {},
   "source": [
    "## SVM classifier using tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d166d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "# Create SVM model\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x_train_std, y_train)\n",
    "\n",
    "# predict on test data\n",
    "predictions = clf.predict(x_test_std)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43858a5",
   "metadata": {},
   "source": [
    "How do count and tf-idf feature vectors compare for different classifiers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
