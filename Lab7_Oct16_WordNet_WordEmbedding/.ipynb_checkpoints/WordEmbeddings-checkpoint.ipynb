{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores word embedding through `gensim` package.\n",
    "\n",
    "We will train embeddings from scratch as well as use pre-trained word vectors.\n",
    "We will then attempt to use embeddings as features in text classification on the COVID tweet dataset. \n",
    "\n",
    "Before running, install gensim with:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Some materials on word embeddings are adapted from: https://github.com/dbamman/anlp21/blob/main/4.embeddings/WordEmbeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from gensim) (1.21.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim and related libraries\n",
    "import re\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a word2vec model on a small Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train a new word2vec model on Wikipedia text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/wordembeddings/wiki.10K.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/owenmonroe/Desktop/GitHub/TextMiningFall23/Lab7_Oct16_WordNet_WordEmbedding/.ipynb_checkpoints/WordEmbeddings-checkpoint.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owenmonroe/Desktop/GitHub/TextMiningFall23/Lab7_Oct16_WordNet_WordEmbedding/.ipynb_checkpoints/WordEmbeddings-checkpoint.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# file from which to generate word embeddings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owenmonroe/Desktop/GitHub/TextMiningFall23/Lab7_Oct16_WordNet_WordEmbedding/.ipynb_checkpoints/WordEmbeddings-checkpoint.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDatasets/wordembeddings/wiki.10K.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/owenmonroe/Desktop/GitHub/TextMiningFall23/Lab7_Oct16_WordNet_WordEmbedding/.ipynb_checkpoints/WordEmbeddings-checkpoint.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owenmonroe/Desktop/GitHub/TextMiningFall23/Lab7_Oct16_WordNet_WordEmbedding/.ipynb_checkpoints/WordEmbeddings-checkpoint.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owenmonroe/Desktop/GitHub/TextMiningFall23/Lab7_Oct16_WordNet_WordEmbedding/.ipynb_checkpoints/WordEmbeddings-checkpoint.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         words\u001b[39m=\u001b[39mline\u001b[39m.\u001b[39mrstrip()\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/textmining/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets/wordembeddings/wiki.10K.txt'"
     ]
    }
   ],
   "source": [
    "sentences=[]\n",
    "# file from which to generate word embeddings\n",
    "filename=\"Datasets/wordembeddings/wiki.10K.txt\"\n",
    "with open(filename, 'rb') as file:\n",
    "    for line in file:\n",
    "        words=line.rstrip().lower().decode('utf-8')\n",
    "        # this file is already tokenized, so we can split on whitespace\n",
    "        # but first let's replace any sequence of whitespace (space, tab, newline, etc.) with single space\n",
    "        words=re.sub(\"\\s+\", \" \", words)\n",
    "        sentences.append(words.split(\" \"))\n",
    "\n",
    "model_wiki = Word2Vec(\n",
    "        sentences,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=10)\n",
    "\n",
    "my_trained_vectors = model_wiki.wv\n",
    "\n",
    "# save vectors to file if you want to use them later\n",
    "#my_trained_vectors.save_word2vec_format('Datasets/wordembeddings/embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_trained_vectors.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained Glove vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in vectors that have already been trained on a much bigger dataset. [Glove vectors](https://nlp.stanford.edu/projects/glove/) are trained using a different method than word2vec, but its vectors can also be loaded by `gensim`.  Here we'll use a 100-dimensional model trained on 6B words (from Wikipedia and news), but even bigger models are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to convert the Glove format into w2v format; this creates a new file\n",
    "glove_file=\"Datasets/wordembeddings/glove.6B.100d.100K.txt\"\n",
    "glove_in_w2v_format=\"Datasets/wordembeddings/glove.6B.100d.100K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(\"Datasets/wordembeddings/glove.6B.100d.100K.w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`most_similar` computes cosine similarity between the given word and the vectors for each vocabulary word in the model and returns the top N words. You can play around with this function to discover other analogies that have been learned in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one + two = three + ?\n",
    "one=\"man\"\n",
    "two=\"king\"\n",
    "three=\"woman\"\n",
    "\n",
    "glove.most_similar(positive=[two, three], negative=[one], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one=\"paris\"\n",
    "two=\"france\"\n",
    "three=\"berlin\"\n",
    "\n",
    "glove.most_similar(positive=[two, three], negative=[one], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing classification results for count based vectors to word embedding vectors\n",
    "\n",
    "We will now try to use embeddings as features for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### LOAD DATASETS ####\n",
    "\n",
    "train_data_file = \"Datasets/Corona_NLP/Tweets_preprocessed_train_data.csv\"\n",
    "test_data_file = \"Datasets/Corona_NLP/Tweets_preprocessed_test_data.csv\"\n",
    "\n",
    "# Import train and test dataset into data frames and print out the original lengths\n",
    "train_data_df = pd.read_csv(train_data_file)\n",
    "test_data_df = pd.read_csv(test_data_file)\n",
    "print (\"Original train set: \",len(train_data_df))\n",
    "print (\"Original test set: \",len(test_data_df))\n",
    "\n",
    "### CLEAN DATASETS ###\n",
    "# Remove empty rows from both sets and print out the new lengths\n",
    "train_data_df = train_data_df[~train_data_df[\"OriginalTweet\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"OriginalTweet\"].isnull()]\n",
    "print (\"After removing empty tweets, train set size: \",len(train_data_df))\n",
    "print (\"After removing empty tweets, test set size: \",len(test_data_df))\n",
    "\n",
    "# Remove rows with null labels\n",
    "train_data_df = train_data_df[~train_data_df[\"Sentiment\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"Sentiment\"].isnull()]\n",
    "print (\"After removing instances with no labels, train set size: \", len(train_data_df))\n",
    "print (\"After removing instances with no labels, test set size: \", len(test_data_df))\n",
    "\n",
    "# print out top 5 rows of the train set\n",
    "display(train_data_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use original tweets for model building\n",
    "y_train = train_data_df[\"Sentiment\"]\n",
    "y_test = test_data_df[\"Sentiment\"]\n",
    "\n",
    "train_text = train_data_df[\"CleanedTweet\"]\n",
    "test_text = test_data_df[\"CleanedTweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based feature extraction and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something you have done many times and should be able to interpret well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# set the n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# create training data representation\n",
    "train_data_cv = vectorizer.fit_transform(train_text)\n",
    "test_data_cv = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc\n",
    "\n",
    "\n",
    "lg = LogisticRegression(random_state=0, solver='liblinear')\n",
    "lg.fit(train_data_cv, y_train)\n",
    "predictions = lg.predict(test_data_cv)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions, average=\"weighted\"))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions, average = \"weighted\"))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings-based feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create features directly from word embeddings. We can use different word embedding vectors: \n",
    "- word-embedding model trained on Wikipedia \n",
    "- pre-trained Glove vectors\n",
    "- word-embedding model trained on the COVID19 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calculate the vector representation for input data given Wikipedia word embeddings\n",
    "\n",
    "We already trained a model on Wikipedia data above. We will use that model to extract vector representation of training and testing data. Examine carefully how these vectors for the basis of the features below.\n",
    "\n",
    "Essentially, we can find the vector for each word in the sentence and calculate the mean of all vectors as the representation of the sentence. This is a very simple method, and generally may not be the most effective.\n",
    "\n",
    "Also note that some words from the dataset may not appear at all in the trained vectors. We refer to these as OOV (out-of-vocabulary) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_data_for_word_model(model, data_df):\n",
    "    v = model.wv.get_vector('king')\n",
    "    X = np.zeros((len(data_df), v.shape[0]))\n",
    "    n = 0\n",
    "    for index, row in data_df.iterrows():\n",
    "        tokens = row[\"CleanedTweet\"].split()\n",
    "        vecs = []\n",
    "        m = 0\n",
    "        emptycount = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                # throws KeyError if word not found\n",
    "                vec = model.wv.get_vector(word)\n",
    "                vecs.append(vec)\n",
    "                m += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(vecs) > 0:\n",
    "            vecs = np.array(vecs)\n",
    "            X[n] = vecs.mean(axis=0)\n",
    "        else:\n",
    "            emptycount += 1\n",
    "        n+=1\n",
    "    return X\n",
    "\n",
    "\n",
    "xtrain = transform_data_for_word_model(model_wiki,train_data_df )\n",
    "xtest = transform_data_for_word_model(model_wiki,test_data_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating the model using Wikipedia word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=0, solver='liblinear')\n",
    "lg.fit(xtrain, y_train)\n",
    "predictions = lg.predict(xtest)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions, average=\"weighted\"))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions, average = \"weighted\"))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating the model using Glove vectors\n",
    "\n",
    "The difference between this function and the function above is that the pretrained Glove vectors are accessed slightly differently. In the former model that we trained, we accessed the model using `model.wv.get_vector` or `model.wv.most_similar`. For this pretrained Glove model, we have read/loaded it slightly differently and hence we do `model.get_vector` and `model.most_similar`. (wv specifically refers to `word2vec`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_glove(model, data_df):\n",
    "    v = model.get_vector('king')\n",
    "    X = np.zeros((len(data_df), v.shape[0]))\n",
    "    n = 0\n",
    "    for index, row in data_df.iterrows():\n",
    "        tokens = row[\"CleanedTweet\"].split()\n",
    "        vecs = []\n",
    "        m = 0\n",
    "        emptycount = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                # throws KeyError if word not found\n",
    "                vec = model.get_vector(word)\n",
    "                vecs.append(vec)\n",
    "                m += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(vecs) > 0:\n",
    "            vecs = np.array(vecs)\n",
    "            X[n] = vecs.mean(axis=0)\n",
    "        else:\n",
    "            emptycount += 1\n",
    "        n+=1\n",
    "    return X\n",
    "\n",
    "xtrain_glove = transform_data_for_glove(glove,train_data_df )\n",
    "xtest_glove = transform_data_for_glove(glove,test_data_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=0, solver='liblinear')\n",
    "lg.fit(xtrain_glove, y_train)\n",
    "predictions = lg.predict(xtest_glove)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions, average=\"weighted\"))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions, average = \"weighted\"))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the Wikipedia word2vec and the pretrained Glove model perform better on the tweet dataset? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using COVID19 tweets to create word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a word2vec model from the COVID tweets. Then we will transform our training and testing data into the vectors using this word model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [train_data_df, test_data_df]\n",
    "all_dataset = pd.concat(frames)\n",
    "sentences_from_data = [x.split() for x in all_dataset[\"CleanedTweet\"]]\n",
    "\n",
    "model_covid = Word2Vec(\n",
    "        sentences_from_data,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating the model using COVID19 embeddings\n",
    "\n",
    "We will use the same transformation function from above to transform the tweet data into features using COVID19 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_covid = transform_data_for_word_model(model_covid,train_data_df )\n",
    "xtest_covid = transform_data_for_word_model(model_covid,test_data_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=0, solver='liblinear')\n",
    "lg.fit(xtrain_covid, y_train)\n",
    "predictions = lg.predict(xtest_covid)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions, average=\"weighted\"))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions, average = \"weighted\"))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the embedding-based classification models perform in comparison to the count-based models? How can we interpret the results?\n",
    "\n",
    "You can also explore how the predictions from different models compare. What are some of the test examples that count-based model classify accurately, while the embedding-based model do not (and vice versa)? Checking the model predictions is a good way to gain more insight into model behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
