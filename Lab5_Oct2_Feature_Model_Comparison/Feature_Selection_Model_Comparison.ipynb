{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates:  \n",
    "(1) Various feature selection techniques and evaluation schemes on the movie review dataset;  \n",
    "(2) Model comparison using cross-validation and paired t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#### LOAD DATASETS ####\n",
    "\n",
    "train_data_file = \"movie_train.csv\"\n",
    "test_data_file = \"movie_test.csv\"\n",
    "\n",
    "# Import train and test dataset into data frames and print out the original lengths\n",
    "train_data_df = pd.read_csv(train_data_file)\n",
    "test_data_df = pd.read_csv(test_data_file)\n",
    "print (\"Original train set: \",len(train_data_df))\n",
    "print (\"Original test set: \",len(test_data_df))\n",
    "\n",
    "### CLEAN DATASETS ###\n",
    "# Remove empty rows from both sets and print out the new lengths\n",
    "train_data_df = train_data_df[~train_data_df[\"review\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"review\"].isnull()]\n",
    "print (\"After removing empty reviews, train set size: \",len(train_data_df))\n",
    "print (\"After removing empty reviews, test set size: \",len(test_data_df))\n",
    "\n",
    "# Remove rows with null labels\n",
    "train_data_df = train_data_df[~train_data_df[\"sentiment\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"sentiment\"].isnull()]\n",
    "print (\"After removing instances with no labels, train set size: \", len(train_data_df))\n",
    "print (\"After removing instances with no labels, test set size: \", len(test_data_df))\n",
    "\n",
    "# print out top 5 rows of the train set\n",
    "display(train_data_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use original reviews for model building\n",
    "y_train = train_data_df[\"sentiment\"]\n",
    "y_test = test_data_df[\"sentiment\"]\n",
    "\n",
    "train_text = train_data_df[\"review\"]\n",
    "test_text = test_data_df[\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# set the n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# create training data representation\n",
    "train_data_cv = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# observe the words in the created dictionary across the document\n",
    "print(len(vectorizer.vocabulary_), \" ... \", list(vectorizer.vocabulary_.items())[0:100],\"\\n\")\n",
    "\n",
    "print(train_data_cv.shape,\"\\n\") \n",
    "\n",
    "# create test data representation\n",
    "test_data_cv = vectorizer.transform(test_text)\n",
    "print(test_data_cv.shape,\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of features? Note that it is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(train_data_cv, y_train)\n",
    "predictions = naive_bayes.predict(test_data_cv)\n",
    "\n",
    "# average could be of 3 kinds = weighted, macro, micro\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions, average=\"weighted\"))\n",
    "print(\"Recall score: \", recall_score(y_test, predictions, average = \"weighted\"))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-variance feature removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold = 0.001)\n",
    "\n",
    "X_train_features_filtered_var_thr = selector.fit(train_data_cv).transform(train_data_cv)\n",
    "print (\"Train feature space before filtering: \", train_data_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_var_thr.shape)\n",
    "\n",
    "X_test_features_filtered_var_thr = selector.transform(test_data_cv)\n",
    "print (\"Test feature space before filtering: \", test_data_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_var_thr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened with low-variance feature removal? What does the results tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different variance threshold values and observe how the number of features changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using chi-squared statistic and k-best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=200)\n",
    "X_train_features_filtered_kbest = selector.fit_transform(train_data_cv, y_train)\n",
    "print (\"Train feature space before filtering: \", train_data_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_kbest.shape)\n",
    "\n",
    "X_test_features_filtered_kbest = selector.transform(test_data_cv)\n",
    "print (\"Test feature space before filtering: \", test_data_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_kbest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating a model with selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code from above to train and evaluate a model. Only this time with the `X_train_features_filtered_var_thr` and  `X_test_features_filtered_var_thr` (features selected after low-variance feature removal). Compare the results with the earlier model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your training/evaluation code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train/evaluate the model with features selected using chi-squared statistic, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your training/evaluation code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select features for a logistic regression model with L1 (LASSO) regularization, which implicitly performs feature selection by setting feature weights to zero. See the below code and interpret the printed outputs in the cell. The selector is using an estimator to keep the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression(solver=\"liblinear\", penalty=\"l1\")).fit(train_data_cv, y_train)\n",
    "\n",
    "# estimator_: The base estimator from which the transformer is built. This attribute exists only when fit has been called.\n",
    "# In this case, the estimator is Logistic Regression.\n",
    "# coef_ : an attribute of the Logistic Regression estimator. Tha value represents the weight of each of the features fed to the model.\n",
    "print(\"Coefficients of the features fed to the \",selector.estimator_ , \" estimator\")\n",
    "print(selector.estimator_.coef_)\n",
    "\n",
    "print()\n",
    "# threshold_: Threshold value used for feature selection.\n",
    "print(\"selector.threshold_ \",selector.threshold_)\n",
    "\n",
    "print()\n",
    "# gives the features which were kept or dropped according to the model.\n",
    "print(\"Selector support: \", selector.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cv_selectedfrommodel = selector.transform(train_data_cv)\n",
    "test_data_cv_selectedfrommodel = selector.transform(test_data_cv)\n",
    "\n",
    "print (\"Train feature space before filtering: \", train_data_cv.shape)\n",
    "print (\"Train feature space after filtering: \", train_data_cv_selectedfrommodel.shape)\n",
    "print (\"Test feature space before filtering: \", test_data_cv.shape)\n",
    "print (\"Test feature space after filtering: \", test_data_cv_selectedfrommodel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipelines in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn pipelines can be used to specify multiple steps in a data modeling process and execute them in sequence. The pipeline below does feature selection, then builds a multinomial NB model using the selected features. \n",
    "\n",
    "It is possible to add more steps to the pipeline such as StandardScaler. You can read more about the scikit-learn Pipeline here- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define x_all and y_all to complete this code and run to see to results\n",
    "x_all = # your code here\n",
    "y_all = # your code here\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size = 0.25)\n",
    " \n",
    "\n",
    "pipeline = Pipeline([('var_th', VarianceThreshold(threshold = 0.001)),('mnb', MultinomialNB())], verbose = True)\n",
    " \n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# to see all the hyper parameters\n",
    "print()\n",
    "print(pipeline.get_params())\n",
    "\n",
    "print()\n",
    "print(\"Evaluation accuracy: \",pipeline.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the stratified k-fold cross-validation divides the data into training and test sets based on the value of number of splits. You can update this code further to incorporate the model training and evaluation for each split and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## combined dataset\n",
    "frames = [train_data_df, test_data_df]\n",
    "\n",
    "all_dataset = pd.concat(frames)\n",
    "\n",
    "x_all_text = all_dataset[\"review\"]\n",
    "y_all = all_dataset[\"sentiment\"]\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "x_all = vectorizer.fit_transform(x_all_text)\n",
    "\n",
    "print(len(vectorizer.vocabulary_), \" ... \", list(vectorizer.vocabulary_.items())[0:100],\"\\n\")\n",
    "print(x_all.shape,\"\\n\") \n",
    "print(y_all.shape,\"\\n\") \n",
    "\n",
    "# create test data representation\n",
    "# test_data_cv = vectorizer.transform(test_text)\n",
    "# print(test_data_cv.shape,\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "print(skf)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(x_all, y_all):\n",
    "    print(\"TRAIN:\", type(train_index))\n",
    "    print(\"TEST:\", test_index)\n",
    "    print()\n",
    "    X_train = x_all[train_index]\n",
    "    X_test = x_all[test_index]\n",
    "    Y_train = y_all[train_index]\n",
    "    Y_test = y_all[test_index]\n",
    "\n",
    "    pipeline = Pipeline([('chi2', SelectKBest(chi2, k=200)),('mnb', MultinomialNB())], verbose = True)\n",
    "\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "\n",
    "    # to see all the hyper parameters\n",
    "    print()\n",
    "    print(pipeline.get_params())\n",
    "\n",
    "    print()\n",
    "    print(\"Evaluation accuracy: \",pipeline.score(X_test, Y_test))\n",
    "    print(\"\\n\\n.........................\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a one liner code to implement an ML pipeline with Sklearn Pipeline and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('chi2', SelectKBest(chi2, k=200)),('mnb', MultinomialNB())], verbose = True)\n",
    "scores = cross_val_score(pipeline, x_all, y_all, cv=5)\n",
    "print(\"Scores of the K fold stratified cross validations= \", scores)\n",
    "print(\"Average score of the K fold stratified cross validation= \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Combine the training and test data because we'd like to do cross-validation\n",
    "frames = [train_data_df, test_data_df]\n",
    "\n",
    "all_dataset = pd.concat(frames)\n",
    "\n",
    "x_all_text = all_dataset[\"review\"]\n",
    "y_all = all_dataset[\"sentiment\"]\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "# set the n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# create training data representation\n",
    "x_all = vectorizer.fit_transform(x_all_text)\n",
    "\n",
    "# observe the words in the created dictionary across the document\n",
    "print(len(vectorizer.vocabulary_), \" ... \", list(vectorizer.vocabulary_.items())[0:100],\"\\n\")\n",
    "\n",
    "print(x_all.shape,\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison across k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train 2 different models (Multinomial Naive Bayes and Logistic Regression) on the complete dataset. We use stratified K-fold to split data into train and test sets with roughly similar proportions of each label. In each fold, we compare the performance of the two models on the same test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "#from sklearn.model_selection import cross_validate\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "print(skf)\n",
    "\n",
    "\n",
    "i=0\n",
    "f1s_nb = []\n",
    "f1s_lr = []\n",
    "\n",
    "for train_index, test_index in skf.split(x_all, y_all):\n",
    "    print(\"Training instance indexes:\", train_index)\n",
    "    print(\"Test instance indexes:\", test_index)\n",
    "    X_train = x_all[train_index]\n",
    "    X_test = x_all[test_index]\n",
    "    Y_train = y_all[train_index]\n",
    "    Y_test = y_all[test_index]\n",
    "\n",
    "    naive_bayes = MultinomialNB()\n",
    "    naive_bayes.fit(X_train, Y_train)\n",
    "    predictions_nb = naive_bayes.predict(X_test)\n",
    "\n",
    "    f1_nb = f1_score(Y_test, predictions_nb, average = \"weighted\")\n",
    "    f1s_nb.append(f1_nb)\n",
    "    \n",
    "    LR = LogisticRegression(solver=\"liblinear\", penalty=\"l1\")\n",
    "    LR.fit(X_train, Y_train)\n",
    "    predictions_lr = LR.predict(X_test)\n",
    "\n",
    "    f1_lr = f1_score(Y_test, predictions_lr, average = \"weighted\")\n",
    "    f1s_lr.append(f1_lr)\n",
    "    \n",
    "    print(\"F1 scores at fold \", i+1  ,\" for NB and LR are : \", f1_nb , f1_lr)\n",
    "    print()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring statistical significance of the model performance using Student's t-test\n",
    "\n",
    "Because we have results from 5-fold cross validation, we can measure if there's a statistically significant difference between the mean F1 scores of the two classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Student's t-test \n",
    "nb_lr_ttest = stats.ttest_ind(f1s_nb,f1s_lr)\n",
    "print(\"t-test result: \", nb_lr_ttest) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the result above indicate, if the significance level is 95%? How about 99.9%? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate confidence intervals for accuracy (since we can treat the classification outcome as \"accurate\" or \"not accurate\") of Naive Bayes and logistic regression classifiers. You can learn more about how to calculate confidence intervals for accuracy here: https://machinelearningmastery.com/confidence-intervals-for-machine-learning/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how important individual features (or components of a model) are, we can conduct an ablation test.\n",
    "- Train the full model with all features included and conduct evaluation\n",
    "- Remove feature (or group of features) and conduct evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# use the original training/test split\n",
    "x_train = train_data_df[\"review\"]\n",
    "x_test = test_data_df[\"review\"]\n",
    "y_train = train_data_df[\"sentiment\"]\n",
    "y_test = test_data_df[\"sentiment\"]\n",
    "\n",
    "# we will test different n-gram features for their contribution to logistic regression classifier\n",
    "def ablation_test(x_train, y_train, x_test, y_test, ngram_range):\n",
    "# set the n-gram range\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range = ngram_range)\n",
    "\n",
    "    # create training data representation\n",
    "    grams = vectorizer.fit(x_train)\n",
    "    \n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    # observe the words in the created dictionary across the document\n",
    "    print(\"number of features for ngram_range \", ngram_range, \" : \",len(vectorizer.vocabulary_))\n",
    "\n",
    "    LR = LogisticRegression(solver=\"liblinear\", penalty=\"l1\")\n",
    "    LR.fit(x_train, y_train)\n",
    "    predictions_lr = LR.predict(x_test)\n",
    "\n",
    "    f1_lr = f1_score(y_test, predictions_lr, average = \"weighted\")\n",
    "    print(\"F1 score for \", ngram_range, \" : \",f1_lr, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ablation test for unigrams, unigrams+bigrams, unigrams+bigrams+trigrams, etc.\n",
    "for i in range(1,4):\n",
    "    for j in range(3,0,-1): \n",
    "        if(i<=j):\n",
    "            ablation_test(x_train, y_train, x_test, y_test, (i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the results indicate? Which n-gram feature is most useful? least useful? Do the results align with your expectations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
