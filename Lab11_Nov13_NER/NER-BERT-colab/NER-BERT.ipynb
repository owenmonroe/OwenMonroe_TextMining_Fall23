{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE_YzIVRSulq"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2022/06/fine-tune-bert-model-for-named-entity-recognition-in-google-colab/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SMcM7GZUGif"
      },
      "source": [
        "## Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IEHMf-5yQcuJ"
      },
      "outputs": [],
      "source": [
        "# We need to install the necessary libraries to work with the HuggingFace transformer\n",
        "# datasets library to fetch data\n",
        "# tokenizers to preprocess the data\n",
        "# transformers to fine-tune the models\n",
        "# seqeval to compute model metrics\n",
        "\n",
        "!pip install datasets -q\n",
        "!pip install tokenizers -q\n",
        "!pip install transformers -q\n",
        "!pip install seqeval -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uIWIHgaTt4x"
      },
      "source": [
        "## Load English dataset\n",
        "\n",
        "We will be using an English language NER dataset from the HuggingFace datasets module. It follows the BIO (Beginning, Inside, Outside) format for tagging sentence tokens for the Named Entity Recognition task.\n",
        "\n",
        "The dataset contains 3 sets of data, train, validation, and test. It consists of tokens, ner_tags, langs, and spans. The ner_tags have ids corresponding to BIO format, I-TYPE, which means the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have the tag B-TYPE to show that it starts a new phrase. A word with the tag O is not part of a phrase.\n",
        "\n",
        "There is a total of 4 classes, Person(PER), Organization(ORG), Location(LOC), and others(O)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ftvO0-aTiJt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikiann\", \"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yfiy27gQTwZM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['validation', 'test', 'train'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OowzPZ87T1_b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g55jcoCKT6P1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1b5huqbZag0E"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'validation': ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              " 'test': ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              " 'train': ['tokens', 'ner_tags', 'langs', 'spans']}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JKZRaT8xf385"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'validation': (10000, 4), 'test': (10000, 4), 'train': (20000, 4)}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gsDnocmvXg8k"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lGnp1jCceEDs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tokens': [['R.H.',\n",
              "   'Saunders',\n",
              "   '(',\n",
              "   'St.',\n",
              "   'Lawrence',\n",
              "   'River',\n",
              "   ')',\n",
              "   '(',\n",
              "   '968',\n",
              "   'MW',\n",
              "   ')'],\n",
              "  [';', \"'\", \"''\", 'Anders', 'Lindström', \"''\", \"'\"]],\n",
              " 'ner_tags': [[3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0], [0, 0, 0, 1, 2, 0, 0]],\n",
              " 'langs': [['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'],\n",
              "  ['en', 'en', 'en', 'en', 'en', 'en', 'en']],\n",
              " 'spans': [['ORG: R.H. Saunders', 'ORG: St. Lawrence River'],\n",
              "  ['PER: Anders Lindström']]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8yIdgPmYclq"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfKJrBniq9eS"
      },
      "source": [
        "- Bert expects input in `input_ids`, `token_type_ids` and `attention_mask` format\n",
        "- The label also requires adjustment due to subword tokenization used by BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yaH8jnb-6aLO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXYojTrc1f_0"
      },
      "source": [
        "### Let's see why we need to adjust the labels\n",
        "\n",
        "- We will process the tokens using tokenizer object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "in4FMztz20M4"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"tokens\"], padding=\"max_length\", truncation=True, is_split_into_words=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PwWB89V320Jf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 20000/20000 [00:01<00:00, 11011.63 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets_ = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N2_ltXdZ20G8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[101,\n",
              " 1054,\n",
              " 1012,\n",
              " 1044,\n",
              " 1012,\n",
              " 15247,\n",
              " 1006,\n",
              " 2358,\n",
              " 1012,\n",
              " 5623,\n",
              " 2314,\n",
              " 1007,\n",
              " 1006,\n",
              " 5986,\n",
              " 2620,\n",
              " 12464,\n",
              " 1007,\n",
              " 102,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets_['train'][0]['input_ids'][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7wqqPSvj6LvV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets_['train'][0]['ner_tags'][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PqvSbwI112_t"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_datasets_['train'][0]['input_ids']) == len(tokenized_datasets_['train'][0]['ner_tags'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeVi-DT_6jVG"
      },
      "source": [
        "- We can see that len of `input_ids` is not matching with `ner_tags` that's why we require to adjust the labels according to the tokenized output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeK0Bv5m1vCs"
      },
      "source": [
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6FGMjSS2kXw"
      },
      "source": [
        "- We will use the argument truncation=True (to truncate texts that are bigger than the maximum size allowed by the model) as there is a sequence in data which has length>512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3fAoZmDdYcE6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 10000/10000 [00:00<00:00, 28447.32 examples/s]\n",
            "Map: 100%|██████████| 10000/10000 [00:00<00:00, 31084.55 examples/s]\n",
            "Map: 100%|██████████| 20000/20000 [00:00<00:00, 30543.81 examples/s]\n"
          ]
        }
      ],
      "source": [
        "#Get the values for input_ids, attention_mask, adjusted labels\n",
        "def tokenize_adjust_labels(all_samples_per_split):\n",
        "  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True, truncation=True)\n",
        "\n",
        "  total_adjusted_labels = []\n",
        "\n",
        "  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
        "    prev_wid = -1\n",
        "    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
        "    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
        "    i = -1\n",
        "    adjusted_label_ids = []\n",
        "\n",
        "    for word_idx in word_ids_list:\n",
        "      # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "      # ignored in the loss function.\n",
        "      if(word_idx is None):\n",
        "        adjusted_label_ids.append(-100)\n",
        "      elif(word_idx!=prev_wid):\n",
        "        i = i + 1\n",
        "        adjusted_label_ids.append(existing_label_ids[i])\n",
        "        prev_wid = word_idx\n",
        "      else:\n",
        "        label_name = label_names[existing_label_ids[i]]\n",
        "        adjusted_label_ids.append(existing_label_ids[i])\n",
        "\n",
        "    total_adjusted_labels.append(adjusted_label_ids)\n",
        "\n",
        "  #add adjusted labels to the tokenized samples\n",
        "  tokenized_samples[\"labels\"] = total_adjusted_labels\n",
        "  return tokenized_samples\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True, remove_columns=['tokens', 'ner_tags', 'langs', 'spans'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teMUwqXiW9IY"
      },
      "source": [
        "- To understand word ids, consider following example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Psi3CWY9XGBN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2986, 8694, 11265, 2099, 1999, 8224, 15270, 2497, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = tokenizer(\"Fine tune NER in google colab!\")\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MH2FE6fUWOsv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 2, 3, 4, 5, 5, 6, None]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.word_ids(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limsUXrUXCQ8"
      },
      "source": [
        "Here, we can see 2 and 5 ids are repeated twice due to sub-word tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW7_lOfYqp8W"
      },
      "source": [
        "- We will now have all the required fields for training, 'input_ids', 'token_type_ids', 'attention_mask', 'labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tbjxPbzXqeDd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "n2p9F2nGqmyn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[101,\n",
              "   1054,\n",
              "   1012,\n",
              "   1044,\n",
              "   1012,\n",
              "   15247,\n",
              "   1006,\n",
              "   2358,\n",
              "   1012,\n",
              "   5623,\n",
              "   2314,\n",
              "   1007,\n",
              "   1006,\n",
              "   5986,\n",
              "   2620,\n",
              "   12464,\n",
              "   1007,\n",
              "   102],\n",
              "  [101,\n",
              "   1025,\n",
              "   1005,\n",
              "   1005,\n",
              "   1005,\n",
              "   15387,\n",
              "   11409,\n",
              "   5104,\n",
              "   13887,\n",
              "   1005,\n",
              "   1005,\n",
              "   1005,\n",
              "   102]],\n",
              " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              " 'labels': [[-100, 3, 3, 3, 3, 4, 0, 3, 3, 4, 4, 0, 0, 0, 0, 0, 0, -100],\n",
              "  [-100, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, -100]]}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset['train'][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aURLeBrsCz"
      },
      "source": [
        "- As we can see, different sample have different length therefore we need to\n",
        "pad the tokens to have same length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwd6wLo-stzS"
      },
      "source": [
        "- https://huggingface.co/docs/transformers/main/main_classes/data_collator#transformers.DataCollatorForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5Vdz1YTAragY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oS98dpbUsHfE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataCollatorForTokenClassification(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUlrCMFctHGN"
      },
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7sFRXB7IsI6s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForTokenClassification, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cEvT2ePDt7CB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check if gpu is present\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BAW3u9v28gY"
      },
      "source": [
        "- We will use Distillbert-base-uncased model for fine tuning\n",
        "- We need to specify the number of labels present in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "B3eooh_yt8nC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistilBertForTokenClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_names))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xcmvgctxCxJ"
      },
      "source": [
        "- Create a function to generate metrics\n",
        "- We will use `seqeval` metrics, commonly used for token classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vMk846nk87FZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lI45-NU1xqV2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/8c/38d5fhqx2_jf385d_b3cq3j00000gn/T/ipykernel_6544/4044278287.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"seqeval\")\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "\n",
        "    #select predicted index with maximum logit for each token\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "al2A2hiVBB28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
              " 'overall_precision': 1.0,\n",
              " 'overall_recall': 1.0,\n",
              " 'overall_f1': 1.0,\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = dataset[\"train\"][1]\n",
        "labels = [label_names[i] for i in example[f\"ner_tags\"]]\n",
        "metric.compute(predictions=[labels], references=[labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjFwNtQQ33FA"
      },
      "source": [
        "- Fine Tuning using Trainer API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "28E8ZpWVRr1q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from accelerate) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from accelerate) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: requests in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/owenmonroe/anaconda3/envs/textminingvis/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "u7OHFke0wWIt"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "batch_size = 16\n",
        "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
        "epochs = 2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"Lab11_Nov13_NER/NER-BERT-colab\",\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    logging_steps=logging_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "S_ywBZhpwWFR"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IqWyTbZ80r8H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101,\n",
              "  1054,\n",
              "  1012,\n",
              "  1044,\n",
              "  1012,\n",
              "  15247,\n",
              "  1006,\n",
              "  2358,\n",
              "  1012,\n",
              "  5623,\n",
              "  2314,\n",
              "  1007,\n",
              "  1006,\n",
              "  5986,\n",
              "  2620,\n",
              "  12464,\n",
              "  1007,\n",
              "  102],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'labels': [-100, 3, 3, 3, 3, 4, 0, 3, 3, 4, 4, 0, 0, 0, 0, 0, 0, -100]}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "NbUAdUIHADaM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101,\n",
              "  16615,\n",
              "  4212,\n",
              "  5196,\n",
              "  1006,\n",
              "  16615,\n",
              "  4212,\n",
              "  1010,\n",
              "  2148,\n",
              "  7734,\n",
              "  1007,\n",
              "  102],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'labels': [-100, 3, 4, 4, 0, 5, 6, 6, 6, 6, 0, -100]}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.eval_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "81mGl05TECUI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 33%|███▎      | 832/2500 [2:05:51<08:44,  3.18it/s]     "
          ]
        }
      ],
      "source": [
        "#fine tune using train method\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaP_mXvSgu_p"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJWzwcECtLh_"
      },
      "source": [
        "To get the precision/recall/f1 computed for each category for test set, we can apply the same function as before on the result of the `predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0xDmf9ssd-x"
      },
      "outputs": [],
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_dataset[\"test\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPYxuTQwxNCD"
      },
      "source": [
        "## Observations\n",
        "\n",
        "- f1 score for LOC and PER is >85% and ORG has <75%\n",
        "- Overall f1 score is ~83%\n",
        "- We can improve the accuracy by training the model for more number of epochs (currently only 2 epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
