{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dee59a17"
   },
   "source": [
    "This notebook describes named entity recognition (NER) using conditional random fields (CRFs), which uses a sequence labeling approach to identify entities in text. The notebook has been adapted from https://github.com/steveneale/ner_crf/blob/master/ner_crf.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f189ea18"
   },
   "source": [
    "### Named entity recognition using spacy\n",
    "We will first see how NER can be done using an existing model, such as one that comes with spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed4749e3",
    "outputId": "e7e6c3aa-ddc5-4cc0-9c4d-cbcde48c9696"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "tweet = \"all the pasta, pasta sauce and pizza were sold out at the grocery store. did everyone in dallas become italian grandmas? #dallas #coronapocolypse #covid2019\"\n",
    "doc = nlp(tweet)\n",
    "print(tweet)\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95352a70"
   },
   "source": [
    "The results from spacy for named entities is not great! Only hashtag-related items were recognized as named entities and their labels (PERSON) don't make a lot of sense. \n",
    "\n",
    "Now let's improve the capitalization of this tweet and remove the hashtags, making it more like sentences you might see in a news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56b371ca",
    "outputId": "b52b740d-9f8c-4a66-8ebe-e98bcd93b46f"
   },
   "outputs": [],
   "source": [
    "improved_tweet=\"All the pasta, pasta sauce and pizza were sold out at the grocery store. Did everyone in Dallas become Italian grandmas?\"\n",
    "doc = nlp(improved_tweet)\n",
    "print(improved_tweet)\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95352a70"
   },
   "source": [
    "The results are better now, because the standard spacy NER model was trained on news articles.\n",
    "\n",
    "As a first step for your NER needs, you might want to try to test spacy or similar tools (e.g., Stanza) to whether they help your task. Note that these tools often come with multiple models for NER trained on different corpora, so it is important to select the most appropriate one for your use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02998293"
   },
   "source": [
    "### Training our own NER model using CRFs\n",
    "You may have specific NER needs that are unmet by existing models, in which case you can train a CRF NER model (among other possibilities), assuming you have labeled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e97bb8a",
    "outputId": "7cb4d71e-87ed-4959-e7f1-a4853e60a358",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "583c7191"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# from model_plots import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59475145"
   },
   "source": [
    "#### Load the NER dataset into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f34bdb1d"
   },
   "source": [
    "First, the data is loaded into a Pandas DataFrame. This can be done easily using the `read_csv` function, specifying that the separator is a space. It's also useful to keep the blank lines, which are helpful later for determining the sentence breaks.\n",
    "\n",
    "Once the data is loaded into a DataFrame, the easy access we have to columns allows a couple of useful things to be done - group the data by the \"ne\" column to see the distributions of each tag, and extract the classes (disregarding 'O' and blank lines with NaN values) as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "kAmX6xpPg26q",
    "outputId": "df0cc179-6e19-4511-c02a-57ab1765a301"
   },
   "outputs": [],
   "source": [
    "# # Upload data files if you are working on colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVf2TkY9g9jz"
   },
   "outputs": [],
   "source": [
    "# # Use this if you are using colab\n",
    "# train_file = io.BytesIO(uploaded['train.txt'])\n",
    "# test_file = io.BytesIO(uploaded['test.txt'])\n",
    "# valid_file = io.BytesIO(uploaded['valid.txt'])\n",
    "\n",
    "# Use this if your are working locally\n",
    "train_file = \"train.txt\"\n",
    "test_file = \"test.txt\"\n",
    "valid_file = \"valid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b99ede5",
    "outputId": "578d6778-83d6-4b1f-9979-2e93f441f29e"
   },
   "outputs": [],
   "source": [
    "# Read the NER data using spaces as separators, keeping blank lines and adding columns\n",
    "ner_data = pd.read_csv(train_file, sep=\" \", header=None, skip_blank_lines=False, encoding=\"utf-8\")\n",
    "ner_data.columns = [\"token\", \"pos\", \"chunk\", \"ne\"]\n",
    "\n",
    "# Explore the distribution of NE tags in the dataset\n",
    "tag_distribution = ner_data.groupby(\"ne\").size().reset_index(name='counts')\n",
    "print(tag_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3a73b43",
    "outputId": "85659547-be19-4f6a-9ea6-59986de65fc6"
   },
   "outputs": [],
   "source": [
    "# Extract the useful classes (B and I tags, not 'O' or NaN values) as a list\n",
    "classes = list(filter(lambda x: x not in [\"O\", np.nan], list(ner_data[\"ne\"].unique())))\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62603cd1"
   },
   "source": [
    "## Extract sentences from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67ce1abe"
   },
   "source": [
    "Next, sentences need to be extracted from the data - it's useful to have the sentences as a list of lists, with each sublist containing the token, POS tag, syntactic chunk, and NE label for every word token in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee445cdb"
   },
   "outputs": [],
   "source": [
    "# Create a sentences dictionary and an initial single sentence dictionary\n",
    "sentences, sentence = [], []\n",
    "\n",
    "# For each row in the NER data...\n",
    "for index, row in ner_data.iterrows():\n",
    "    # If the row is empty (no string in the token column)\n",
    "    if type(row[\"token\"]) != str:\n",
    "        # If the current sentence is not empty, append it to the sentences and create a new sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "    # Otherwise...\n",
    "    else:\n",
    "        # If the row does not indicate the start of a document, add the token to the current sentence\n",
    "        if type(row[\"token\"]) != float and type(row[\"pos\"]) != float and type(row[\"ne\"]) != float:\n",
    "            if not row[\"token\"].startswith(\"-DOCSTART-\"):\n",
    "                sentence.append([row[\"token\"], row[\"pos\"], row[\"chunk\"], row[\"ne\"]])\n",
    "#     pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4EB7HjZHOgV",
    "outputId": "a124a45a-ac24-4e7c-f119-5c466963d5e1"
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01e49600"
   },
   "source": [
    "## Extract sentence features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3f44ad7"
   },
   "source": [
    "The 'sklearn-crfsuite' website provides a tutorial on their CRF model, which contains sample code for extracting word features as a dictionary ready-formatted for use with the model. The function below is based on their model, making use of:\n",
    "\n",
    "* Current words\n",
    "* Previous words\n",
    "* Next words\n",
    "* Current POS tags\n",
    "* Previous and next POS tags\n",
    "\n",
    "These features are all used in the Stanford NLP group's work on using CRFs for NER (Finkel et al., 2005). They also make use of a 'current word shape' feature, which generally shows the upper-cased letters, lower-cased letters, and digits that make up a word (For example, 'CoNLL-2003' => 'XxXXX-dddd'). In the 'sclearn-crfsuite' implementation below, the 'word.isupper()', 'word.istitle()', and 'word.isdigit()' features are used in place of this.\n",
    "\n",
    "The function below has also had a flag added to it to include chunk tags from the training data as features, for the current, previous, and next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8f8ffcc"
   },
   "outputs": [],
   "source": [
    "def word_features(sentence, i, use_chunks=False):\n",
    "    # Get the current word and POS\n",
    "    word = sentence[i][0]\n",
    "    pos = sentence[i][1]\n",
    "    # Create a feature dictionary, based on characteristics of the current word and POS\n",
    "    features = { \"bias\": 1.0,\n",
    "                 \"word.lower()\": word.lower(),\n",
    "                 \"word[-3:]\": word[-3:],\n",
    "                 \"word[-2:]\": word[-2:],\n",
    "                 \"word.isupper()\": word.isupper(),\n",
    "                 \"word.istitle()\": word.istitle(),\n",
    "                 \"word.isdigit()\": word.isdigit(),\n",
    "                 \"pos\": pos,\n",
    "                 \"pos[:2]\": pos[:2], # POS category, like verb, noun, rather than the fine-grained POS tag\n",
    "               }\n",
    "    # If chunks are being used, add the current chunk to the feature dictionary\n",
    "    if use_chunks:\n",
    "        chunk = sentence[i][2]\n",
    "        features.update({ \"chunk\": chunk })\n",
    "    # If this is not the first word in the sentence...\n",
    "    if i > 0:\n",
    "        # Get the sentence's previous word and POS\n",
    "        prev_word = sentence[i-1][0]\n",
    "        prev_pos = sentence[i-1][1]\n",
    "        # Add characteristics of the sentence's previous word and POS to the feature dictionary\n",
    "        features.update({ \"-1:word.lower()\": prev_word.lower(),\n",
    "                          \"-1:word.istitle()\": prev_word.istitle(),\n",
    "                          \"-1:word.isupper()\": prev_word.isupper(),\n",
    "                          \"-1:pos\": prev_pos,\n",
    "                          \"-1:pos[:2]\": prev_pos[:2],\n",
    "                        })\n",
    "        # If chunks are being used, add the previous chunk to the feature dictionary\n",
    "        if use_chunks:\n",
    "            prev_chunk = sentence[i-1][2]\n",
    "            features.update({ \"-1:chunk\": prev_chunk })\n",
    "    # Otherwise, add 'BOS' (beginning of sentence) to the feature dictionary\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "    # If this is not the last word in the sentence...\n",
    "    if i < len(sentence)-1:\n",
    "        # Get the sentence's next word and POS\n",
    "        next_word = sentence[i+1][0]\n",
    "        next_pos = sentence[i+1][1]\n",
    "        # Add characteristics of the sentence's previous next and POS to the feature dictionary\n",
    "        features.update({ \"+1:word.lower()\": next_word.lower(),\n",
    "                          \"+1:word.istitle()\": next_word.istitle(),\n",
    "                          \"+1:word.isupper()\": next_word.isupper(),\n",
    "                          \"+1:pos\": next_pos,\n",
    "                          \"+1:pos[:2]\": next_pos[:2],\n",
    "                        })\n",
    "        # If chunks are being used, add the next chunk to the feature dictionary\n",
    "        if use_chunks:\n",
    "            next_chunk = sentence[i+1][2]\n",
    "            features.update({ \"+1:chunk\": next_chunk })\n",
    "    # Otherwise, add 'EOS' (end of sentence) to the feature dictionary\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "    # Return the feature dictionary\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46b9b466"
   },
   "source": [
    "Using the word_features function, a list of feature dictionaries for each word token in a sentence can be extracted, corresponding to a list of NE labels for each word token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48797618"
   },
   "outputs": [],
   "source": [
    "# Return a feature dictionary for each word in a given sentence\n",
    "def sentence_features(sentence, use_chunks=False):\n",
    "    return [word_features(sentence, i, use_chunks) for i in range(len(sentence))]\n",
    "\n",
    "# Return the label (NER tag) for each word in a given sentence\n",
    "def sentence_labels(sentence):\n",
    "    return [label for token, pos, chunk, label in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f19f63fc"
   },
   "source": [
    "## Split the sentences into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be979a94"
   },
   "source": [
    "Using the predefined functions, X and y can be extracted as lists of feature dictionaries for each word token in each sentence, and as lists of NE labels for each word token in each sentence, respectively. scikit-learn's 'test_train_split' function can then be used to split X and y into training and test sets, split 80% training to 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7085562b",
    "outputId": "a396351b-26e1-416e-a43d-7d3e798c92e0"
   },
   "outputs": [],
   "source": [
    "# For each sentence, extract the sentence features as X, and the labels as y\n",
    "X = [sentence_features(sentence) for sentence in sentences]\n",
    "y = [sentence_labels(sentence) for sentence in sentences]\n",
    "\n",
    "# Split X and y into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"First token features:\\n{}\\n{}\".format(\"-\"*21, X_train[0][0]))\n",
    "print(\"\\nFirst token label:\\n{}\\n{}\".format(\"-\"*18, y_train[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c1dbcfc"
   },
   "source": [
    "## Train a CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "463866e3"
   },
   "source": [
    "The training data can now be used to train a CRF model to map the feature dictionaries to output NE labels. CRF's have been a popular choice for training named entity recognition models following the success of the Stanford NLP group's work on NER (Finkel et al., 2005). The model employs the gradient descent-based L-BFGS algorithm, and uses elastic net (C1+C2) regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84c008c5"
   },
   "outputs": [],
   "source": [
    "CRF._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IIjZ3geiW_x"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.23.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a53919f3",
    "outputId": "88a962f8-f488-47dc-c352-141ac82818b1"
   },
   "outputs": [],
   "source": [
    "# Create a new CRF model\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "crf = CRF(algorithm=\"lbfgs\",\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=True)\n",
    "\n",
    "\n",
    "# Train the CRF model on the supplied training data\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1edaee9d"
   },
   "source": [
    "## Evalute the CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8602746f"
   },
   "source": [
    "The trained model can now be used to make predictions based on the test data, which can in turn be compared to the expected labels from the test data to produce a classification report (precision, recall and F1 scores).\n",
    "\n",
    "The model is performing pretty well, with a 91% F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76676b23",
    "outputId": "0086fbf1-10ac-48e3-e005-b264098a4edb"
   },
   "outputs": [],
   "source": [
    "# Use the CRF model to make predictions on the test data\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi-BvcW4E4JH"
   },
   "source": [
    "Check the state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIvpAbJQE3Ri",
    "outputId": "44c3dce0-5a33-4b18-afc4-f0b8a1af9d3c"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d161f666"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b3acd13"
   },
   "source": [
    "Finkel, J.R., Grenager, T. & Manning, C. (2005). 'Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling'. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL '05). pp. 363–370.\n",
    "\n",
    "sklearn-crfsuite (n.d.). 'Tutorial - scklearn-crfsuite 0.3 documentation' [https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#features]. Accessed 2018-11-30."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
