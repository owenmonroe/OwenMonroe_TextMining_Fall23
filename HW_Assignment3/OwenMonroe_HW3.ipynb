{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is Owen Monroe's Homework Assignment # 3 for Text Mining IS 567\n",
    "October 8, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train and evaluate a unigram-based baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829\n",
      "458\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/21/2023 16:42</td>\n",
       "      <td>Much more accessible for blind users than the ...</td>\n",
       "      <td>Up to this point I?€?ve mostly been using Chat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/11/2023 12:24</td>\n",
       "      <td>Much anticipated, wasn?€?t let down.</td>\n",
       "      <td>I?€?ve been a user since it?€?s initial roll o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/19/2023 10:16</td>\n",
       "      <td>Almost 5 stars, but?€? no search function</td>\n",
       "      <td>This app would almost be perfect if it wasn?€?...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/27/2023 21:57</td>\n",
       "      <td>4.5 stars, here?€?s why</td>\n",
       "      <td>I recently downloaded the app and overall, it'...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/9/2023 7:49</td>\n",
       "      <td>Good, but Siri support would take it to the ne...</td>\n",
       "      <td>I appreciate the devs implementing Siri suppor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>6/4/2023 10:50</td>\n",
       "      <td>pad os???</td>\n",
       "      <td>Please make a iPad version of this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>5/19/2023 1:23</td>\n",
       "      <td>Goated app</td>\n",
       "      <td>Best app</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>6/21/2023 20:02</td>\n",
       "      <td>Co</td>\n",
       "      <td>Why it?€?s not available in Ethiopia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>6/7/2023 10:25</td>\n",
       "      <td>Crazy world views</td>\n",
       "      <td>It agrees with letting children be forced into...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>5/18/2023 23:07</td>\n",
       "      <td>Good app</td>\n",
       "      <td>It?€?s good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1829 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                              title  \\\n",
       "0     5/21/2023 16:42  Much more accessible for blind users than the ...   \n",
       "1     7/11/2023 12:24               Much anticipated, wasn?€?t let down.   \n",
       "2     5/19/2023 10:16          Almost 5 stars, but?€? no search function   \n",
       "3     5/27/2023 21:57                            4.5 stars, here?€?s why   \n",
       "4       6/9/2023 7:49  Good, but Siri support would take it to the ne...   \n",
       "...               ...                                                ...   \n",
       "1829   6/4/2023 10:50                                          pad os???   \n",
       "1830   5/19/2023 1:23                                         Goated app   \n",
       "1831  6/21/2023 20:02                                                 Co   \n",
       "1832   6/7/2023 10:25                                  Crazy world views   \n",
       "1833  5/18/2023 23:07                                           Good app   \n",
       "\n",
       "                                                 review  rating  \n",
       "0     Up to this point I?€?ve mostly been using Chat...       4  \n",
       "1     I?€?ve been a user since it?€?s initial roll o...       4  \n",
       "2     This app would almost be perfect if it wasn?€?...       4  \n",
       "3     I recently downloaded the app and overall, it'...       4  \n",
       "4     I appreciate the devs implementing Siri suppor...       4  \n",
       "...                                                 ...     ...  \n",
       "1829                 Please make a iPad version of this       1  \n",
       "1830                                           Best app       5  \n",
       "1831               Why it?€?s not available in Ethiopia       1  \n",
       "1832  It agrees with letting children be forced into...       1  \n",
       "1833                                        It?€?s good       5  \n",
       "\n",
       "[1829 rows x 4 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Data and Removing Empties\n",
    "\n",
    "train_df = pd.read_csv('chatgpt_train.csv')\n",
    "test_df = pd.read_csv('chatgpt_test.csv')\n",
    "\n",
    "full_train_df = train_df.dropna()\n",
    "full_test_df = test_df.dropna()\n",
    "\n",
    "print(len(full_train_df['review'].to_list()))\n",
    "print(len(full_test_df['review'].to_list()))\n",
    "\n",
    "full_train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Convert Ratings to Three-Way Ratings\n",
    "\n",
    "def change_five_to_three(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 'negative'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    elif rating in [4, 5]:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/38d5fhqx2_jf385d_b3cq3j00000gn/T/ipykernel_55994/896341391.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_train_df['three rating'] = three_rate_train_list\n"
     ]
    }
   ],
   "source": [
    "# New Rating List for Train and Test Data\n",
    "\n",
    "three_rate_train_list = full_train_df['rating'].apply(change_five_to_three).to_list()\n",
    "three_rate_test_list = full_test_df['rating'].apply(change_five_to_three).to_list()\n",
    "\n",
    "\n",
    "full_train_df['three rating'] = three_rate_train_list\n",
    "full_test_df['three rating'] = three_rate_test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>three rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/21/2023 16:42</td>\n",
       "      <td>Much more accessible for blind users than the ...</td>\n",
       "      <td>Up to this point I?€?ve mostly been using Chat...</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/11/2023 12:24</td>\n",
       "      <td>Much anticipated, wasn?€?t let down.</td>\n",
       "      <td>I?€?ve been a user since it?€?s initial roll o...</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/19/2023 10:16</td>\n",
       "      <td>Almost 5 stars, but?€? no search function</td>\n",
       "      <td>This app would almost be perfect if it wasn?€?...</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/27/2023 21:57</td>\n",
       "      <td>4.5 stars, here?€?s why</td>\n",
       "      <td>I recently downloaded the app and overall, it'...</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/9/2023 7:49</td>\n",
       "      <td>Good, but Siri support would take it to the ne...</td>\n",
       "      <td>I appreciate the devs implementing Siri suppor...</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>6/4/2023 10:50</td>\n",
       "      <td>pad os???</td>\n",
       "      <td>Please make a iPad version of this</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>5/19/2023 1:23</td>\n",
       "      <td>Goated app</td>\n",
       "      <td>Best app</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>6/21/2023 20:02</td>\n",
       "      <td>Co</td>\n",
       "      <td>Why it?€?s not available in Ethiopia</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>6/7/2023 10:25</td>\n",
       "      <td>Crazy world views</td>\n",
       "      <td>It agrees with letting children be forced into...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>5/18/2023 23:07</td>\n",
       "      <td>Good app</td>\n",
       "      <td>It?€?s good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1829 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                              title  \\\n",
       "0     5/21/2023 16:42  Much more accessible for blind users than the ...   \n",
       "1     7/11/2023 12:24               Much anticipated, wasn?€?t let down.   \n",
       "2     5/19/2023 10:16          Almost 5 stars, but?€? no search function   \n",
       "3     5/27/2023 21:57                            4.5 stars, here?€?s why   \n",
       "4       6/9/2023 7:49  Good, but Siri support would take it to the ne...   \n",
       "...               ...                                                ...   \n",
       "1829   6/4/2023 10:50                                          pad os???   \n",
       "1830   5/19/2023 1:23                                         Goated app   \n",
       "1831  6/21/2023 20:02                                                 Co   \n",
       "1832   6/7/2023 10:25                                  Crazy world views   \n",
       "1833  5/18/2023 23:07                                           Good app   \n",
       "\n",
       "                                                 review  rating three rating  \n",
       "0     Up to this point I?€?ve mostly been using Chat...       4     positive  \n",
       "1     I?€?ve been a user since it?€?s initial roll o...       4     positive  \n",
       "2     This app would almost be perfect if it wasn?€?...       4     positive  \n",
       "3     I recently downloaded the app and overall, it'...       4     positive  \n",
       "4     I appreciate the devs implementing Siri suppor...       4     positive  \n",
       "...                                                 ...     ...          ...  \n",
       "1829                 Please make a iPad version of this       1     negative  \n",
       "1830                                           Best app       5     positive  \n",
       "1831               Why it?€?s not available in Ethiopia       1     negative  \n",
       "1832  It agrees with letting children be forced into...       1     negative  \n",
       "1833                                        It?€?s good       5     positive  \n",
       "\n",
       "[1829 rows x 5 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>three rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/19/2023 6:09</td>\n",
       "      <td>error unsupported country</td>\n",
       "      <td>cant login</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/19/2023 9:39</td>\n",
       "      <td>Hype junk</td>\n",
       "      <td>More harm than help.</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/19/2023 4:12</td>\n",
       "      <td>your gpt 4 is fake</td>\n",
       "      <td>Fix it</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/20/2023 3:01</td>\n",
       "      <td>Please impose IPadOS</td>\n",
       "      <td>We need IPadOS!!!</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/19/2023 20:49</td>\n",
       "      <td>Amazing</td>\n",
       "      <td>Great</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>5/19/2023 0:17</td>\n",
       "      <td>Andrew Justino Wilson 5/19/23</td>\n",
       "      <td>This has to be a beginning to something crazy ...</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>5/18/2023 19:13</td>\n",
       "      <td>Superb AI</td>\n",
       "      <td>I?€?ve been using chat and have been a proud p...</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>5/18/2023 18:27</td>\n",
       "      <td>Fantastic App with Room for Enhancements</td>\n",
       "      <td>The ChatGPT iOS app is an outstanding product....</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>5/18/2023 17:17</td>\n",
       "      <td>Awesome technology, deplorable tactics</td>\n",
       "      <td>Sam Altman?€?s blatant attempt at regulatory c...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>7/25/2023 0:50</td>\n",
       "      <td>I like how there r no limits thanks &lt;33</td>\n",
       "      <td>.</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date                                     title  \\\n",
       "0     5/19/2023 6:09                 error unsupported country   \n",
       "1     5/19/2023 9:39                                 Hype junk   \n",
       "2     5/19/2023 4:12                        your gpt 4 is fake   \n",
       "3     5/20/2023 3:01                      Please impose IPadOS   \n",
       "4    5/19/2023 20:49                                   Amazing   \n",
       "..               ...                                       ...   \n",
       "453   5/19/2023 0:17             Andrew Justino Wilson 5/19/23   \n",
       "454  5/18/2023 19:13                                 Superb AI   \n",
       "455  5/18/2023 18:27  Fantastic App with Room for Enhancements   \n",
       "456  5/18/2023 17:17    Awesome technology, deplorable tactics   \n",
       "457   7/25/2023 0:50   I like how there r no limits thanks <33   \n",
       "\n",
       "                                                review  rating three rating  \n",
       "0                                           cant login       2     negative  \n",
       "1                                 More harm than help.       1     negative  \n",
       "2                                               Fix it       1     negative  \n",
       "3                                    We need IPadOS!!!       5     positive  \n",
       "4                                                Great       5     positive  \n",
       "..                                                 ...     ...          ...  \n",
       "453  This has to be a beginning to something crazy ...       5     positive  \n",
       "454  I?€?ve been using chat and have been a proud p...       5     positive  \n",
       "455  The ChatGPT iOS app is an outstanding product....       5     positive  \n",
       "456  Sam Altman?€?s blatant attempt at regulatory c...       2     negative  \n",
       "457                                                  .       5     positive  \n",
       "\n",
       "[458 rows x 5 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829, 5551) \n",
      "\n",
      "(458, 5551) \n",
      "\n",
      "  (0, 5224)\t1\n",
      "  (0, 4985)\t9\n",
      "  (0, 4932)\t5\n",
      "  (0, 3613)\t1\n",
      "  (0, 5282)\t1\n",
      "  (0, 3134)\t1\n",
      "  (0, 493)\t2\n",
      "  (0, 5253)\t2\n",
      "  (0, 776)\t2\n",
      "  (0, 3331)\t3\n",
      "  (0, 3165)\t2\n",
      "  (0, 5454)\t1\n",
      "  (0, 1302)\t3\n",
      "  (0, 2139)\t1\n",
      "  (0, 806)\t1\n",
      "  (0, 5421)\t1\n",
      "  (0, 2666)\t6\n",
      "  (0, 1426)\t1\n",
      "  (0, 4267)\t3\n",
      "  (0, 3883)\t2\n",
      "  (0, 3184)\t1\n",
      "  (0, 2660)\t3\n",
      "  (0, 3692)\t2\n",
      "  (0, 1352)\t1\n",
      "  (0, 4906)\t16\n",
      "  :\t:\n",
      "  (1824, 2949)\t1\n",
      "  (1824, 3593)\t1\n",
      "  (1824, 2644)\t1\n",
      "  (1824, 5301)\t1\n",
      "  (1825, 291)\t1\n",
      "  (1825, 527)\t1\n",
      "  (1826, 2666)\t1\n",
      "  (1826, 2456)\t1\n",
      "  (1826, 413)\t1\n",
      "  (1826, 3247)\t1\n",
      "  (1826, 5433)\t1\n",
      "  (1826, 1695)\t1\n",
      "  (1827, 2666)\t1\n",
      "  (1827, 474)\t1\n",
      "  (1827, 5464)\t1\n",
      "  (1827, 2619)\t1\n",
      "  (1827, 3372)\t1\n",
      "  (1827, 1976)\t1\n",
      "  (1827, 2807)\t1\n",
      "  (1827, 794)\t1\n",
      "  (1827, 195)\t1\n",
      "  (1827, 3699)\t1\n",
      "  (1827, 2328)\t1\n",
      "  (1828, 2666)\t1\n",
      "  (1828, 2135)\t1\n"
     ]
    }
   ],
   "source": [
    "train_rev_text = full_train_df[\"review\"]\n",
    "test_rev_text = full_test_df[\"review\"]\n",
    "\n",
    "# Setting n-gram range to uni-grams\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "# Creating training data representation\n",
    "train_rev_uni_cv = vectorizer.fit_transform(train_rev_text.values.astype('U'))\n",
    "print(train_rev_uni_cv.shape,\"\\n\") \n",
    "\n",
    "# Creating test data representation\n",
    "test_rev_uni_cv = vectorizer.transform(test_rev_text.values.astype('U'))\n",
    "print(test_rev_uni_cv.shape,\"\\n\") \n",
    "\n",
    "print(train_rev_uni_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'neutral', 'neutral', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv = train_rev_uni_cv\n",
    "y_train_rev_series = full_train_df[\"three rating\"]\n",
    "x_test_cv = test_rev_uni_cv\n",
    "y_test_rev_series = full_test_df[\"three rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions = mnb_model.predict(x_test_cv)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list = list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reports - Unigrams NB Baseline\n",
      "\n",
      "Accuracy Score: 0.7358078602620087\n",
      "\n",
      "Precision Scores\n",
      "macro precision score for unigrams = 0.5420844714259355\n",
      "micro precision score for unigrams = 0.7358078602620087\n",
      "weighted precision score for unigrams = 0.7128098567692405\n",
      "\n",
      "Recall Scores\n",
      "macro recall score for unigrams = 0.5007752841737103\n",
      "micro recall score for unigrams = 0.7358078602620087\n",
      "weighted recall score for unigrams = 0.7358078602620087\n",
      "\n",
      "F1 Scores\n",
      "macro F1 score for unigrams = 0.5116057233704292\n",
      "micro F1 score for unigrams = 0.7358078602620087\n",
      "weighted F1 score for unigrams = 0.7176894078769239\n"
     ]
    }
   ],
   "source": [
    "# Getting Evaluation Metrics for NB Unigrams \n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list = list(predictions)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predict_label_list)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_list, predict_label_list, average='macro')\n",
    "prec_score_micro = precision_score(true_label_list, predict_label_list, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_list, predict_label_list, average='weighted')\n",
    "\n",
    "rec_score_macro = recall_score(true_label_list, predict_label_list, average='macro')\n",
    "rec_score_micro = recall_score(true_label_list, predict_label_list, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_list, predict_label_list, average='weighted')\n",
    "\n",
    "f1_score_macro = f1_score(true_label_list, predict_label_list, average='macro')\n",
    "f1_score_micro = f1_score(true_label_list, predict_label_list, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_list, predict_label_list, average='weighted')\n",
    "\n",
    "\n",
    "print('Evaluation Reports - Unigrams NB Baseline')\n",
    "print()\n",
    "print(f'Accuracy Score: {acc_score}')\n",
    "print()\n",
    "print('Precision Scores')\n",
    "print(f'macro precision score for unigrams = {prec_score_macro}')\n",
    "print(f'micro precision score for unigrams = {prec_score_micro}')\n",
    "print(f'weighted precision score for unigrams = {prec_score_weighted}')\n",
    "print()\n",
    "print('Recall Scores')\n",
    "print(f'macro recall score for unigrams = {rec_score_macro}')\n",
    "print(f'micro recall score for unigrams = {rec_score_micro}')\n",
    "print(f'weighted recall score for unigrams = {rec_score_weighted}')\n",
    "print()\n",
    "print('F1 Scores')\n",
    "print(f'macro F1 score for unigrams = {f1_score_macro}')\n",
    "print(f'micro F1 score for unigrams = {f1_score_micro}')\n",
    "print(f'weighted F1 score for unigrams = {f1_score_weighted}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove Features with Low Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space before filtering:  (1829, 5551)\n",
      "Train feature space after filtering:  (1829, 3054)\n",
      "Test feature space before filtering:  (458, 5551)\n",
      "Test feature space after filtering:  (458, 3054)\n",
      "  (0, 389)\t1\n",
      "  (0, 1619)\t1\n",
      "  (1, 1243)\t1\n",
      "  (1, 1267)\t1\n",
      "  (1, 1753)\t1\n",
      "  (1, 2682)\t1\n",
      "  (2, 1067)\t1\n",
      "  (2, 1478)\t1\n",
      "  (3, 1467)\t1\n",
      "  (3, 1789)\t1\n",
      "  (3, 2960)\t1\n",
      "  (4, 1204)\t1\n",
      "  (5, 164)\t1\n",
      "  (5, 1187)\t1\n",
      "  (5, 1473)\t1\n",
      "  (5, 2168)\t1\n",
      "  (5, 2706)\t1\n",
      "  (6, 1063)\t1\n",
      "  (7, 164)\t1\n",
      "  (7, 1853)\t1\n",
      "  (8, 1195)\t2\n",
      "  (8, 1473)\t1\n",
      "  (8, 1890)\t1\n",
      "  (8, 2706)\t1\n",
      "  (10, 443)\t1\n",
      "  :\t:\n",
      "  (455, 2912)\t1\n",
      "  (455, 2991)\t1\n",
      "  (455, 3003)\t2\n",
      "  (455, 3004)\t1\n",
      "  (455, 3028)\t5\n",
      "  (456, 62)\t1\n",
      "  (456, 107)\t1\n",
      "  (456, 128)\t1\n",
      "  (456, 214)\t1\n",
      "  (456, 222)\t1\n",
      "  (456, 223)\t1\n",
      "  (456, 281)\t1\n",
      "  (456, 317)\t1\n",
      "  (456, 443)\t1\n",
      "  (456, 772)\t1\n",
      "  (456, 1091)\t1\n",
      "  (456, 1139)\t1\n",
      "  (456, 1846)\t1\n",
      "  (456, 1872)\t2\n",
      "  (456, 2330)\t1\n",
      "  (456, 2483)\t1\n",
      "  (456, 2687)\t1\n",
      "  (456, 2738)\t1\n",
      "  (456, 2972)\t1\n",
      "  (456, 3003)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Removing Features with threshold below 0.001\n",
    "\n",
    "selector = VarianceThreshold(threshold = 0.001)\n",
    "\n",
    "X_train_features_filtered_var_thr_1 = selector.fit(train_rev_uni_cv).transform(train_rev_uni_cv)\n",
    "print (\"Train feature space before filtering: \", train_rev_uni_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_var_thr_1.shape)\n",
    "\n",
    "X_test_features_filtered_var_thr_1 = selector.transform(test_rev_uni_cv)\n",
    "print (\"Test feature space before filtering: \", test_rev_uni_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_var_thr_1.shape)\n",
    "\n",
    "print(X_test_features_filtered_var_thr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'neutral', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training an NB Model for the Feature Space with Low Variance Features Removed threshold below 0.001\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv_low1 = X_train_features_filtered_var_thr_1\n",
    "y_train_rev_series = full_train_df[\"three rating\"]\n",
    "x_test_cv_low1 = X_test_features_filtered_var_thr_1\n",
    "y_test_rev_series = full_test_df[\"three rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv_low1, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions_low1 = mnb_model.predict(x_test_cv_low1)\n",
    "predictions_low1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reports - NB with Low Variance Features Removed, Threshold of 0.001 \n",
      "\n",
      "Accuracy Score: 0.7270742358078602\n",
      "\n",
      "Precision Scores\n",
      "macro precision score = 0.5286054212610907\n",
      "micro precision score = 0.7270742358078602\n",
      "weighted precision score = 0.7060217680304899\n",
      "\n",
      "Recall Scores\n",
      "macro recall score = 0.4974315878104861\n",
      "micro recall score = 0.7270742358078602\n",
      "weighted recall score = 0.7270742358078602\n",
      "\n",
      "F1 Scores\n",
      "macro F1 score = 0.5065362940263409\n",
      "micro F1 score = 0.7270742358078602\n",
      "weighted F1 score = 0.7117444835328599\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics for NB with Low Variance Features Removed, Threshold of 0.001\n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list_low1 = list(predictions_low1)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predict_label_list_low1)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_list, predict_label_list_low1, average='macro')\n",
    "prec_score_micro = precision_score(true_label_list, predict_label_list_low1, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_list, predict_label_list_low1, average='weighted')\n",
    "\n",
    "rec_score_macro = recall_score(true_label_list, predict_label_list_low1, average='macro')\n",
    "rec_score_micro = recall_score(true_label_list, predict_label_list_low1, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_list, predict_label_list_low1, average='weighted')\n",
    "\n",
    "f1_score_macro = f1_score(true_label_list, predict_label_list_low1, average='macro')\n",
    "f1_score_micro = f1_score(true_label_list, predict_label_list_low1, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_list, predict_label_list_low1, average='weighted')\n",
    "\n",
    "\n",
    "print('Evaluation Reports - NB with Low Variance Features Removed, Threshold of 0.001 ')\n",
    "print()\n",
    "print(f'Accuracy Score: {acc_score}')\n",
    "print()\n",
    "print('Precision Scores')\n",
    "print(f'macro precision score = {prec_score_macro}')\n",
    "print(f'micro precision score = {prec_score_micro}')\n",
    "print(f'weighted precision score = {prec_score_weighted}')\n",
    "print()\n",
    "print('Recall Scores')\n",
    "print(f'macro recall score = {rec_score_macro}')\n",
    "print(f'micro recall score = {rec_score_micro}')\n",
    "print(f'weighted recall score = {rec_score_weighted}')\n",
    "print()\n",
    "print('F1 Scores')\n",
    "print(f'macro F1 score = {f1_score_macro}')\n",
    "print(f'micro F1 score = {f1_score_micro}')\n",
    "print(f'weighted F1 score = {f1_score_weighted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space before filtering:  (1829, 5551)\n",
      "Train feature space after filtering:  (1829, 980)\n",
      "Test feature space before filtering:  (458, 5551)\n",
      "Test feature space after filtering:  (458, 980)\n",
      "  (0, 507)\t1\n",
      "  (1, 387)\t1\n",
      "  (1, 546)\t1\n",
      "  (1, 831)\t1\n",
      "  (2, 320)\t1\n",
      "  (2, 459)\t1\n",
      "  (3, 558)\t1\n",
      "  (3, 932)\t1\n",
      "  (4, 367)\t1\n",
      "  (5, 65)\t1\n",
      "  (5, 361)\t1\n",
      "  (5, 455)\t1\n",
      "  (5, 680)\t1\n",
      "  (5, 846)\t1\n",
      "  (6, 318)\t1\n",
      "  (7, 65)\t1\n",
      "  (7, 577)\t1\n",
      "  (8, 364)\t2\n",
      "  (8, 455)\t1\n",
      "  (8, 589)\t1\n",
      "  (8, 846)\t1\n",
      "  (10, 142)\t1\n",
      "  (10, 572)\t1\n",
      "  (10, 579)\t1\n",
      "  (10, 914)\t1\n",
      "  :\t:\n",
      "  (455, 870)\t1\n",
      "  (455, 891)\t1\n",
      "  (455, 900)\t3\n",
      "  (455, 901)\t4\n",
      "  (455, 908)\t1\n",
      "  (455, 913)\t1\n",
      "  (455, 954)\t2\n",
      "  (455, 955)\t1\n",
      "  (455, 966)\t5\n",
      "  (456, 25)\t1\n",
      "  (456, 40)\t1\n",
      "  (456, 83)\t1\n",
      "  (456, 84)\t1\n",
      "  (456, 104)\t1\n",
      "  (456, 142)\t1\n",
      "  (456, 235)\t1\n",
      "  (456, 325)\t1\n",
      "  (456, 342)\t1\n",
      "  (456, 572)\t1\n",
      "  (456, 584)\t2\n",
      "  (456, 775)\t1\n",
      "  (456, 835)\t1\n",
      "  (456, 856)\t1\n",
      "  (456, 936)\t1\n",
      "  (456, 954)\t1\n"
     ]
    }
   ],
   "source": [
    "selector_5 = VarianceThreshold(threshold = 0.005)\n",
    "\n",
    "X_train_features_filtered_var_thr_5 = selector_5.fit(train_rev_uni_cv).transform(train_rev_uni_cv)\n",
    "print (\"Train feature space before filtering: \", train_rev_uni_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_var_thr_5.shape)\n",
    "\n",
    "X_test_features_filtered_var_thr_5 = selector_5.transform(test_rev_uni_cv)\n",
    "print (\"Test feature space before filtering: \", test_rev_uni_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_var_thr_5.shape)\n",
    "\n",
    "print(X_test_features_filtered_var_thr_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'neutral', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'neutral', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'neutral', 'neutral',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training an NB Model for the Feature Space with Low Variance Features Removed threshold below 0.005\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv_low5 = X_train_features_filtered_var_thr_5\n",
    "y_train_rev_series = full_train_df[\"three rating\"]\n",
    "x_test_cv_low5 = X_test_features_filtered_var_thr_5\n",
    "y_test_rev_series = full_test_df[\"three rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv_low5, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions_low5 = mnb_model.predict(x_test_cv_low5)\n",
    "predictions_low5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reports - NB with Low Variance Features Removed, Threshold of 0.005 \n",
      "\n",
      "Accuracy Score: 0.7358078602620087\n",
      "\n",
      "Precision Scores\n",
      "macro precision score = 0.5362426035502958\n",
      "micro precision score = 0.7358078602620087\n",
      "weighted precision score = 0.7181904214361385\n",
      "\n",
      "Recall Scores\n",
      "macro recall score = 0.4995527704912724\n",
      "micro recall score = 0.7358078602620087\n",
      "weighted recall score = 0.7358078602620087\n",
      "\n",
      "F1 Scores\n",
      "macro F1 score = 0.5100713456114153\n",
      "micro F1 score = 0.7358078602620087\n",
      "weighted F1 score = 0.7195576238803115\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics for NB with Low Variance Features Removed, Threshold of 0.005\n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list_low5 = list(predictions_low5)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predict_label_list_low5)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_list, predict_label_list_low5, average='macro')\n",
    "prec_score_micro = precision_score(true_label_list, predict_label_list_low5, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_list, predict_label_list_low5, average='weighted')\n",
    "\n",
    "rec_score_macro = recall_score(true_label_list, predict_label_list_low5, average='macro')\n",
    "rec_score_micro = recall_score(true_label_list, predict_label_list_low5, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_list, predict_label_list_low5, average='weighted')\n",
    "\n",
    "f1_score_macro = f1_score(true_label_list, predict_label_list_low5, average='macro')\n",
    "f1_score_micro = f1_score(true_label_list, predict_label_list_low5, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_list, predict_label_list_low5, average='weighted')\n",
    "\n",
    "\n",
    "print('Evaluation Reports - NB with Low Variance Features Removed, Threshold of 0.005 ')\n",
    "print()\n",
    "print(f'Accuracy Score: {acc_score}')\n",
    "print()\n",
    "print('Precision Scores')\n",
    "print(f'macro precision score = {prec_score_macro}')\n",
    "print(f'micro precision score = {prec_score_micro}')\n",
    "print(f'weighted precision score = {prec_score_weighted}')\n",
    "print()\n",
    "print('Recall Scores')\n",
    "print(f'macro recall score = {rec_score_macro}')\n",
    "print(f'micro recall score = {rec_score_micro}')\n",
    "print(f'weighted recall score = {rec_score_weighted}')\n",
    "print()\n",
    "print('F1 Scores')\n",
    "print(f'macro F1 score = {f1_score_macro}')\n",
    "print(f'micro F1 score = {f1_score_micro}')\n",
    "print(f'weighted F1 score = {f1_score_weighted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select top k-best features using information gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space before filtering:  (1829, 5551)\n",
      "Train feature space after filtering:  (1829, 1000)\n",
      "Test feature space before filtering:  (458, 5551)\n",
      "Test feature space after filtering:  (458, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Selecting K-best features with k = 1000\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=1000)\n",
    "X_train_features_filtered_kbest1 = selector.fit_transform(x_train_cv, y_train_rev_series)\n",
    "print (\"Train feature space before filtering: \", x_train_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_kbest1.shape)\n",
    "\n",
    "X_test_features_filtered_kbest1 = selector.transform(x_test_cv)\n",
    "print (\"Test feature space before filtering: \", x_test_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_kbest1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'positive',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'neutral', 'neutral', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'positive', 'neutral',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'neutral', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training an NB Model for the Feature Space with K-best features k = 1000\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv_k1 = X_train_features_filtered_kbest1\n",
    "y_train_rev_series = full_train_df[\"three rating\"]\n",
    "x_test_cv_k1 = X_test_features_filtered_kbest1\n",
    "y_test_rev_series = full_test_df[\"three rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv_k1, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions_k1 = mnb_model.predict(x_test_cv_k1)\n",
    "predictions_k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reports - NB wfor the Feature Space with K-best features k = 1000\n",
      "\n",
      "Accuracy Score: 0.7489082969432315\n",
      "\n",
      "Precision Scores\n",
      "macro precision score = 0.5667439727843752\n",
      "micro precision score = 0.7489082969432315\n",
      "weighted precision score = 0.7331256698532228\n",
      "\n",
      "Recall Scores\n",
      "macro recall score = 0.5185938663816834\n",
      "micro recall score = 0.7489082969432315\n",
      "weighted recall score = 0.7489082969432315\n",
      "\n",
      "F1 Scores\n",
      "macro F1 score = 0.5327846831403961\n",
      "micro F1 score = 0.7489082969432315\n",
      "weighted F1 score = 0.7320941141075613\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics for NB for the Feature Space with K-best features k = 1000\n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list_k1 = list(predictions_k1)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predict_label_list_k1)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_list, predict_label_list_k1, average='macro')\n",
    "prec_score_micro = precision_score(true_label_list, predict_label_list_k1, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_list, predict_label_list_k1, average='weighted')\n",
    "\n",
    "rec_score_macro = recall_score(true_label_list, predict_label_list_k1, average='macro')\n",
    "rec_score_micro = recall_score(true_label_list, predict_label_list_k1, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_list, predict_label_list_k1, average='weighted')\n",
    "\n",
    "f1_score_macro = f1_score(true_label_list, predict_label_list_k1, average='macro')\n",
    "f1_score_micro = f1_score(true_label_list, predict_label_list_k1, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_list, predict_label_list_k1, average='weighted')\n",
    "\n",
    "\n",
    "print('Evaluation Reports - NB for the Feature Space with K-best features k = 1000')\n",
    "print()\n",
    "print(f'Accuracy Score: {acc_score}')\n",
    "print()\n",
    "print('Precision Scores')\n",
    "print(f'macro precision score = {prec_score_macro}')\n",
    "print(f'micro precision score = {prec_score_micro}')\n",
    "print(f'weighted precision score = {prec_score_weighted}')\n",
    "print()\n",
    "print('Recall Scores')\n",
    "print(f'macro recall score = {rec_score_macro}')\n",
    "print(f'micro recall score = {rec_score_micro}')\n",
    "print(f'weighted recall score = {rec_score_weighted}')\n",
    "print()\n",
    "print('F1 Scores')\n",
    "print(f'macro F1 score = {f1_score_macro}')\n",
    "print(f'micro F1 score = {f1_score_micro}')\n",
    "print(f'weighted F1 score = {f1_score_weighted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space before filtering:  (1829, 5551)\n",
      "Train feature space after filtering:  (1829, 2000)\n",
      "Test feature space before filtering:  (458, 5551)\n",
      "Test feature space after filtering:  (458, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Selecting K-best features with k = 2000\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=2000)\n",
    "X_train_features_filtered_kbest2 = selector.fit_transform(x_train_cv, y_train_rev_series)\n",
    "print (\"Train feature space before filtering: \", x_train_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_kbest2.shape)\n",
    "\n",
    "X_test_features_filtered_kbest2 = selector.transform(x_test_cv)\n",
    "print (\"Test feature space before filtering: \", x_test_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_kbest2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'positive',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'neutral', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'neutral', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training an NB Model for the Feature Space with K-best features k = 2000\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv_k2 = X_train_features_filtered_kbest2\n",
    "y_train_rev_series = full_train_df[\"three rating\"]\n",
    "x_test_cv_k2 = X_test_features_filtered_kbest2\n",
    "y_test_rev_series = full_test_df[\"three rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv_k2, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions_k2 = mnb_model.predict(x_test_cv_k2)\n",
    "predictions_k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reports - NB for the Feature Space with K-best features k = 2000\n",
      "\n",
      "Accuracy Score: 0.7379912663755459\n",
      "\n",
      "Precision Scores\n",
      "macro precision score = 0.5207628839081421\n",
      "micro precision score = 0.7379912663755459\n",
      "weighted precision score = 0.7152427888953939\n",
      "\n",
      "Recall Scores\n",
      "macro recall score = 0.48728002849833213\n",
      "micro recall score = 0.7379912663755459\n",
      "weighted recall score = 0.7379912663755459\n",
      "\n",
      "F1 Scores\n",
      "macro F1 score = 0.49498356240232805\n",
      "micro F1 score = 0.7379912663755459\n",
      "weighted F1 score = 0.7179665704150677\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics for NB for the Feature Space with K-best features k = 2000\n",
    "\n",
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list_k2 = list(predictions_k2)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predict_label_list_k2)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_list, predict_label_list_k2, average='macro')\n",
    "prec_score_micro = precision_score(true_label_list, predict_label_list_k2, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_list, predict_label_list_k2, average='weighted')\n",
    "\n",
    "rec_score_macro = recall_score(true_label_list, predict_label_list_k2, average='macro')\n",
    "rec_score_micro = recall_score(true_label_list, predict_label_list_k2, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_list, predict_label_list_k2, average='weighted')\n",
    "\n",
    "f1_score_macro = f1_score(true_label_list, predict_label_list_k2, average='macro')\n",
    "f1_score_micro = f1_score(true_label_list, predict_label_list_k2, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_list, predict_label_list_k2, average='weighted')\n",
    "\n",
    "\n",
    "print('Evaluation Reports - NB for the Feature Space with K-best features k = 2000')\n",
    "print()\n",
    "print(f'Accuracy Score: {acc_score}')\n",
    "print()\n",
    "print('Precision Scores')\n",
    "print(f'macro precision score = {prec_score_macro}')\n",
    "print(f'micro precision score = {prec_score_micro}')\n",
    "print(f'weighted precision score = {prec_score_weighted}')\n",
    "print()\n",
    "print('Recall Scores')\n",
    "print(f'macro recall score = {rec_score_macro}')\n",
    "print(f'micro recall score = {rec_score_micro}')\n",
    "print(f'weighted recall score = {rec_score_weighted}')\n",
    "print()\n",
    "print('F1 Scores')\n",
    "print(f'macro F1 score = {f1_score_macro}')\n",
    "print(f'micro F1 score = {f1_score_micro}')\n",
    "print(f'weighted F1 score = {f1_score_weighted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lexicon-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborts',\n",
       " 'abrade',\n",
       " 'abrasive',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abscond',\n",
       " 'absence',\n",
       " 'absent-minded',\n",
       " 'absentee',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'accidental',\n",
       " 'accost',\n",
       " 'accursed',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accusingly',\n",
       " 'acerbate',\n",
       " 'acerbic',\n",
       " 'acerbically',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'aches',\n",
       " 'achey',\n",
       " 'aching',\n",
       " 'acrid',\n",
       " 'acridly',\n",
       " 'acridness',\n",
       " 'acrimonious',\n",
       " 'acrimoniously',\n",
       " 'acrimony',\n",
       " 'adamant',\n",
       " 'adamantly',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addicts',\n",
       " 'admonish',\n",
       " 'admonisher',\n",
       " 'admonishingly',\n",
       " 'admonishment',\n",
       " 'admonition',\n",
       " 'adulterate',\n",
       " 'adulterated',\n",
       " 'adulteration',\n",
       " 'adulterier',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'afflict',\n",
       " 'affliction',\n",
       " 'afflictive',\n",
       " 'affront',\n",
       " 'afraid',\n",
       " 'aggravate',\n",
       " 'aggravating',\n",
       " 'aggravation',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressiveness',\n",
       " 'aggressor',\n",
       " 'aggrieve',\n",
       " 'aggrieved',\n",
       " 'aggrivation',\n",
       " 'aghast',\n",
       " 'agonies',\n",
       " 'agonize',\n",
       " 'agonizing',\n",
       " 'agonizingly',\n",
       " 'agony',\n",
       " 'aground',\n",
       " 'ail',\n",
       " 'ailing',\n",
       " 'ailment',\n",
       " 'aimless',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarmingly',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienation',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'allege',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'aloof',\n",
       " 'altercation',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'ambush',\n",
       " 'amiss',\n",
       " 'amputate',\n",
       " 'anarchism',\n",
       " 'anarchist',\n",
       " 'anarchistic',\n",
       " 'anarchy',\n",
       " 'anemic',\n",
       " 'anger',\n",
       " 'angrily',\n",
       " 'angriness',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'animosity',\n",
       " 'annihilate',\n",
       " 'annihilation',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyances',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'anomalous',\n",
       " 'anomaly',\n",
       " 'antagonism',\n",
       " 'antagonist',\n",
       " 'antagonistic',\n",
       " 'antagonize',\n",
       " 'anti-',\n",
       " 'anti-american',\n",
       " 'anti-israeli',\n",
       " 'anti-occupation',\n",
       " 'anti-proliferation',\n",
       " 'anti-semites',\n",
       " 'anti-social',\n",
       " 'anti-us',\n",
       " 'anti-white',\n",
       " 'antipathy',\n",
       " 'antiquated',\n",
       " 'antithetical',\n",
       " 'anxieties',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'anxiousness',\n",
       " 'apathetic',\n",
       " 'apathetically',\n",
       " 'apathy',\n",
       " 'apocalypse',\n",
       " 'apocalyptic',\n",
       " 'apologist',\n",
       " 'apologists',\n",
       " 'appal',\n",
       " 'appall',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'appallingly',\n",
       " 'apprehension',\n",
       " 'apprehensions',\n",
       " 'apprehensive',\n",
       " 'apprehensively',\n",
       " 'arbitrary',\n",
       " 'arcane',\n",
       " 'archaic',\n",
       " 'arduous',\n",
       " 'arduously',\n",
       " 'argumentative',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'ashamed',\n",
       " 'asinine',\n",
       " 'asininely',\n",
       " 'asinininity',\n",
       " 'askance',\n",
       " 'asperse',\n",
       " 'aspersion',\n",
       " 'aspersions',\n",
       " 'assail',\n",
       " 'assassin',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assult',\n",
       " 'astray',\n",
       " 'asunder',\n",
       " 'atrocious',\n",
       " 'atrocities',\n",
       " 'atrocity',\n",
       " 'atrophy',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'audacious',\n",
       " 'audaciously',\n",
       " 'audaciousness',\n",
       " 'audacity',\n",
       " 'audiciously',\n",
       " 'austere',\n",
       " 'authoritarian',\n",
       " 'autocrat',\n",
       " 'autocratic',\n",
       " 'avalanche',\n",
       " 'avarice',\n",
       " 'avaricious',\n",
       " 'avariciously',\n",
       " 'avenge',\n",
       " 'averse',\n",
       " 'aversion',\n",
       " 'aweful',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awfulness',\n",
       " 'awkward',\n",
       " 'awkwardness',\n",
       " 'ax',\n",
       " 'babble',\n",
       " 'back-logged',\n",
       " 'back-wood',\n",
       " 'back-woods',\n",
       " 'backache',\n",
       " 'backaches',\n",
       " 'backaching',\n",
       " 'backbite',\n",
       " 'backbiting',\n",
       " 'backward',\n",
       " 'backwardness',\n",
       " 'backwood',\n",
       " 'backwoods',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'baffle',\n",
       " 'baffled',\n",
       " 'bafflement',\n",
       " 'baffling',\n",
       " 'bait',\n",
       " 'balk',\n",
       " 'banal',\n",
       " 'banalize',\n",
       " 'bane',\n",
       " 'banish',\n",
       " 'banishment',\n",
       " 'bankrupt',\n",
       " 'barbarian',\n",
       " 'barbaric',\n",
       " 'barbarically',\n",
       " 'barbarity',\n",
       " 'barbarous',\n",
       " 'barbarously',\n",
       " 'barren',\n",
       " 'baseless',\n",
       " 'bash',\n",
       " 'bashed',\n",
       " 'bashful',\n",
       " 'bashing',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " 'battered',\n",
       " 'battering',\n",
       " 'batty',\n",
       " 'bearish',\n",
       " 'beastly',\n",
       " 'bedlam',\n",
       " 'bedlamite',\n",
       " 'befoul',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'beggarly',\n",
       " 'begging',\n",
       " 'beguile',\n",
       " 'belabor',\n",
       " 'belated',\n",
       " 'beleaguer',\n",
       " 'belie',\n",
       " 'belittle',\n",
       " 'belittled',\n",
       " 'belittling',\n",
       " 'bellicose',\n",
       " 'belligerence',\n",
       " 'belligerent',\n",
       " 'belligerently',\n",
       " 'bemoan',\n",
       " 'bemoaning',\n",
       " 'bemused',\n",
       " 'bent',\n",
       " 'berate',\n",
       " 'bereave',\n",
       " 'bereavement',\n",
       " 'bereft',\n",
       " 'berserk',\n",
       " 'beseech',\n",
       " 'beset',\n",
       " 'besiege',\n",
       " 'besmirch',\n",
       " 'bestial',\n",
       " 'betray',\n",
       " 'betrayal',\n",
       " 'betrayals',\n",
       " 'betrayer',\n",
       " 'betraying',\n",
       " 'betrays',\n",
       " 'bewail',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewildered',\n",
       " 'bewildering',\n",
       " 'bewilderingly',\n",
       " 'bewilderment',\n",
       " 'bewitch',\n",
       " 'bias',\n",
       " 'biased',\n",
       " 'biases',\n",
       " 'bicker',\n",
       " 'bickering',\n",
       " 'bid-rigging',\n",
       " 'bigotries',\n",
       " 'bigotry',\n",
       " 'bitch',\n",
       " 'bitchy',\n",
       " 'biting',\n",
       " 'bitingly',\n",
       " 'bitter',\n",
       " 'bitterly',\n",
       " 'bitterness',\n",
       " 'bizarre',\n",
       " 'bla',\n",
       " 'blabber',\n",
       " 'blackmail',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blameworthy',\n",
       " 'bland',\n",
       " 'blandish',\n",
       " 'blaspheme',\n",
       " 'blasphemous',\n",
       " 'blasphemy',\n",
       " 'blasted',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'blather',\n",
       " 'bleak',\n",
       " 'bleakly',\n",
       " 'bleakness',\n",
       " 'bleed',\n",
       " 'bleeding',\n",
       " 'bleeds',\n",
       " 'blemish',\n",
       " 'blind',\n",
       " 'blinding',\n",
       " 'blindingly',\n",
       " 'blindside',\n",
       " 'blister',\n",
       " 'blistering',\n",
       " 'bloated',\n",
       " 'blockage',\n",
       " 'blockhead',\n",
       " 'bloodshed',\n",
       " 'bloodthirsty',\n",
       " 'bloody',\n",
       " 'blotchy',\n",
       " 'blow',\n",
       " 'blunder',\n",
       " 'blundering',\n",
       " 'blunders',\n",
       " 'blunt',\n",
       " 'blur',\n",
       " 'bluring',\n",
       " 'blurred',\n",
       " 'blurring',\n",
       " 'blurry',\n",
       " 'blurs',\n",
       " 'blurt',\n",
       " 'boastful',\n",
       " 'boggle',\n",
       " 'bogus',\n",
       " 'boil',\n",
       " 'boiling',\n",
       " 'boisterous',\n",
       " 'bom',\n",
       " 'bombard',\n",
       " 'bombardment',\n",
       " 'bombastic',\n",
       " 'bondage',\n",
       " 'bonkers',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'bores',\n",
       " 'boring',\n",
       " 'botch',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bothering',\n",
       " 'bothers',\n",
       " 'bothersome',\n",
       " 'bowdlerize',\n",
       " 'boycott',\n",
       " 'braggart',\n",
       " 'bragger',\n",
       " 'brainless',\n",
       " 'brainwash',\n",
       " 'brash',\n",
       " 'brashly',\n",
       " 'brashness',\n",
       " 'brat',\n",
       " 'bravado',\n",
       " 'brazen',\n",
       " 'brazenly',\n",
       " 'brazenness',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'break-up',\n",
       " 'break-ups',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakup',\n",
       " 'breakups',\n",
       " 'bribery',\n",
       " 'brimstone',\n",
       " 'bristle',\n",
       " 'brittle',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'broken-hearted',\n",
       " 'brood',\n",
       " 'browbeat',\n",
       " 'bruise',\n",
       " 'bruised',\n",
       " 'bruises',\n",
       " 'bruising',\n",
       " 'brusque',\n",
       " 'brutal',\n",
       " 'brutalising',\n",
       " 'brutalities',\n",
       " 'brutality',\n",
       " 'brutalize',\n",
       " 'brutalizing',\n",
       " 'brutally',\n",
       " 'brute',\n",
       " 'brutish',\n",
       " 'bs',\n",
       " 'buckle',\n",
       " 'bug',\n",
       " 'bugging',\n",
       " 'buggy',\n",
       " 'bugs',\n",
       " 'bulkier',\n",
       " 'bulkiness',\n",
       " 'bulky',\n",
       " 'bulkyness',\n",
       " 'bull****',\n",
       " 'bull----',\n",
       " 'bullies',\n",
       " 'bullshit',\n",
       " 'bullshyt',\n",
       " 'bully',\n",
       " 'bullying',\n",
       " 'bullyingly',\n",
       " 'bum',\n",
       " 'bump',\n",
       " 'bumped',\n",
       " 'bumping',\n",
       " 'bumpping',\n",
       " 'bumps',\n",
       " 'bumpy',\n",
       " 'bungle',\n",
       " 'bungler',\n",
       " 'bungling',\n",
       " 'bunk',\n",
       " 'burden',\n",
       " 'burdensome',\n",
       " 'burdensomely',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'bust',\n",
       " 'busts',\n",
       " 'busybody',\n",
       " 'butcher',\n",
       " 'butchery',\n",
       " 'buzzing',\n",
       " 'byzantine',\n",
       " 'cackle',\n",
       " 'calamities',\n",
       " 'calamitous',\n",
       " 'calamitously',\n",
       " 'calamity',\n",
       " 'callous',\n",
       " 'calumniate',\n",
       " 'calumniation',\n",
       " 'calumnies',\n",
       " 'calumnious',\n",
       " 'calumniously',\n",
       " 'calumny',\n",
       " 'cancer',\n",
       " 'cancerous',\n",
       " 'cannibal',\n",
       " 'cannibalize',\n",
       " 'capitulate',\n",
       " 'capricious',\n",
       " 'capriciously',\n",
       " 'capriciousness',\n",
       " 'capsize',\n",
       " 'careless',\n",
       " 'carelessness',\n",
       " 'caricature',\n",
       " 'carnage',\n",
       " 'carp',\n",
       " 'cartoonish',\n",
       " 'cash-strapped',\n",
       " 'castigate',\n",
       " 'castrated',\n",
       " 'casualty',\n",
       " 'cataclysm',\n",
       " 'cataclysmal',\n",
       " 'cataclysmic',\n",
       " 'cataclysmically',\n",
       " 'catastrophe',\n",
       " 'catastrophes',\n",
       " 'catastrophic',\n",
       " 'catastrophically',\n",
       " 'catastrophies',\n",
       " 'caustic',\n",
       " 'caustically',\n",
       " 'cautionary',\n",
       " 'cave',\n",
       " 'censure',\n",
       " 'chafe',\n",
       " 'chaff',\n",
       " 'chagrin',\n",
       " 'challenging',\n",
       " 'chaos',\n",
       " 'chaotic',\n",
       " 'chasten',\n",
       " 'chastise',\n",
       " 'chastisement',\n",
       " 'chatter',\n",
       " 'chatterbox',\n",
       " 'cheap',\n",
       " 'cheapen',\n",
       " 'cheaply',\n",
       " 'cheat',\n",
       " 'cheated',\n",
       " 'cheater',\n",
       " 'cheating',\n",
       " 'cheats',\n",
       " 'checkered',\n",
       " 'cheerless',\n",
       " 'cheesy',\n",
       " 'chide',\n",
       " 'childish',\n",
       " 'chill',\n",
       " 'chilly',\n",
       " 'chintzy',\n",
       " 'choke',\n",
       " 'choleric',\n",
       " 'choppy',\n",
       " 'chore',\n",
       " 'chronic',\n",
       " 'chunky',\n",
       " 'clamor',\n",
       " 'clamorous',\n",
       " 'clash',\n",
       " 'cliche',\n",
       " 'cliched',\n",
       " 'clique',\n",
       " 'clog',\n",
       " 'clogged',\n",
       " 'clogs',\n",
       " 'cloud',\n",
       " 'clouding',\n",
       " 'cloudy',\n",
       " 'clueless',\n",
       " 'clumsy',\n",
       " 'clunky',\n",
       " 'coarse',\n",
       " 'cocky',\n",
       " 'coerce',\n",
       " 'coercion',\n",
       " 'coercive',\n",
       " 'cold',\n",
       " 'coldly',\n",
       " 'collapse',\n",
       " 'collude',\n",
       " 'collusion',\n",
       " 'combative',\n",
       " 'combust',\n",
       " 'comical',\n",
       " 'commiserate',\n",
       " 'commonplace',\n",
       " 'commotion',\n",
       " 'commotions',\n",
       " 'complacent',\n",
       " 'complain',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'complains',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'complication',\n",
       " 'complicit',\n",
       " 'compulsion',\n",
       " 'compulsive',\n",
       " 'concede',\n",
       " 'conceded',\n",
       " 'conceit',\n",
       " 'conceited',\n",
       " 'concen',\n",
       " 'concens',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerns',\n",
       " 'concession',\n",
       " 'concessions',\n",
       " 'condemn',\n",
       " 'condemnable',\n",
       " 'condemnation',\n",
       " 'condemned',\n",
       " 'condemns',\n",
       " 'condescend',\n",
       " 'condescending',\n",
       " 'condescendingly',\n",
       " 'condescension',\n",
       " 'confess',\n",
       " 'confession',\n",
       " 'confessions',\n",
       " 'confined',\n",
       " 'conflict',\n",
       " 'conflicted',\n",
       " 'conflicting',\n",
       " 'conflicts',\n",
       " 'confound',\n",
       " 'confounded',\n",
       " 'confounding',\n",
       " 'confront',\n",
       " 'confrontation',\n",
       " 'confrontational',\n",
       " 'confuse',\n",
       " 'confused',\n",
       " 'confuses',\n",
       " 'confusing',\n",
       " 'confusion',\n",
       " 'confusions',\n",
       " 'congested',\n",
       " 'congestion',\n",
       " 'cons',\n",
       " 'conscons',\n",
       " 'conservative',\n",
       " 'conspicuous',\n",
       " 'conspicuously',\n",
       " 'conspiracies',\n",
       " 'conspiracy',\n",
       " 'conspirator',\n",
       " 'conspiratorial',\n",
       " 'conspire',\n",
       " 'consternation',\n",
       " 'contagious',\n",
       " 'contaminate',\n",
       " 'contaminated',\n",
       " 'contaminates',\n",
       " 'contaminating',\n",
       " 'contamination',\n",
       " 'contempt',\n",
       " 'contemptible',\n",
       " 'contemptuous',\n",
       " 'contemptuously',\n",
       " 'contend',\n",
       " 'contention',\n",
       " 'contentious',\n",
       " 'contort',\n",
       " 'contortions',\n",
       " 'contradict',\n",
       " 'contradiction',\n",
       " 'contradictory',\n",
       " 'contrariness',\n",
       " 'contravene',\n",
       " 'contrive',\n",
       " 'contrived',\n",
       " 'controversial',\n",
       " 'controversy',\n",
       " 'convoluted',\n",
       " 'corrode',\n",
       " 'corrosion',\n",
       " 'corrosions',\n",
       " 'corrosive',\n",
       " 'corrupt',\n",
       " 'corrupted',\n",
       " 'corrupting',\n",
       " 'corruption',\n",
       " 'corrupts',\n",
       " 'corruptted',\n",
       " 'costlier',\n",
       " 'costly',\n",
       " 'counter-productive',\n",
       " 'counterproductive',\n",
       " 'coupists',\n",
       " 'covetous',\n",
       " 'coward',\n",
       " 'cowardly',\n",
       " 'crabby',\n",
       " 'crack',\n",
       " 'cracked',\n",
       " 'cracks',\n",
       " 'craftily',\n",
       " 'craftly',\n",
       " 'crafty',\n",
       " 'cramp',\n",
       " 'cramped',\n",
       " 'cramping',\n",
       " 'cranky',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'craps',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'crashes',\n",
       " 'crashing',\n",
       " 'crass',\n",
       " 'craven',\n",
       " 'cravenly',\n",
       " 'craze',\n",
       " 'crazily',\n",
       " 'craziness',\n",
       " 'crazy',\n",
       " 'creak',\n",
       " 'creaking',\n",
       " 'creaks',\n",
       " 'credulous',\n",
       " 'creep',\n",
       " 'creeping',\n",
       " 'creeps',\n",
       " 'creepy',\n",
       " 'crept',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'cringe',\n",
       " 'cringed',\n",
       " 'cringes',\n",
       " 'cripple',\n",
       " 'crippled',\n",
       " 'cripples',\n",
       " 'crippling',\n",
       " 'crisis',\n",
       " 'critic',\n",
       " 'critical',\n",
       " 'criticism',\n",
       " 'criticisms',\n",
       " 'criticize',\n",
       " 'criticized',\n",
       " 'criticizing',\n",
       " 'critics',\n",
       " 'cronyism',\n",
       " 'crook',\n",
       " 'crooked',\n",
       " 'crooks',\n",
       " 'crowded',\n",
       " 'crowdedness',\n",
       " 'crude',\n",
       " 'cruel',\n",
       " 'crueler',\n",
       " 'cruelest',\n",
       " 'cruelly',\n",
       " 'cruelness',\n",
       " 'cruelties',\n",
       " 'cruelty',\n",
       " 'crumble',\n",
       " 'crumbling',\n",
       " 'crummy',\n",
       " 'crumple',\n",
       " 'crumpled',\n",
       " 'crumples',\n",
       " 'crush',\n",
       " 'crushed',\n",
       " 'crushing',\n",
       " 'cry',\n",
       " 'culpable',\n",
       " 'culprit',\n",
       " 'cumbersome',\n",
       " 'cunt',\n",
       " 'cunts',\n",
       " 'cuplrit',\n",
       " 'curse',\n",
       " 'cursed',\n",
       " 'curses',\n",
       " 'curt',\n",
       " 'cuss',\n",
       " 'cussed',\n",
       " 'cutthroat',\n",
       " 'cynical',\n",
       " 'cynicism',\n",
       " 'd*mn',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'damages',\n",
       " 'damaging',\n",
       " 'damn',\n",
       " 'damnable',\n",
       " 'damnably',\n",
       " 'damnation',\n",
       " 'damned',\n",
       " 'damning',\n",
       " 'damper',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'dangerousness',\n",
       " 'dark',\n",
       " 'darken',\n",
       " 'darkened',\n",
       " 'darker',\n",
       " 'darkness',\n",
       " 'dastard',\n",
       " 'dastardly',\n",
       " 'daunt',\n",
       " 'daunting',\n",
       " 'dauntingly',\n",
       " 'dawdle',\n",
       " 'daze',\n",
       " 'dazed',\n",
       " 'dead',\n",
       " 'deadbeat',\n",
       " 'deadlock',\n",
       " 'deadly',\n",
       " 'deadweight',\n",
       " 'deaf',\n",
       " 'dearth',\n",
       " 'death',\n",
       " 'debacle',\n",
       " 'debase',\n",
       " 'debasement',\n",
       " 'debaser',\n",
       " 'debatable',\n",
       " 'debauch',\n",
       " 'debaucher',\n",
       " 'debauchery',\n",
       " 'debilitate',\n",
       " 'debilitating',\n",
       " 'debility',\n",
       " 'debt',\n",
       " 'debts',\n",
       " 'decadence',\n",
       " 'decadent',\n",
       " 'decay',\n",
       " 'decayed',\n",
       " 'deceit',\n",
       " 'deceitful',\n",
       " 'deceitfully',\n",
       " 'deceitfulness',\n",
       " 'deceive',\n",
       " 'deceiver',\n",
       " 'deceivers',\n",
       " 'deceiving',\n",
       " 'deception',\n",
       " 'deceptive',\n",
       " 'deceptively',\n",
       " 'declaim',\n",
       " 'decline',\n",
       " 'declines',\n",
       " 'declining',\n",
       " 'decrement',\n",
       " 'decrepit',\n",
       " 'decrepitude',\n",
       " 'decry',\n",
       " 'defamation',\n",
       " 'defamations',\n",
       " 'defamatory',\n",
       " 'defame',\n",
       " 'defect',\n",
       " 'defective',\n",
       " 'defects',\n",
       " 'defensive',\n",
       " 'defiance',\n",
       " 'defiant',\n",
       " 'defiantly',\n",
       " 'deficiencies',\n",
       " 'deficiency',\n",
       " 'deficient',\n",
       " 'defile',\n",
       " 'defiler',\n",
       " 'deform',\n",
       " 'deformed',\n",
       " 'defrauding',\n",
       " 'defunct',\n",
       " 'defy',\n",
       " 'degenerate',\n",
       " 'degenerately',\n",
       " 'degeneration',\n",
       " 'degradation',\n",
       " 'degrade',\n",
       " 'degrading',\n",
       " 'degradingly',\n",
       " 'dehumanization',\n",
       " 'dehumanize',\n",
       " 'deign',\n",
       " 'deject',\n",
       " 'dejected',\n",
       " 'dejectedly',\n",
       " 'dejection',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'delaying',\n",
       " 'delays',\n",
       " 'delinquency',\n",
       " 'delinquent',\n",
       " 'delirious',\n",
       " 'delirium',\n",
       " 'delude',\n",
       " 'deluded',\n",
       " 'deluge',\n",
       " 'delusion',\n",
       " 'delusional',\n",
       " 'delusions',\n",
       " 'demean',\n",
       " 'demeaning',\n",
       " 'demise',\n",
       " 'demolish',\n",
       " 'demolisher',\n",
       " 'demon',\n",
       " 'demonic',\n",
       " 'demonize',\n",
       " 'demonized',\n",
       " 'demonizes',\n",
       " 'demonizing',\n",
       " 'demoralize',\n",
       " 'demoralizing',\n",
       " 'demoralizingly',\n",
       " 'denial',\n",
       " 'denied',\n",
       " 'denies',\n",
       " 'denigrate',\n",
       " 'denounce',\n",
       " 'dense',\n",
       " 'dent',\n",
       " 'dented',\n",
       " 'dents',\n",
       " 'denunciate',\n",
       " 'denunciation',\n",
       " 'denunciations',\n",
       " 'deny',\n",
       " 'denying',\n",
       " 'deplete',\n",
       " 'deplorable',\n",
       " 'deplorably',\n",
       " 'deplore',\n",
       " 'deploring',\n",
       " 'deploringly',\n",
       " 'deprave',\n",
       " 'depraved',\n",
       " 'depravedly',\n",
       " 'deprecate',\n",
       " 'depress',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'depressingly',\n",
       " 'depression',\n",
       " 'depressions',\n",
       " 'deprive',\n",
       " 'deprived',\n",
       " 'deride',\n",
       " 'derision',\n",
       " 'derisive',\n",
       " 'derisively',\n",
       " 'derisiveness',\n",
       " 'derogatory',\n",
       " 'desecrate',\n",
       " 'desert',\n",
       " 'desertion',\n",
       " 'desiccate',\n",
       " 'desiccated',\n",
       " 'desititute',\n",
       " 'desolate',\n",
       " 'desolately',\n",
       " 'desolation',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despairingly',\n",
       " 'desperate',\n",
       " 'desperately',\n",
       " 'desperation',\n",
       " 'despicable',\n",
       " 'despicably',\n",
       " ...]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Lexicon Word Lists\n",
    "\n",
    "positive_words = open('positive_words_list.txt', 'r')\n",
    "posi_word_str = positive_words.read()\n",
    "posi_word_list = posi_word_str.replace('\\n', ' ').split(' ')\n",
    "\n",
    "negative_words = open('negative_words_list.txt', 'r')\n",
    "neg_word_str = negative_words.read()\n",
    "neg_word_list = neg_word_str.replace('\\n', ' ').split(' ')\n",
    "\n",
    "lex_word_list =  neg_word_list + posi_word_list\n",
    "lex_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/38d5fhqx2_jf385d_b3cq3j00000gn/T/ipykernel_55994/1885815054.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_train_df['lex review'] = full_train_df.review.apply(lambda p: ' '.join(w for w in p.split() if w in lex_word_list))\n"
     ]
    }
   ],
   "source": [
    "# Filtering the DataFrame to only include lexicon words\n",
    "\n",
    "# https://stackoverflow.com/questions/61926076/how-to-remove-words-from-a-data-frame-that-are-not-in-list-in-python\n",
    "\n",
    "full_train_df['lex review'] = full_train_df.review.apply(lambda p: ' '.join(w for w in p.split() if w in lex_word_list))\n",
    "full_train_df[['lex review', 'review', 'rating']][0:50]\n",
    "\n",
    "full_test_df['lex review'] = full_test_df.review.apply(lambda p: ' '.join(w for w in p.split() if w in lex_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829, 915) \n",
      "\n",
      "(458, 915) \n",
      "\n",
      "  (0, 609)\t2\n",
      "  (0, 183)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 341)\t1\n",
      "  (0, 892)\t1\n",
      "  (0, 105)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 736)\t3\n",
      "  (0, 782)\t1\n",
      "  (0, 347)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 493)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 219)\t1\n",
      "  (0, 287)\t1\n",
      "  (1, 736)\t1\n",
      "  (1, 782)\t1\n",
      "  (1, 493)\t1\n",
      "  (1, 219)\t1\n",
      "  (1, 266)\t2\n",
      "  (1, 7)\t1\n",
      "  (1, 605)\t1\n",
      "  (1, 559)\t1\n",
      "  (1, 508)\t1\n",
      "  (1, 356)\t1\n",
      "  :\t:\n",
      "  (1813, 54)\t1\n",
      "  (1817, 786)\t1\n",
      "  (1818, 273)\t1\n",
      "  (1818, 799)\t2\n",
      "  (1818, 278)\t1\n",
      "  (1818, 83)\t1\n",
      "  (1818, 47)\t1\n",
      "  (1818, 491)\t1\n",
      "  (1818, 898)\t2\n",
      "  (1818, 681)\t1\n",
      "  (1818, 798)\t1\n",
      "  (1818, 519)\t2\n",
      "  (1818, 533)\t1\n",
      "  (1818, 541)\t1\n",
      "  (1818, 66)\t1\n",
      "  (1818, 589)\t1\n",
      "  (1818, 650)\t1\n",
      "  (1818, 808)\t1\n",
      "  (1818, 231)\t1\n",
      "  (1818, 376)\t1\n",
      "  (1818, 904)\t1\n",
      "  (1818, 67)\t1\n",
      "  (1826, 39)\t1\n",
      "  (1827, 610)\t1\n",
      "  (1828, 341)\t1\n"
     ]
    }
   ],
   "source": [
    "# Creating a New Model based on Lexicon Word list\n",
    "# Building the Representation \n",
    "\n",
    "train_lex_text = full_train_df[\"lex review\"]\n",
    "test_lex_text = full_test_df[\"lex review\"]\n",
    "\n",
    "# Setting n-gram range to uni-grams\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "# Creating training data representation\n",
    "train_lex_cv = vectorizer.fit_transform(train_lex_text.values.astype('U'))\n",
    "print(train_lex_cv.shape,\"\\n\") \n",
    "\n",
    "# Creating test data representation\n",
    "test_lex_cv = vectorizer.transform(test_lex_text.values.astype('U'))\n",
    "print(test_lex_cv.shape,\"\\n\") \n",
    "\n",
    "print(train_lex_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'neutral', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive'], dtype='<U8')"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a Model\n",
    "\n",
    "# Define true labels from train set\n",
    "\n",
    "x_train_cv_lex = train_lex_cv\n",
    "y_train_rev_series = full_train_df[\"three rating\"]\n",
    "x_test_cv_lex = test_lex_cv\n",
    "y_test_rev_series = full_test_df[\"three rating\"]\n",
    "\n",
    "# Build model on the training data\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv_lex, y_train_rev_series)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "\n",
    "predictions_lex = mnb_model.predict(x_test_cv_lex)\n",
    "predictions_lex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Reports - NB for the Feature Space with Lexicon based features\n",
      "\n",
      "Accuracy Score: 0.6593886462882096\n",
      "\n",
      "Precision Scores\n",
      "macro precision score = 0.43833048833550886\n",
      "micro precision score = 0.6593886462882096\n",
      "weighted precision score = 0.6233608011186476\n",
      "\n",
      "Recall Scores\n",
      "macro recall score = 0.3630865636840572\n",
      "micro recall score = 0.6593886462882096\n",
      "weighted recall score = 0.6593886462882096\n",
      "\n",
      "F1 Scores\n",
      "macro F1 score = 0.324883655733109\n",
      "micro F1 score = 0.6593886462882096\n",
      "weighted F1 score = 0.5610832559010729\n"
     ]
    }
   ],
   "source": [
    "true_label_list = y_test_rev_series.to_list()\n",
    "predict_label_list_lex = list(predictions_lex)\n",
    "\n",
    "acc_score = accuracy_score(true_label_list, predictions_lex)\n",
    "\n",
    "prec_score_macro = precision_score(true_label_list, predictions_lex, average='macro')\n",
    "prec_score_micro = precision_score(true_label_list, predictions_lex, average='micro')\n",
    "prec_score_weighted = precision_score(true_label_list, predictions_lex, average='weighted')\n",
    "\n",
    "rec_score_macro = recall_score(true_label_list, predictions_lex, average='macro')\n",
    "rec_score_micro = recall_score(true_label_list, predictions_lex, average='micro')\n",
    "rec_score_weighted = recall_score(true_label_list, predictions_lex, average='weighted')\n",
    "\n",
    "f1_score_macro = f1_score(true_label_list, predictions_lex, average='macro')\n",
    "f1_score_micro = f1_score(true_label_list, predictions_lex, average='micro')\n",
    "f1_score_weighted = f1_score(true_label_list, predictions_lex, average='weighted')\n",
    "\n",
    "\n",
    "print('Evaluation Reports - NB for the Feature Space with Lexicon based features')\n",
    "print()\n",
    "print(f'Accuracy Score: {acc_score}')\n",
    "print()\n",
    "print('Precision Scores')\n",
    "print(f'macro precision score = {prec_score_macro}')\n",
    "print(f'micro precision score = {prec_score_micro}')\n",
    "print(f'weighted precision score = {prec_score_weighted}')\n",
    "print()\n",
    "print('Recall Scores')\n",
    "print(f'macro recall score = {rec_score_macro}')\n",
    "print(f'micro recall score = {rec_score_micro}')\n",
    "print(f'weighted recall score = {rec_score_weighted}')\n",
    "print()\n",
    "print('F1 Scores')\n",
    "print(f'macro F1 score = {f1_score_macro}')\n",
    "print(f'micro F1 score = {f1_score_micro}')\n",
    "print(f'weighted F1 score = {f1_score_weighted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Extract and Select Best Unigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space before filtering:  (1829, 5551)\n",
      "Train feature space after filtering:  (1829, 2000)\n",
      "Test feature space before filtering:  (458, 5551)\n",
      "Test feature space after filtering:  (458, 2000)\n",
      "(1829,)\n",
      "(458,)\n"
     ]
    }
   ],
   "source": [
    "# Selecting K-best features with k = 2000\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=2000)\n",
    "X_train_features_filtered_kbest2 = selector.fit_transform(x_train_cv, y_train_rev_series)\n",
    "print (\"Train feature space before filtering: \", x_train_cv.shape)\n",
    "print (\"Train feature space after filtering: \", X_train_features_filtered_kbest2.shape)\n",
    "\n",
    "X_test_features_filtered_kbest2 = selector.transform(x_test_cv)\n",
    "print (\"Test feature space before filtering: \", x_test_cv.shape)\n",
    "print (\"Test feature space after filtering: \", X_test_features_filtered_kbest2.shape)\n",
    "print(y_train_rev_series.shape)\n",
    "print(y_test_rev_series.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train and Evaluate a Naive Bayes classifier using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829,)\n"
     ]
    }
   ],
   "source": [
    "y_train_array = y_train_rev_series.values\n",
    "print(y_train_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 for NB with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7814207650273224\n",
      "Weighted Precision Score: 0.7546294236715083\n",
      "Weighted Recall Score: 0.7814207650273224\n",
      "Weighted F1 Score: 0.7523821717880836\n",
      "\n",
      "Fold 2 for NB with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7622950819672131\n",
      "Weighted Precision Score: 0.7578797527358261\n",
      "Weighted Recall Score: 0.7622950819672131\n",
      "Weighted F1 Score: 0.7488948197322874\n",
      "\n",
      "Fold 3 for NB with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7759562841530054\n",
      "Weighted Precision Score: 0.7693163436170125\n",
      "Weighted Recall Score: 0.7759562841530054\n",
      "Weighted F1 Score: 0.7650201919325562\n",
      "\n",
      "Fold 4 for NB with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.8005464480874317\n",
      "Weighted Precision Score: 0.7983973058695532\n",
      "Weighted Recall Score: 0.8005464480874317\n",
      "Weighted F1 Score: 0.7875139029933749\n",
      "\n",
      "Fold 5 for NB with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7890410958904109\n",
      "Weighted Precision Score: 0.7380959832873405\n",
      "Weighted Recall Score: 0.7890410958904109\n",
      "Weighted F1 Score: 0.7555909711508711\n",
      "\n",
      "Weighted F1 Score for NB Classifier: [0.7523821717880836, 0.7488948197322874, 0.7650201919325562, 0.7875139029933749, 0.7555909711508711]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a Multinomial Naive Bayes model\n",
    "mnb_model = MultinomialNB()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "f1_scores_nb = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_train_features_filtered_kbest2, y_train_array), 1):\n",
    "    X_train = X_train_features_filtered_kbest2[train_index]\n",
    "    X_test = X_train_features_filtered_kbest2[test_index]\n",
    "    Y_train = y_train_array[train_index]\n",
    "    Y_test = y_train_array[test_index]\n",
    "\n",
    "    mnb_model.fit(X_train, Y_train)\n",
    "    predictions_kfold = mnb_model.predict(X_test)\n",
    "    \n",
    "    acc_score = accuracy_score(Y_test, predictions_kfold)\n",
    "    prec_score_weighted = precision_score(Y_test, predictions_kfold, average='weighted')\n",
    "    rec_score_weighted = recall_score(Y_test, predictions_kfold, average='weighted')\n",
    "    f1_score_weighted_nb = f1_score(Y_test, predictions_kfold, average='weighted')\n",
    "\n",
    "    f1_scores_nb.append(f1_score_weighted_nb)\n",
    "\n",
    "    print(f'Fold {fold_idx} for NB with kbest = 2000, 5 fold cross-validation:')\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    print(f'Weighted Precision Score: {prec_score_weighted}')\n",
    "    print(f'Weighted Recall Score: {rec_score_weighted}')\n",
    "    print(f'Weighted F1 Score: {f1_score_weighted_nb}')\n",
    "    print()\n",
    "\n",
    "print(f'Weighted F1 Score for NB Classifier: {f1_scores_nb}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Training a Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 SVM with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.680327868852459\n",
      "Weighted Precision Score: 0.6722251593005485\n",
      "Weighted Recall Score: 0.680327868852459\n",
      "Weighted F1 Score: 0.6753358415487529\n",
      "\n",
      "Fold 2 SVM with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7021857923497268\n",
      "Weighted Precision Score: 0.7266452062939771\n",
      "Weighted Recall Score: 0.7021857923497268\n",
      "Weighted F1 Score: 0.7128095551776402\n",
      "\n",
      "Fold 3 SVM with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.6857923497267759\n",
      "Weighted Precision Score: 0.6716295431227868\n",
      "Weighted Recall Score: 0.6857923497267759\n",
      "Weighted F1 Score: 0.6777760249704226\n",
      "\n",
      "Fold 4 SVM with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7540983606557377\n",
      "Weighted Precision Score: 0.7350966747636427\n",
      "Weighted Recall Score: 0.7540983606557377\n",
      "Weighted F1 Score: 0.7397134339363093\n",
      "\n",
      "Fold 5 SVM with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7205479452054795\n",
      "Weighted Precision Score: 0.6867177900150185\n",
      "Weighted Recall Score: 0.7205479452054795\n",
      "Weighted F1 Score: 0.6741125228297807\n",
      "\n",
      "F1 Scores List for SVM Classifier: [0.6753358415487529, 0.7128095551776402, 0.6777760249704226, 0.7397134339363093, 0.6741125228297807]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building a Linear SVM Classifier using the k=2000 kbest features with 5-fold cross validation\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "f1_scores_svm = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_train_features_filtered_kbest2, y_train_array), 1):\n",
    "    X_train = X_train_features_filtered_kbest2[train_index]\n",
    "    X_test = X_train_features_filtered_kbest2[test_index]\n",
    "    Y_train = y_train_array[train_index]\n",
    "    Y_test = y_train_array[test_index]\n",
    "\n",
    "    svm.fit(X_train, Y_train)\n",
    "    predictions_svm = svm.predict(X_test)\n",
    "    \n",
    "    acc_score = accuracy_score(Y_test, predictions_svm)\n",
    "    prec_score_weighted = precision_score(Y_test, predictions_svm, average='weighted')\n",
    "    rec_score_weighted = recall_score(Y_test, predictions_svm, average='weighted')\n",
    "    f1_score_weighted_svm = f1_score(Y_test, predictions_svm, average='weighted')\n",
    "\n",
    "    f1_scores_svm.append(f1_score_weighted_svm)\n",
    "\n",
    "    print(f'Fold {fold_idx} for SVM with kbest = 2000, 5 fold cross-validation:')\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    print(f'Weighted Precision Score: {prec_score_weighted}')\n",
    "    print(f'Weighted Recall Score: {rec_score_weighted}')\n",
    "    print(f'Weighted F1 Score: {f1_score_weighted_svm}')\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f'F1 Scores List for SVM Classifier: {f1_scores_svm}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Training a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 for Logistic Regression Classifier with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.680327868852459\n",
      "Weighted Precision Score: 0.6637314577244461\n",
      "Weighted Recall Score: 0.680327868852459\n",
      "Weighted F1 Score: 0.6711642843855948\n",
      "\n",
      "Fold 2 for Logistic Regression Classifier with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7021857923497268\n",
      "Weighted Precision Score: 0.6880777591160104\n",
      "Weighted Recall Score: 0.7021857923497268\n",
      "Weighted F1 Score: 0.6941893311877188\n",
      "\n",
      "Fold 3 for Logistic Regression Classifier with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.674863387978142\n",
      "Weighted Precision Score: 0.6249423390816834\n",
      "Weighted Recall Score: 0.674863387978142\n",
      "Weighted F1 Score: 0.6458191809006956\n",
      "\n",
      "Fold 4 for Logistic Regression Classifier with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.726775956284153\n",
      "Weighted Precision Score: 0.6960043927831167\n",
      "Weighted Recall Score: 0.726775956284153\n",
      "Weighted F1 Score: 0.6898253575549457\n",
      "\n",
      "Fold 5 for Logistic Regression Classifier with kbest = 2000, 5 fold cross-validation:\n",
      "Accuracy Score: 0.7150684931506849\n",
      "Weighted Precision Score: 0.6779822227832779\n",
      "Weighted Recall Score: 0.7150684931506849\n",
      "Weighted F1 Score: 0.6546125672042057\n",
      "\n",
      " Weighted F1 Score for Logistic Regression: [0.6711642843855948, 0.6941893311877188, 0.6458191809006956, 0.6898253575549457, 0.6546125672042057]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building a Logistic Regression Classifier using the k=2000 kbest features with 5-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "f1_scores_lr = []\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", penalty=\"l1\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_train_features_filtered_kbest2, y_train_array), 1):\n",
    "    X_train = X_train_features_filtered_kbest2[train_index]\n",
    "    X_test = X_train_features_filtered_kbest2[test_index]\n",
    "    Y_train = y_train_array[train_index]\n",
    "    Y_test = y_train_array[test_index]\n",
    "\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    predictions_lr = logreg.predict(X_test)\n",
    "    \n",
    "    acc_score = accuracy_score(Y_test, predictions_lr)\n",
    "    prec_score_weighted = precision_score(Y_test, predictions_lr, average='weighted')\n",
    "    rec_score_weighted = recall_score(Y_test, predictions_lr, average='weighted')\n",
    "    f1_score_weighted_lr = f1_score(Y_test, predictions_lr, average='weighted')\n",
    "\n",
    "    f1_scores_lr.append(f1_score_weighted_lr)\n",
    "\n",
    "\n",
    "    print(f'Fold {fold_idx} for Logistic Regression Classifier with kbest = 2000, 5 fold cross-validation:')\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    print(f'Weighted Precision Score: {prec_score_weighted}')\n",
    "    print(f'Weighted Recall Score: {rec_score_weighted}')\n",
    "    print(f'Weighted F1 Score: {f1_score_weighted_lr}')\n",
    "    print()\n",
    "\n",
    "print(f' Weighted F1 Score for Logistic Regression: {f1_scores_lr}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Model Comparison Using a t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test result SVC and NB:  Ttest_indResult(statistic=-4.4466151416194934, pvalue=0.002148950144364985)\n",
      "t-test result SVC and LR:  Ttest_indResult(statistic=1.5358455037809184, pvalue=0.16312705181395892)\n",
      "t-test result SVC and LR:  Ttest_indResult(statistic=7.725791715151462, pvalue=5.608100793024348e-05)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# LinearSVC vs. Naive Bayes t-test\n",
    "\n",
    "svc_nb_ttest = stats.ttest_ind(f1_scores_svm, f1_scores_nb,)\n",
    "print(\"t-test result SVC and NB: \", svc_nb_ttest) \n",
    "\n",
    "# LinearSVC vs Logistic Regression\n",
    "\n",
    "svc_lr_ttest = stats.ttest_ind(f1_scores_svm, f1_scores_lr,)\n",
    "print(\"t-test result SVC and LR: \", svc_lr_ttest) \n",
    "\n",
    "# Naive Bayes vs Logistic Regression\n",
    "\n",
    "nb_lr_ttest = stats.ttest_ind(f1_scores_nb, f1_scores_lr,)\n",
    "print(\"t-test result NB and LR: \", nb_lr_ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 0.1\n",
      "Best Accuracy: 0.7288325774931039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(f\"Best C value: {best_C}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 for SVM Classifier with C = 0.1:\n",
      "Accuracy Score: 0.6885245901639344\n",
      "Weighted Precision Score: 0.6827053973732145\n",
      "Weighted Recall Score: 0.6885245901639344\n",
      "Weighted F1 Score: 0.6848756576532293\n",
      "\n",
      "Fold 2 for SVM Classifier with C = 0.1:\n",
      "Accuracy Score: 0.726775956284153\n",
      "Weighted Precision Score: 0.7209931899592448\n",
      "Weighted Recall Score: 0.726775956284153\n",
      "Weighted F1 Score: 0.7234081828931319\n",
      "\n",
      "Fold 3 for SVM Classifier with C = 0.1:\n",
      "Accuracy Score: 0.7076502732240437\n",
      "Weighted Precision Score: 0.6543492936935559\n",
      "Weighted Recall Score: 0.7076502732240437\n",
      "Weighted F1 Score: 0.676255096138491\n",
      "\n",
      "Fold 4 for SVM Classifier with C = 0.1:\n",
      "Accuracy Score: 0.7650273224043715\n",
      "Weighted Precision Score: 0.7588775335882374\n",
      "Weighted Recall Score: 0.7650273224043715\n",
      "Weighted F1 Score: 0.735960533371943\n",
      "\n",
      "Fold 5 for SVM Classifier with C = 0.1:\n",
      "Accuracy Score: 0.6958904109589041\n",
      "Weighted Precision Score: 0.63343870741131\n",
      "Weighted Recall Score: 0.6958904109589041\n",
      "Weighted F1 Score: 0.6229003186637857\n",
      "\n",
      "F1 Scores List for SVM Classifier: [0.6848756576532293, 0.7234081828931319, 0.676255096138491, 0.735960533371943, 0.6229003186637857]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best C Value = 0.1 with Accuracy = Best Accuracy: 0.7288325774931039\n",
    "# Running the Linear SVC Model with that C Value\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(C=0.1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "f1_scores_svm = []\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_train_features_filtered_kbest2, y_train_array), 1):\n",
    "    X_train = X_train_features_filtered_kbest2[train_index]\n",
    "    X_test = X_train_features_filtered_kbest2[test_index]\n",
    "    Y_train = y_train_array[train_index]\n",
    "    Y_test = y_train_array[test_index]\n",
    "\n",
    "    svm.fit(X_train, Y_train)\n",
    "    predictions_svm = svm.predict(X_test)\n",
    "    \n",
    "    acc_score = accuracy_score(Y_test, predictions_svm)\n",
    "    prec_score_weighted = precision_score(Y_test, predictions_svm, average='weighted')\n",
    "    rec_score_weighted = recall_score(Y_test, predictions_svm, average='weighted')\n",
    "    f1_score_weighted_svm = f1_score(Y_test, predictions_svm, average='weighted')\n",
    "\n",
    "    f1_scores_svm.append(f1_score_weighted_svm)\n",
    "\n",
    "    print(f'Fold {fold_idx} for SVM Classifier with C = 0.1:')\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    print(f'Weighted Precision Score: {prec_score_weighted}')\n",
    "    print(f'Weighted Recall Score: {rec_score_weighted}')\n",
    "    print(f'Weighted F1 Score: {f1_score_weighted_svm}')\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f'F1 Scores List for SVM Classifier: {f1_scores_svm}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 for Logistic Regression Classifier with C=0.1:\n",
      "Accuracy Score: 0.6939890710382514\n",
      "Weighted Precision Score: 0.6190646718658814\n",
      "Weighted Recall Score: 0.6939890710382514\n",
      "Weighted F1 Score: 0.6345482985131958\n",
      "\n",
      "Fold 2 for Logistic Regression Classifier with C=0.1:\n",
      "Accuracy Score: 0.6939890710382514\n",
      "Weighted Precision Score: 0.6132043861045261\n",
      "Weighted Recall Score: 0.6939890710382514\n",
      "Weighted F1 Score: 0.6325667726274002\n",
      "\n",
      "Fold 3 for Logistic Regression Classifier with C=0.1:\n",
      "Accuracy Score: 0.6775956284153005\n",
      "Weighted Precision Score: 0.618549518762485\n",
      "Weighted Recall Score: 0.6775956284153005\n",
      "Weighted F1 Score: 0.6007652327540162\n",
      "\n",
      "Fold 4 for Logistic Regression Classifier with C=0.1:\n",
      "Accuracy Score: 0.6557377049180327\n",
      "Weighted Precision Score: 0.5882123341139734\n",
      "Weighted Recall Score: 0.6557377049180327\n",
      "Weighted F1 Score: 0.5698634285358923\n",
      "\n",
      "Fold 5 for Logistic Regression Classifier with C=0.1:\n",
      "Accuracy Score: 0.6657534246575343\n",
      "Weighted Precision Score: 0.6004755821218423\n",
      "Weighted Recall Score: 0.6657534246575343\n",
      "Weighted F1 Score: 0.5737095722071418\n",
      "\n",
      " Weighted F1 Score for Logistic Regression: [0.6345482985131958, 0.6325667726274002, 0.6007652327540162, 0.5698634285358923, 0.5737095722071418]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with C=0.1\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "f1_scores_lr = []\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(X_train_features_filtered_kbest2, y_train_array), 1):\n",
    "    X_train = X_train_features_filtered_kbest2[train_index]\n",
    "    X_test = X_train_features_filtered_kbest2[test_index]\n",
    "    Y_train = y_train_array[train_index]\n",
    "    Y_test = y_train_array[test_index]\n",
    "\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    predictions_lr = logreg.predict(X_test)\n",
    "    \n",
    "    acc_score = accuracy_score(Y_test, predictions_lr)\n",
    "    prec_score_weighted = precision_score(Y_test, predictions_lr, average='weighted')\n",
    "    rec_score_weighted = recall_score(Y_test, predictions_lr, average='weighted')\n",
    "    f1_score_weighted_lr = f1_score(Y_test, predictions_lr, average='weighted')\n",
    "\n",
    "    f1_scores_lr.append(f1_score_weighted_lr)\n",
    "\n",
    "\n",
    "    print(f'Fold {fold_idx} for Logistic Regression Classifier with C=0.1:')\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    print(f'Weighted Precision Score: {prec_score_weighted}')\n",
    "    print(f'Weighted Recall Score: {rec_score_weighted}')\n",
    "    print(f'Weighted F1 Score: {f1_score_weighted_lr}')\n",
    "    print()\n",
    "\n",
    "print(f' Weighted F1 Score for Logistic Regression: {f1_scores_lr}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
