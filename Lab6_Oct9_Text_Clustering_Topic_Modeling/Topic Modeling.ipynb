{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore topic modeling for discovering broad themes in a collection of **movie summaries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code from :\n",
    "https://github.com/dbamman/anlp21/blob/main/5.eda/TopicModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/37/16/9266c7e205d344cd6bea5074ed769e878c9b3919ab4e1e6adf0ad6370eb8/gensim-4.3.2-cp38-cp38-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp38-cp38-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from gensim) (1.21.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/owenmonroe/anaconda3/envs/textmining/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Downloading gensim-4.3.2-cp38-cp38-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/owenmonroe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import operator\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords(filename):\n",
    "    stopwords={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            stopwords[line.rstrip()]=1\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're running topic modeling on text with lots of names, we'll add the Jockers list of stopwords, which includes character names, to our stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {k:1 for k in stopwords.words('english')}\n",
    "stop_words.update(read_stopwords(\"Datasets/movies_data/jockers.stopwords\"))\n",
    "stop_words[\"'s\"]=1\n",
    "stop_words=list(stop_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to exclude words from a text\n",
    "def filter(word, stopwords):\n",
    "    # no stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    \n",
    "    # has to contain at least one letter\n",
    "    if re.search(\"[A-Za-z]\", word) is not None:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(plotFile, metadataFile, stopwords):\n",
    "    \n",
    "    names={}\n",
    "    box={}\n",
    "    \n",
    "    with open(metadataFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            name=cols[2]\n",
    "            boxoffice=cols[4]\n",
    "            if len(boxoffice) != 0:\n",
    "                box[idd]=int(boxoffice)\n",
    "                names[idd]=name\n",
    "    \n",
    "    n=5000\n",
    "    target_movies={}\n",
    "\n",
    "\n",
    "    sorted_box = sorted(box.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_box[:n]:\n",
    "        target_movies[k]=names[k]\n",
    "    \n",
    "    docs=[]\n",
    "    names=[]\n",
    "   \n",
    "    with open(plotFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            text=cols[1]\n",
    "            \n",
    "            if idd in target_movies:\n",
    "                tokens=nltk.word_tokenize(text.lower())\n",
    "                tokens=[x for x in tokens if filter(x, stopwords)]\n",
    "                docs.append(tokens)\n",
    "                name=target_movies[idd]\n",
    "                names.append(name)\n",
    "    return docs, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read in summaries of the 5,000 movies with the highest box office revenues and convert the movie summaries into a bag-of-words representation using gensim's *corpora.dictionary* methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataFile=\"Datasets/movies_data/movie.metadata.tsv\"\n",
    "plotFile=\"Datasets/movies_data/plot_summaries.txt\"\n",
    "data, doc_names=read_docs(plotFile, metadataFile, stop_words)\n",
    "\n",
    "# create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents and no more than 50% of all documents\n",
    "\n",
    "dictionary = corpora.Dictionary(data)\n",
    "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)\n",
    "\n",
    "# replace dataset with numeric ids words in vocab (and exclude all other words)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]\n",
    "num_topics=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run topic modeling on this data using gensim's built-in LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics, \n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of the topics by printing the top 10 words with highest P(word|topic) for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(\"topic %s:\\t%s\" % (i, ' '.join([term for term, freq in lda_model.show_topic(i, topn=10)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe from the topics above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to understand topics is to print out the documents that have the highest topic representation -- i.e., for a given topic, the documents with highest P(topic=k|document). How much do the documents listed below align with your understanding of the topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model=lda_model \n",
    "\n",
    "topic_docs=[]\n",
    "for i in range(num_topics):\n",
    "    topic_docs.append({})\n",
    "for doc_id in range(len(corpus)):\n",
    "    doc_topics=topic_model.get_document_topics(corpus[doc_id])\n",
    "    for topic_num, topic_prob in doc_topics:\n",
    "        topic_docs[topic_num][doc_id]=topic_prob\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print(\"%s\\n\" % ' '.join([term for term, freq in topic_model.show_topic(i, topn=10)]))\n",
    "    sorted_x = sorted(topic_docs[i].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_x[:5]:\n",
    "        print(\"%s\\t%.3f\\t%s\" % (i,v,doc_names[k]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
