{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates text clustering. Some visualization techniques are also covered. We will use a COVID tweet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load datasets\n",
    "train_data_file = \"./Datasets/Corona_NLP/Tweets_preprocessed_train_data.csv\"\n",
    "test_data_file = \"./Datasets/Corona_NLP/Tweets_preprocessed_test_data.csv\"\n",
    "\n",
    "# import train and test datasets into data frames and print out their original lengths\n",
    "train_data_df = pd.read_csv(train_data_file)\n",
    "test_data_df = pd.read_csv(test_data_file)\n",
    "print (\"Original train set: \",len(train_data_df))\n",
    "print (\"Original test set: \",len(test_data_df))\n",
    "\n",
    "# remove rows with null labels\n",
    "train_data_df = train_data_df[~train_data_df[\"Sentiment\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"Sentiment\"].isnull()]\n",
    "print (\"After removing instances with no labels, train set size: \", len(train_data_df))\n",
    "print (\"After removing instances with no labels, test set size: \", len(test_data_df))\n",
    "\n",
    "# remove empty rows from both datasets and print out their new lengths\n",
    "train_data_df = train_data_df[~train_data_df[\"CleanedTweet\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"CleanedTweet\"].isnull()]\n",
    "print (\"After removing empty tweets, train set size: \",len(train_data_df))\n",
    "print (\"After removing empty tweets, test set size: \",len(test_data_df))\n",
    "\n",
    "# print out top 5 rows of the training set\n",
    "display(train_data_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine the training and test data for later tasks\n",
    "frames = [train_data_df, test_data_df]\n",
    "all_dataset = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text clustering can be helpful when we do not have labels and when our goal is to get a better understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess a subset of data for use (using the entire dataset might be prohibitive in some cases)\n",
    "subset = all_dataset[0:1000].copy()\n",
    "\n",
    "# we do want to exclude stopwords for clustering\n",
    "subset['ProcessedTweet'] = \"\"\n",
    "for index, row in subset.iterrows():\n",
    "    stemmed_words = []\n",
    "    words = row[\"StopwordRemovedTweet\"] \n",
    "    words = words[1:-1].split(',')\n",
    "    altered_text = \"\"\n",
    "    for word in words:\n",
    "        altered_text = altered_text + \" \" +word.strip()[1:-1]\n",
    "    subset.at[index,\"ProcessedTweet\"] = altered_text\n",
    "\n",
    "x_text = subset['ProcessedTweet']\n",
    "display(x_text.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "vectorizer.fit(x_text)\n",
    "\n",
    "X = vectorizer.transform(x_text)\n",
    "\n",
    "final_df = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering partitions data points into k clusters, where each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroids). Keywords can be extracted from clusters to get an intuitive sense of what the clusters are about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create 3 clusters\n",
    "k=3\n",
    "kmeans = KMeans(n_clusters=k).fit(final_df)\n",
    "\n",
    "# the label of the cluster that each instance was assigned to \n",
    "labels = kmeans.predict(final_df)\n",
    "\n",
    "# find center/centroid of clusters \n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing clusters\n",
    "We will use a simple histogram to observe the most dominant words in each cluster (visualization code from https://nbviewer.org/github/LucasTurtle/national-anthems-clustering/blob/master/Cluster_Anthems.ipynb). Experiment with different K values and find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # indices for each cluster\n",
    "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_array = final_df.to_numpy()\n",
    "prediction = kmeans.predict(final_df)\n",
    "n_feats = 20\n",
    "dfs = get_top_features_cluster(final_df_array, prediction, n_feats)\n",
    "plotWords(dfs, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the plots above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated graphs above and observed the most common words in each cluster, we can also generate word clouds from the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform a centroids dataframe into a dictionary to be used for a WordCloud\n",
    "def centroidsDict(centroids, index):\n",
    "    a = centroids.T[index].sort_values(ascending = False).reset_index().values\n",
    "    centroid_dict = dict()\n",
    "\n",
    "    for i in range(0, len(a)):\n",
    "        centroid_dict.update( {a[i,0] : a[i,1]} )\n",
    "\n",
    "    return centroid_dict\n",
    "\n",
    "def generateWordClouds(centroids):\n",
    "    wordcloud = WordCloud(max_font_size=100, background_color = 'white')\n",
    "    for i in range(0, len(centroids)):\n",
    "        centroid_dict = centroidsDict(centroids, i)        \n",
    "        wordcloud.generate_from_frequencies(centroid_dict)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Cluster {}'.format(i))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
    "centroids.columns = final_df.columns\n",
    "generateWordClouds(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to observe the clusters graphically and more intuitively, we are going to use PCA to reduce the dimensionality of our feature matrix, generating a two-dimension plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    " \n",
    "# transform the data\n",
    "df = pca.fit_transform(final_df)\n",
    " \n",
    "df.shape\n",
    "\n",
    "kmeans = KMeans(n_clusters= k)\n",
    " \n",
    "# predict the labels of clusters\n",
    "label = kmeans.fit_predict(df)\n",
    "\n",
    "u_labels = np.unique(label)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# plot the results \n",
    "for i in u_labels:\n",
    "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "\n",
    "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction helps you see the clusters more clearly. In our case, clusters are not too distinct (i.e., there are some overlapping words between clusters), since most of the tweets are about the panic in early days of COVID-19. Making the text representations better (better cleaning, better weighting, etc.) could lead to better clusters. You can also experiment with different k values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate Silhouette score for the clusters as well. See the details on how to do this: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
