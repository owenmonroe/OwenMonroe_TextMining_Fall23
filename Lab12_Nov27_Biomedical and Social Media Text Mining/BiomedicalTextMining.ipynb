{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d315e91d",
   "metadata": {},
   "source": [
    "This notebook explores biomedical text mining. We will use PubMed e-utilities to search and download recent publications about COVID-19 from PubMed. We will then use PubTator to retrieve several types of named entities from these abstracts and visualize them in several ways to better understand the content of these publications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install these packages if needed\n",
    "#!pip install unidecode\n",
    "#!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c80d9",
   "metadata": {},
   "source": [
    "We will first use e-utilities API **ESearch** function to search for COVID-19 publications in PubMed. Esearch retuns publication identifiers (PMIDs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the base URL for PubMed esearch\n",
    "url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed\"\n",
    "\n",
    "# search string for Covid19\n",
    "# (\"coronavirus\"[All Fields] OR \"ncov\"[All Fields] OR \"cov\"[All Fields] OR \"2019-nCoV\"[All Fields] \n",
    "# OR \"SARS-CoV-2\"[All Fields]) OR \"COVID19\"[All Fields] OR \"COVID-19\"[All Fields] OR \"COVID\"[All Fields]) \n",
    "# AND (\"2000/01/01\"[CRDT] : \"3000/01/01\"[CRDT])\n",
    "# This will return 1000 most recent articles on COVID-19 (defined in terms of the search string above) to current time. \n",
    "search_query = '&term=(\"coronavirus\"[All Fields] OR \"ncov\"[All Fields] OR \"cov\"[All Fields] OR \"2019-nCoV\"[All Fields] OR \"SARS-CoV-2\"[All Fields]) OR \"COVID19\"[All Fields] OR \"COVID-19\"[All Fields] OR \"COVID\"[All Fields]) AND (\"2000/01/01\"[CRDT] : \"3000/01/01\"[CRDT])&retmax=1000'\n",
    "\n",
    "# send request and receive response\n",
    "response = []\n",
    "r = requests.post(url+search_query)\n",
    "if r.status_code != 200 :\n",
    "    print (\"[Error]: HTTP code \"+ str(r.status_code))\n",
    "else:\n",
    "    response = r.text\n",
    "\n",
    "# print returned response \n",
    "print(response)\n",
    "\n",
    "# save the response which includes PMIDs to XML file for later use\n",
    "with open(\"covid_pmid_list.xml\", 'w', encoding='utf8') as outputfile:\n",
    "    outputfile.write(response)\n",
    "outputfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb532ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a JSON representation from PMIDs\n",
    "root = ET.fromstring(r.text)\n",
    "tree = ET.ElementTree(root)\n",
    "\n",
    "# fetch the list of PMIDs from XML response\n",
    "pmid_list = []\n",
    "for item in root.iter('Id'):\n",
    "    pmid_list.append(item.text)\n",
    "\n",
    "pmid_json = {\"pmids\": pmid_list}\n",
    "print(pmid_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e5890",
   "metadata": {},
   "source": [
    "We can use e-utilities **ESummary** function to retrieve the article text and metadata. The input for **ESummary** is a set of PMIDs. \n",
    "\n",
    "We can call the e-utilities API at most 3 times in a second, hence we will use a delay of a few seconds before making another call to the e-utiliites API. We use `time.sleep` function for this purpose. Also notice that we call the API 5 times, retrieving 200 records at a time. This is to ensure that we do no exceed the limit for URL length (which will give a HTTP error 414). We save the combined response to a JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ESummary base URL\n",
    "url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed\"\n",
    "pmid_list_lists = [pmid_list[0:200], pmid_list[200:400], pmid_list[400:600], pmid_list[600:800], pmid_list[800:1000]]\n",
    "\n",
    "all_response_text = \"\"\n",
    "final_data_dict = {}\n",
    "final_data_dict[\"eSummaryResult\"] = {\"DocSum\": []}\n",
    "\n",
    "i=0\n",
    "# send request for 200 articles at a time and save the results in a JSON dictionary\n",
    "for pmid_listt in pmid_list_lists:\n",
    "    pmid_list_string = \"\"\n",
    "    for i in range(len(pmid_listt)):\n",
    "        pmid_list_string = pmid_list_string+pmid_listt[i]+\",\"\n",
    "    pmid_list_string = pmid_list_string[:-1]\n",
    "\n",
    "    r = requests.post(url+\"&id=\"+pmid_list_string)\n",
    "    if r.status_code != 200 :\n",
    "        print (\"[Error]: HTTP code \"+ str(r.status_code))\n",
    "    else:\n",
    "        all_response_text = all_response_text + r.text \n",
    "        data_dict = xmltodict.parse(r.text)\n",
    "        x = final_data_dict[\"eSummaryResult\"][\"DocSum\"]\n",
    "        x = x + data_dict[\"eSummaryResult\"][\"DocSum\"]\n",
    "        final_data_dict[\"eSummaryResult\"][\"DocSum\"]= x\n",
    "    i+=1\n",
    "    time.sleep(3)# delay for 3 seconds\n",
    "\n",
    "json_data = json.dumps(final_data_dict)\n",
    "with open(\"pmid_summary_response.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51445b8c",
   "metadata": {},
   "source": [
    "Create a dataframe with the publication metadata (particularly publication type, i.e., whether the article is a review article, editorial, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8ceb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_docs = final_data_dict[\"eSummaryResult\"][\"DocSum\"]\n",
    "df = pd.DataFrame(columns = [\"id\", \"publication_type\"])\n",
    "\n",
    "for i in range(len(list_of_docs)):\n",
    "    df.at[i,\"id\"] = list_of_docs[i][\"Id\"]\n",
    "    for item in list_of_docs[i][\"Item\"]:\n",
    "        if(item[\"@Name\"]==\"PubTypeList\"):\n",
    "            list_of_pub_types = []\n",
    "            try:\n",
    "                list_of_pub_types_items = item[\"Item\"]\n",
    "            except:\n",
    "#                 print(list_of_docs[i][\"Id\"])\n",
    "                break\n",
    "            if(type(list_of_pub_types_items)==list):\n",
    "                list_of_pub_types = [a[\"#text\"] for a in list_of_pub_types_items]\n",
    "            else:\n",
    "                list_of_pub_types = [item[\"Item\"][\"#text\"]]\n",
    "            break\n",
    "    df.at[i,\"publication_type\"] = list_of_pub_types\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d696cda",
   "metadata": {},
   "source": [
    "Next, we look at the distribution of publication types in the retrieved dataset and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c62c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_pub_type_dic = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    x = row[\"publication_type\"]\n",
    "    for i in x:\n",
    "        if i in unique_pub_type_dic:\n",
    "            unique_pub_type_dic[i] +=1\n",
    "        else:\n",
    "            unique_pub_type_dic[i] =1\n",
    "unique_pub_type_dic\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for k,v in sorted(unique_pub_type_dic.items(),key=lambda item: item[1]):\n",
    "    x.append(k)\n",
    "    y.append(v)\n",
    "    \n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.barh(x, y, color ='maroon')\n",
    " \n",
    "plt.ylabel(\"Publication Type\")\n",
    "plt.xlabel(\"Number of Publications\")\n",
    "plt.title(\"Publications with their types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b6d9b",
   "metadata": {},
   "source": [
    "For more information on accessing PubMed via e-utilities, see https://www.ncbi.nlm.nih.gov/books/NBK25500/. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7def1",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "We will use the PubTator tool for the given publications to retrieve six types of named entities for the publications (Disease, Gene, Chemical, Species, CellLine, Mutation). To learn more about PubTator, see https://www.ncbi.nlm.nih.gov/research/pubtator/. We access PubTator through its API, which is similar to e-utilities. Note that PubTator does not perform named entity recognition on the fly, but rather retrieves pre-processed results for publications that we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioC is a format that PubTator uses\n",
    "Format = \"biocjson\"\n",
    "\n",
    "\n",
    "# pubtator-api endpoint to fetch details for given PMIDs\n",
    "r = requests.post(\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/\"+Format , json = pmid_json)\n",
    "\n",
    "if r.status_code != 200 :\n",
    "    print (\"[Error]: HTTP code \"+ str(r.status_code))\n",
    "else:\n",
    "    pass\n",
    "   \n",
    "# Save PubTator results in a JSON file\n",
    "with open(\"api_response.json\", 'w', encoding='utf8') as outputfile:\n",
    "    outputfile.write(r.text)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82cefa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read PubTator JSON file\n",
    "with open(\"api_response.json\", encoding='utf-8-sig') as fp:\n",
    "    response = fp.read()\n",
    "    response = response.replace('\\n', '')\n",
    "    response = response.replace('}{', '},{')\n",
    "    response = \"[\" + response + \"]\"\n",
    "    data = json.loads(response)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fd180",
   "metadata": {},
   "source": [
    "Create a dataframe from this retrieved data segregating the fields that we need for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e477df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(columns = [\"id\", \"authors\", \"year\", \"tags\", \"journal\", \"created_date\", \"accessions\", \"title\", \"abstract\"])\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data_df.at[i,\"id\"] = data[i][\"id\"]\n",
    "    data_df.at[i,\"authors\"] = data[i][\"authors\"]\n",
    "    data_df.at[i,\"year\"] = data[i][\"year\"]\n",
    "    try:\n",
    "        data_df.at[i,\"tags\"] = data[i][\"tags\"]\n",
    "    except: \n",
    "        data_df.at[i,\"tags\"] = []\n",
    "    data_df.at[i,\"journal\"] = data[i][\"journal\"]\n",
    "    data_df.at[i,\"created_date\"] = data[i][\"created\"][\"$date\"]\n",
    "    data_df.at[i,\"title\"] = data[i][\"passages\"][0][\"text\"]\n",
    "    data_df.at[i,\"abstract\"] = data[i][\"passages\"][1][\"text\"]\n",
    "    typelist = [x.split('@')[0] for x in data[i][\"accessions\"]]\n",
    "    data_df.at[i,\"accessions\"] = set(typelist)\n",
    "    \n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add5be3",
   "metadata": {},
   "source": [
    "We now analyze the entities mentioned in these publications. Note that PubTator performs both named entity recognition and entity linking, linking mentions to relevant knowledge bases like Medical Subject Headings (MeSH) and NCBI Taxonomy. \n",
    "\n",
    "\n",
    "First, we will only look at entity types. PubTator identifies 6 types of entities (Disease, Gene, Chemical, Species, Mutation, Cell Line). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_entity_dic = {}\n",
    "\n",
    "for idx, row in data_df.iterrows():\n",
    "    x = row[\"accessions\"]\n",
    "    for i in x:\n",
    "        if i in unique_entity_dic:\n",
    "            unique_entity_dic[i] +=1\n",
    "        else:\n",
    "            unique_entity_dic[i] =1\n",
    "\n",
    "x, y = zip(*sorted(unique_entity_dic.items(), key=lambda item: item[1]))\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# creating the bar plot\n",
    "plt.barh(x, y, color ='maroon')\n",
    " \n",
    "plt.ylabel(\"Entity type\")\n",
    "plt.xlabel(\"Number of publications\")\n",
    "plt.title(\"Publications with entity type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b86a6d",
   "metadata": {},
   "source": [
    "Using this data, we will now find the annotations of different entities that are referenced in these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d6777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in unique_entity_dic:\n",
    "    data_df[\"\"+str(item)] = \"\"\n",
    "    \n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f6d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of frequency in the dataset\n",
    "global_diseases = {}\n",
    "global_chemicals = {}\n",
    "global_species = {}\n",
    "global_genes = {}\n",
    "global_mutations = {}\n",
    "global_celllines = {}\n",
    "global_diseases_count = {}\n",
    "global_chemicals_count = {}\n",
    "global_species_count = {}\n",
    "global_genes_count = {}\n",
    "global_mutations_count = {}\n",
    "global_celllines_count = {}\n",
    "\n",
    "for i in range(len(data)):\n",
    "    ll = data[i][\"passages\"][1][\"annotations\"]\n",
    "    \n",
    "    diseases = []\n",
    "    for it in ll:\n",
    "        if(it[\"infons\"][\"type\"]==\"Disease\"):\n",
    "            name = it[\"text\"]\n",
    "            identity = it[\"infons\"][\"identifier\"]\n",
    "            diseases.append(name)\n",
    "            \n",
    "            if identity in global_diseases:\n",
    "                global_diseases[identity].add(name)\n",
    "            else:\n",
    "                global_diseases[identity] = {name}\n",
    "            \n",
    "            if identity in global_diseases_count:\n",
    "                global_diseases_count[identity] +=1\n",
    "            else:\n",
    "                global_diseases_count[identity] = 1\n",
    "                \n",
    "            diseases = list(set(diseases))\n",
    "    data_df.at[i,\"disease\"] = diseases\n",
    "    \n",
    "    species = []\n",
    "    for it in ll:\n",
    "        if(it[\"infons\"][\"type\"]==\"Species\"):\n",
    "            name = it[\"text\"]\n",
    "            identity = it[\"infons\"][\"identifier\"]\n",
    "            species.append(name)\n",
    "            \n",
    "            if identity in global_species:\n",
    "                global_species[identity].add(name)\n",
    "            else:\n",
    "                global_species[identity] = {name}\n",
    "                \n",
    "            if identity in global_species_count:\n",
    "                global_species_count[identity] +=1\n",
    "            else:\n",
    "                global_species_count[identity] = 1\n",
    "                \n",
    "            species = list(set(species))\n",
    "    data_df.at[i,\"species\"] = species\n",
    "    \n",
    "    genes = []\n",
    "    for it in ll:\n",
    "        if(it[\"infons\"][\"type\"]==\"Gene\"):\n",
    "            name = it[\"text\"]\n",
    "            identity = it[\"infons\"][\"identifier\"]\n",
    "            genes.append(name)\n",
    "            \n",
    "            if identity in global_genes:\n",
    "                global_genes[identity].add(name)\n",
    "            else:\n",
    "                global_genes[identity] = {name}\n",
    "                \n",
    "            if identity in global_genes_count:\n",
    "                global_genes_count[identity] +=1\n",
    "            else:\n",
    "                global_genes_count[identity] = 1\n",
    "                \n",
    "            genes = list(set(genes))\n",
    "    data_df.at[i,\"gene\"] = genes\n",
    "    \n",
    "    chemicals = []\n",
    "    for it in ll:\n",
    "        if(it[\"infons\"][\"type\"]==\"Chemical\"):\n",
    "            name = it[\"text\"]\n",
    "            identity = it[\"infons\"][\"identifier\"]\n",
    "            chemicals.append(name)\n",
    "            \n",
    "            if identity in global_chemicals:\n",
    "                global_chemicals[identity].add(name)\n",
    "            else:\n",
    "                global_chemicals[identity] = {name}\n",
    "                \n",
    "            if identity in global_chemicals_count:\n",
    "                global_chemicals_count[identity] +=1\n",
    "            else:\n",
    "                global_chemicals_count[identity] = 1\n",
    "                \n",
    "            chemicals = list(set(chemicals))\n",
    "    data_df.at[i,\"chemical\"] = chemicals\n",
    "    \n",
    "    mutations = []\n",
    "    for it in ll:\n",
    "        if(it[\"infons\"][\"type\"]==\"Mutation\"):\n",
    "            name = it[\"text\"]\n",
    "            identity = it[\"infons\"][\"identifier\"]\n",
    "            mutations.append(name)\n",
    "            \n",
    "            if identity in global_mutations:\n",
    "                global_mutations[identity].add(name)\n",
    "            else:\n",
    "                global_mutations[identity] = {name}\n",
    "                \n",
    "            if identity in global_mutations_count:\n",
    "                global_mutations_count[identity] +=1\n",
    "            else:\n",
    "                global_mutations_count[identity] = 1\n",
    "                \n",
    "            mutations = list(set(mutations))\n",
    "    data_df.at[i,\"mutation\"] = mutations\n",
    "    \n",
    "    celllines = []\n",
    "    for it in ll:\n",
    "        if(it[\"infons\"][\"type\"]==\"Cellline\"):\n",
    "            name = it[\"text\"]\n",
    "            identity = it[\"infons\"][\"identifier\"]\n",
    "            celllines.append(name)\n",
    "            \n",
    "            if identity in global_celllines:\n",
    "                global_celllines[identity].add(name)\n",
    "            else:\n",
    "                global_celllines[identity] = {name}\n",
    "                \n",
    "            if identity in global_celllines_count:\n",
    "                global_celllines_count[identity] +=1\n",
    "            else:\n",
    "                global_celllines_count[identity] = 1\n",
    "                \n",
    "            celllines = list(set(celllines))\n",
    "    data_df.at[i,\"cellline\"] = celllines\n",
    "    \n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2440e289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_diseases_identifiers = sorted(global_diseases, key=lambda k: len(global_diseases[k]), reverse=True)\n",
    "global_chemicals_identifiers = sorted(global_chemicals, key=lambda k: len(global_chemicals[k]), reverse=True)\n",
    "global_genes_identifiers = sorted(global_genes, key=lambda k: len(global_genes[k]), reverse=True)\n",
    "\n",
    "global_diseases_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_count = {} \n",
    "chemicals_count = {}\n",
    "genes_count = {}\n",
    "\n",
    "for idx, row in data_df.iterrows():\n",
    "    x = row[\"disease\"]\n",
    "    for i in x:\n",
    "        if i in diseases_count:\n",
    "            diseases_count[i] +=1\n",
    "        else:\n",
    "            diseases_count[i] =1\n",
    "\n",
    "for idx, row in data_df.iterrows():\n",
    "    x = row[\"chemical\"]\n",
    "    for i in x:\n",
    "        if i in chemicals_count:\n",
    "            chemicals_count[i] +=1\n",
    "        else:\n",
    "            chemicals_count[i] =1\n",
    "\n",
    "for idx, row in data_df.iterrows():\n",
    "    x = row[\"gene\"]\n",
    "    for i in x:\n",
    "        if i in genes_count:\n",
    "            genes_count[i] +=1\n",
    "        else:\n",
    "            genes_count[i] =1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdefc2",
   "metadata": {},
   "source": [
    "### Analyzing different entities in these publications\n",
    "\n",
    "We will visualize the diseases most commonly mentioned in these publications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cfc66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(20):\n",
    "    identity = global_diseases_identifiers[i]\n",
    "    name_list = global_diseases[identity]\n",
    "    maxx = -1\n",
    "    name = \"\"\n",
    "    for l in name_list:\n",
    "        if diseases_count[l] > maxx:\n",
    "            maxx = diseases_count[l]\n",
    "            name = l\n",
    "        maxx = max(maxx, diseases_count[l])\n",
    "    \n",
    "    x.append(str(identity)+ \" - \"+ name)\n",
    "    y.append(global_diseases_count[identity])\n",
    "\n",
    "dis_dict = {x[i]: y[i] for i in range(len(x))}\n",
    "x, y = zip(*sorted(dis_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.barh(x, y, color ='maroon')\n",
    "\n",
    "plt.ylabel(\"Disease name and ID\")\n",
    "plt.xlabel(\"Number of publications\")\n",
    "plt.title(\"Most frequent diseases in the publications\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53517d0",
   "metadata": {},
   "source": [
    "Chemicals (including drugs) most commonly mentioned in these publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(20):\n",
    "    identity = global_chemicals_identifiers[i]\n",
    "    name_list = global_chemicals[identity]\n",
    "    maxx = -1\n",
    "    name = \"\"\n",
    "    for l in name_list:\n",
    "        if chemicals_count[l] > maxx:\n",
    "            maxx = chemicals_count[l]\n",
    "            name = l\n",
    "        maxx = max(maxx, chemicals_count[l])\n",
    "    \n",
    "    x.append(str(identity)+ \" - \"+ name)\n",
    "    y.append(global_chemicals_count[identity])\n",
    "\n",
    "chem_dict = {x[i]: y[i] for i in range(len(x))}\n",
    "x, y = zip(*sorted(chem_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.barh(x, y, color ='maroon')\n",
    "\n",
    "plt.ylabel(\"Chemical\")\n",
    "plt.xlabel(\"Number of publications\")\n",
    "plt.title(\"Most frequent chemicals in the publications\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4278e",
   "metadata": {},
   "source": [
    "Genes most commonly mentioned in these publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2843b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(20):\n",
    "    identity = global_genes_identifiers[i]\n",
    "    name_list = global_genes[identity]\n",
    "    maxx = -1\n",
    "    name = \"\"\n",
    "    for l in name_list:\n",
    "        if genes_count[l] > maxx:\n",
    "            maxx = genes_count[l]\n",
    "            name = l\n",
    "        maxx = max(maxx, genes_count[l])\n",
    "    \n",
    "    x.append(str(identity)+ \" - \"+ name)\n",
    "    y.append(global_genes_count[identity])\n",
    "\n",
    "gene_dict = {x[i]: y[i] for i in range(len(x))}\n",
    "x, y = zip(*sorted(gene_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.barh(x, y, color ='maroon')\n",
    "\n",
    "plt.ylabel(\"Genes\")\n",
    "plt.xlabel(\"Number of publications\")\n",
    "plt.title(\"Most frequent genes in the publications\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607123a2",
   "metadata": {},
   "source": [
    "You can do the same with other entity types (species, mutations, etc.). \n",
    "\n",
    "While PubTator currently does not extract relations, it could be interesting to look at the co-occurrence of different entities in the sentences of these publications as well, which is a very rudimentary way of doing relation extraction. Think about how you might implement this using PubTator entities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
